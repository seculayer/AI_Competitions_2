{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-08T09:48:42.472526Z",
     "iopub.status.busy": "2021-03-08T09:48:42.471620Z",
     "iopub.status.idle": "2021-03-08T09:48:50.311716Z",
     "shell.execute_reply": "2021-03-08T09:48:50.310480Z"
    },
    "papermill": {
     "duration": 7.874922,
     "end_time": "2021-03-08T09:48:50.311954",
     "exception": false,
     "start_time": "2021-03-08T09:48:42.437032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action=\"ignore\",category=DeprecationWarning)\n",
    "warnings.filterwarnings(action=\"ignore\",category=FutureWarning)\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from keras.layers import Dense, Input, Activation\n",
    "from keras.layers import BatchNormalization,Add,Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, load_model\n",
    "from keras import callbacks\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-08T09:48:50.358521Z",
     "iopub.status.busy": "2021-03-08T09:48:50.356947Z",
     "iopub.status.idle": "2021-03-08T09:48:50.359233Z",
     "shell.execute_reply": "2021-03-08T09:48:50.359789Z"
    },
    "papermill": {
     "duration": 0.02798,
     "end_time": "2021-03-08T09:48:50.359970",
     "exception": false,
     "start_time": "2021-03-08T09:48:50.331990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '../input'\n",
    "SUBMISSIONS_PATH = './'\n",
    "# use atomic numbers to recode atomic names\n",
    "ATOMIC_NUMBERS = {\n",
    "    'H': 1,\n",
    "    'C': 6,\n",
    "    'N': 7,\n",
    "    'O': 8,\n",
    "    'F': 9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-08T09:48:50.404292Z",
     "iopub.status.busy": "2021-03-08T09:48:50.403503Z",
     "iopub.status.idle": "2021-03-08T09:48:50.407528Z",
     "shell.execute_reply": "2021-03-08T09:48:50.406958Z"
    },
    "papermill": {
     "duration": 0.028112,
     "end_time": "2021-03-08T09:48:50.407664",
     "exception": false,
     "start_time": "2021-03-08T09:48:50.379552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_rows', 120)\n",
    "pd.set_option('display.max_columns', 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-08T09:48:50.456116Z",
     "iopub.status.busy": "2021-03-08T09:48:50.455384Z",
     "iopub.status.idle": "2021-03-08T09:48:58.170669Z",
     "shell.execute_reply": "2021-03-08T09:48:58.169507Z"
    },
    "papermill": {
     "duration": 7.743549,
     "end_time": "2021-03-08T09:48:58.170848",
     "exception": false,
     "start_time": "2021-03-08T09:48:50.427299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_index</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>type</th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.807602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.257000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.807404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.809303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.809502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    molecule_index  atom_index_0  atom_index_1  type  scalar_coupling_constant\n",
       "id                                                                            \n",
       "0   1               1             0             1JHC  84.807602               \n",
       "1   1               1             2             2JHH -11.257000               \n",
       "2   1               1             3             2JHH -11.254800               \n",
       "3   1               1             4             2JHH -11.254300               \n",
       "4   1               2             0             1JHC  84.807404               \n",
       "5   1               2             3             2JHH -11.254100               \n",
       "6   1               2             4             2JHH -11.254800               \n",
       "7   1               3             0             1JHC  84.809303               \n",
       "8   1               3             4             2JHH -11.254300               \n",
       "9   1               4             0             1JHC  84.809502               "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dtypes = {\n",
    "    'molecule_name': 'category',\n",
    "    'atom_index_0': 'int8',\n",
    "    'atom_index_1': 'int8',\n",
    "    'type': 'category',\n",
    "    'scalar_coupling_constant': 'float32'\n",
    "}\n",
    "train_csv = pd.read_csv('../input/champs-scalar-coupling/train.csv', index_col='id', dtype=train_dtypes)\n",
    "train_csv['molecule_index'] = train_csv.molecule_name.str.replace('dsgdb9nsd_', '').astype('int32')\n",
    "train_csv = train_csv[['molecule_index', 'atom_index_0', 'atom_index_1', 'type', 'scalar_coupling_constant']]\n",
    "train_csv.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-08T09:48:58.225278Z",
     "iopub.status.busy": "2021-03-08T09:48:58.224215Z",
     "iopub.status.idle": "2021-03-08T09:48:58.230759Z",
     "shell.execute_reply": "2021-03-08T09:48:58.230157Z"
    },
    "papermill": {
     "duration": 0.036907,
     "end_time": "2021-03-08T09:48:58.230897",
     "exception": false,
     "start_time": "2021-03-08T09:48:58.193990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (4659076, 5)\n",
      "Total:  88522828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index                       37272608\n",
       "molecule_index              18636304\n",
       "atom_index_0                4659076 \n",
       "atom_index_1                4659076 \n",
       "type                        4659460 \n",
       "scalar_coupling_constant    18636304\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Shape: ', train_csv.shape)\n",
    "print('Total: ', train_csv.memory_usage().sum())\n",
    "train_csv.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-08T09:48:58.288816Z",
     "iopub.status.busy": "2021-03-08T09:48:58.288142Z",
     "iopub.status.idle": "2021-03-08T09:49:01.640010Z",
     "shell.execute_reply": "2021-03-08T09:49:01.638911Z"
    },
    "papermill": {
     "duration": 3.388446,
     "end_time": "2021-03-08T09:49:01.640168",
     "exception": false,
     "start_time": "2021-03-08T09:48:58.251722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_index</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4658147</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658148</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658149</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3JHH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658150</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658151</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658152</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658153</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658154</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658155</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2JHH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658156</th>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         molecule_index  atom_index_0  atom_index_1  type\n",
       "id                                                       \n",
       "4658147  4               2             0             2JHC\n",
       "4658148  4               2             1             1JHC\n",
       "4658149  4               2             3             3JHH\n",
       "4658150  4               3             0             1JHC\n",
       "4658151  4               3             1             2JHC\n",
       "4658152  15              3             0             1JHC\n",
       "4658153  15              3             2             3JHC\n",
       "4658154  15              3             4             2JHH\n",
       "4658155  15              3             5             2JHH\n",
       "4658156  15              4             0             1JHC"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_csv = pd.read_csv('../input/test-data/test.csv', index_col='id', dtype=train_dtypes)\n",
    "test_csv['molecule_index'] = test_csv['molecule_name'].str.replace('dsgdb9nsd_', '').astype('int32')\n",
    "test_csv = test_csv[['molecule_index', 'atom_index_0', 'atom_index_1', 'type']]\n",
    "test_csv.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-08T09:49:01.695772Z",
     "iopub.status.busy": "2021-03-08T09:49:01.695062Z",
     "iopub.status.idle": "2021-03-08T09:49:09.150314Z",
     "shell.execute_reply": "2021-03-08T09:49:09.149692Z"
    },
    "papermill": {
     "duration": 7.487682,
     "end_time": "2021-03-08T09:49:09.150501",
     "exception": false,
     "start_time": "2021-03-08T09:49:01.662819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_index</th>\n",
       "      <th>atom_index</th>\n",
       "      <th>atom</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.012698</td>\n",
       "      <td>1.085804</td>\n",
       "      <td>0.008001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>-0.006031</td>\n",
       "      <td>0.001976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.011731</td>\n",
       "      <td>1.463751</td>\n",
       "      <td>0.000277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.540815</td>\n",
       "      <td>1.447527</td>\n",
       "      <td>-0.876644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.523814</td>\n",
       "      <td>1.437933</td>\n",
       "      <td>0.906397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.040426</td>\n",
       "      <td>1.024108</td>\n",
       "      <td>0.062564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017257</td>\n",
       "      <td>0.012545</td>\n",
       "      <td>-0.027377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>1.358745</td>\n",
       "      <td>-0.028758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.520278</td>\n",
       "      <td>1.343532</td>\n",
       "      <td>-0.775543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.034360</td>\n",
       "      <td>0.977540</td>\n",
       "      <td>0.007602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   molecule_index  atom_index  atom         x         y         z\n",
       "0  1               0           6    -0.012698  1.085804  0.008001\n",
       "1  1               1           1     0.002150 -0.006031  0.001976\n",
       "2  1               2           1     1.011731  1.463751  0.000277\n",
       "3  1               3           1    -0.540815  1.447527 -0.876644\n",
       "4  1               4           1    -0.523814  1.437933  0.906397\n",
       "5  2               0           7    -0.040426  1.024108  0.062564\n",
       "6  2               1           1     0.017257  0.012545 -0.027377\n",
       "7  2               2           1     0.915789  1.358745 -0.028758\n",
       "8  2               3           1    -0.520278  1.343532 -0.775543\n",
       "9  3               0           8    -0.034360  0.977540  0.007602"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_dtypes = {\n",
    "    'molecule_name': 'category',\n",
    "    'atom_index': 'int8',\n",
    "    'atom': 'category',\n",
    "    'x': 'float32',\n",
    "    'y': 'float32',\n",
    "    'z': 'float32'\n",
    "}\n",
    "structures_csv = pd.read_csv('../input/champs-scalar-coupling/structures.csv', dtype=structures_dtypes)\n",
    "structures_csv['molecule_index'] = structures_csv.molecule_name.str.replace('dsgdb9nsd_', '').astype('int32')\n",
    "structures_csv = structures_csv[['molecule_index', 'atom_index', 'atom', 'x', 'y', 'z']]\n",
    "structures_csv['atom'] = structures_csv['atom'].replace(ATOMIC_NUMBERS).astype('int8')\n",
    "structures_csv.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-08T09:49:09.211702Z",
     "iopub.status.busy": "2021-03-08T09:49:09.210738Z",
     "iopub.status.idle": "2021-03-08T09:49:09.217198Z",
     "shell.execute_reply": "2021-03-08T09:49:09.216633Z"
    },
    "papermill": {
     "duration": 0.039373,
     "end_time": "2021-03-08T09:49:09.217343",
     "exception": false,
     "start_time": "2021-03-08T09:49:09.177970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (2358875, 6)\n",
      "Total:  42459878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index             128    \n",
       "molecule_index    9435500\n",
       "atom_index        2358875\n",
       "atom              2358875\n",
       "x                 9435500\n",
       "y                 9435500\n",
       "z                 9435500\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Shape: ', structures_csv.shape)\n",
    "print('Total: ', structures_csv.memory_usage().sum())\n",
    "structures_csv.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-08T09:49:09.272561Z",
     "iopub.status.busy": "2021-03-08T09:49:09.271706Z",
     "iopub.status.idle": "2021-03-08T09:49:09.275355Z",
     "shell.execute_reply": "2021-03-08T09:49:09.275937Z"
    },
    "papermill": {
     "duration": 0.03439,
     "end_time": "2021-03-08T09:49:09.276112",
     "exception": false,
     "start_time": "2021-03-08T09:49:09.241722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_type_dataframes(base, structures, coupling_type):\n",
    "    base = base[base['type'] == coupling_type].drop('type', axis=1).copy()\n",
    "    base = base.reset_index()\n",
    "    base['id'] = base['id'].astype('int32')\n",
    "    structures = structures[structures['molecule_index'].isin(base['molecule_index'])]\n",
    "    return base, structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-08T09:49:09.331113Z",
     "iopub.status.busy": "2021-03-08T09:49:09.330281Z",
     "iopub.status.idle": "2021-03-08T09:49:09.334347Z",
     "shell.execute_reply": "2021-03-08T09:49:09.333786Z"
    },
    "papermill": {
     "duration": 0.03433,
     "end_time": "2021-03-08T09:49:09.334497",
     "exception": false,
     "start_time": "2021-03-08T09:49:09.300167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_coordinates(base, structures, index):\n",
    "    df = pd.merge(base, structures, how='inner',\n",
    "                  left_on=['molecule_index', f'atom_index_{index}'],\n",
    "                  right_on=['molecule_index', 'atom_index']).drop(['atom_index'], axis=1)\n",
    "    df = df.rename(columns={\n",
    "        'atom': f'atom_{index}',\n",
    "        'x': f'x_{index}',\n",
    "        'y': f'y_{index}',\n",
    "        'z': f'z_{index}'\n",
    "    })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-08T09:49:09.388874Z",
     "iopub.status.busy": "2021-03-08T09:49:09.387064Z",
     "iopub.status.idle": "2021-03-08T09:49:09.389618Z",
     "shell.execute_reply": "2021-03-08T09:49:09.390147Z"
    },
    "papermill": {
     "duration": 0.032056,
     "end_time": "2021-03-08T09:49:09.390300",
     "exception": false,
     "start_time": "2021-03-08T09:49:09.358244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_atoms(base, atoms):\n",
    "    df = pd.merge(base, atoms, how='inner',\n",
    "                  on=['molecule_index', 'atom_index_0', 'atom_index_1'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-08T09:49:09.444071Z",
     "iopub.status.busy": "2021-03-08T09:49:09.443260Z",
     "iopub.status.idle": "2021-03-08T09:49:09.447516Z",
     "shell.execute_reply": "2021-03-08T09:49:09.446905Z"
    },
    "papermill": {
     "duration": 0.03359,
     "end_time": "2021-03-08T09:49:09.447649",
     "exception": false,
     "start_time": "2021-03-08T09:49:09.414059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_all_atoms(base, structures):\n",
    "    df = pd.merge(base, structures, how='left',\n",
    "                  left_on=['molecule_index'],\n",
    "                  right_on=['molecule_index'])\n",
    "    df = df[(df.atom_index_0 != df.atom_index) & (df.atom_index_1 != df.atom_index)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-08T09:49:09.514759Z",
     "iopub.status.busy": "2021-03-08T09:49:09.512752Z",
     "iopub.status.idle": "2021-03-08T09:49:09.515643Z",
     "shell.execute_reply": "2021-03-08T09:49:09.516194Z"
    },
    "papermill": {
     "duration": 0.044552,
     "end_time": "2021-03-08T09:49:09.516388",
     "exception": false,
     "start_time": "2021-03-08T09:49:09.471836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 原子对的中心距离\n",
    "def add_center(df):\n",
    "    df['x_c'] = ((df['x_1'] + df['x_0']) * np.float32(0.5))\n",
    "    df['y_c'] = ((df['y_1'] + df['y_0']) * np.float32(0.5))\n",
    "    df['z_c'] = ((df['z_1'] + df['z_0']) * np.float32(0.5))\n",
    "#原子到中心点的距离\n",
    "def add_distance_to_center(df):\n",
    "    df['d_c'] = ((\n",
    "        (df['x_c'] - df['x'])**np.float32(2) +\n",
    "        (df['y_c'] - df['y'])**np.float32(2) + \n",
    "        (df['z_c'] - df['z'])**np.float32(2)\n",
    "    )**np.float32(0.5))\n",
    "\n",
    "def add_distance_between(df, suffix1, suffix2):\n",
    "    df[f'd_{suffix1}_{suffix2}'] = ((\n",
    "        (df[f'x_{suffix1}'] - df[f'x_{suffix2}'])**np.float32(2) +\n",
    "        (df[f'y_{suffix1}'] - df[f'y_{suffix2}'])**np.float32(2) + \n",
    "        (df[f'z_{suffix1}'] - df[f'z_{suffix2}'])**np.float32(2)\n",
    "    )**np.float32(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-08T09:49:09.573611Z",
     "iopub.status.busy": "2021-03-08T09:49:09.572885Z",
     "iopub.status.idle": "2021-03-08T09:49:09.577552Z",
     "shell.execute_reply": "2021-03-08T09:49:09.576943Z"
    },
    "papermill": {
     "duration": 0.036159,
     "end_time": "2021-03-08T09:49:09.577697",
     "exception": false,
     "start_time": "2021-03-08T09:49:09.541538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_distances(df):\n",
    "    n_atoms = 1 + max([int(c.split('_')[1]) for c in df.columns if c.startswith('x_')])\n",
    "    \n",
    "    for i in range(1, n_atoms):\n",
    "        for vi in range(min(4, i)):\n",
    "            add_distance_between(df, i, vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-08T09:49:09.636347Z",
     "iopub.status.busy": "2021-03-08T09:49:09.634831Z",
     "iopub.status.idle": "2021-03-08T09:49:09.637098Z",
     "shell.execute_reply": "2021-03-08T09:49:09.637634Z"
    },
    "papermill": {
     "duration": 0.034761,
     "end_time": "2021-03-08T09:49:09.637807",
     "exception": false,
     "start_time": "2021-03-08T09:49:09.603046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_n_atoms(base, structures):\n",
    "    dfs = structures['molecule_index'].value_counts().rename('n_atoms').to_frame()\n",
    "    return pd.merge(base, dfs, left_on='molecule_index', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-08T09:49:09.706535Z",
     "iopub.status.busy": "2021-03-08T09:49:09.704447Z",
     "iopub.status.idle": "2021-03-08T09:49:09.707311Z",
     "shell.execute_reply": "2021-03-08T09:49:09.707865Z"
    },
    "papermill": {
     "duration": 0.044991,
     "end_time": "2021-03-08T09:49:09.708050",
     "exception": false,
     "start_time": "2021-03-08T09:49:09.663059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_couple_dataframe(some_csv, structures_csv, coupling_type, n_atoms=10):\n",
    "    base, structures = build_type_dataframes(some_csv, structures_csv, coupling_type)\n",
    "    base = add_coordinates(base, structures, 0)\n",
    "    base = add_coordinates(base, structures, 1)\n",
    "    \n",
    "    base = base.drop(['atom_0', 'atom_1'], axis=1)\n",
    "    atoms = base.drop('id', axis=1).copy()\n",
    "    if 'scalar_coupling_constant' in some_csv:\n",
    "        atoms = atoms.drop(['scalar_coupling_constant'], axis=1)\n",
    "        \n",
    "    add_center(atoms)\n",
    "    atoms = atoms.drop(['x_0', 'y_0', 'z_0', 'x_1', 'y_1', 'z_1'], axis=1)\n",
    "\n",
    "    atoms = merge_all_atoms(atoms, structures)\n",
    "    \n",
    "    add_distance_to_center(atoms)\n",
    "    \n",
    "    atoms = atoms.drop(['x_c', 'y_c', 'z_c', 'atom_index'], axis=1)\n",
    "    atoms.sort_values(['molecule_index', 'atom_index_0', 'atom_index_1', 'd_c'], inplace=True)\n",
    "    atom_groups = atoms.groupby(['molecule_index', 'atom_index_0', 'atom_index_1'])\n",
    "    atoms['num'] = atom_groups.cumcount() + 2\n",
    "    atoms = atoms.drop(['d_c'], axis=1)\n",
    "    atoms = atoms[atoms['num'] < n_atoms]\n",
    "\n",
    "    atoms = atoms.set_index(['molecule_index', 'atom_index_0', 'atom_index_1', 'num']).unstack()\n",
    "    atoms.columns = [f'{col[0]}_{col[1]}' for col in atoms.columns]\n",
    "    atoms = atoms.reset_index()\n",
    "    \n",
    "    # downcast back to int8\n",
    "    for col in atoms.columns:\n",
    "        if col.startswith('atom_'):\n",
    "            atoms[col] = atoms[col].fillna(0).astype('int8')\n",
    "            \n",
    "    atoms['molecule_index'] = atoms['molecule_index'].astype('int32')\n",
    "    \n",
    "    full = add_atoms(base, atoms)\n",
    "    add_distances(full)\n",
    "    \n",
    "    full.sort_values('id', inplace=True)\n",
    "    \n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-08T09:49:09.767214Z",
     "iopub.status.busy": "2021-03-08T09:49:09.766445Z",
     "iopub.status.idle": "2021-03-08T09:49:09.769599Z",
     "shell.execute_reply": "2021-03-08T09:49:09.768908Z"
    },
    "papermill": {
     "duration": 0.036031,
     "end_time": "2021-03-08T09:49:09.769746",
     "exception": false,
     "start_time": "2021-03-08T09:49:09.733715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def take_n_atoms(df, n_atoms, four_start=4):\n",
    "    labels = []\n",
    "    for i in range(2, n_atoms):\n",
    "        label = f'atom_{i}'\n",
    "        labels.append(label)\n",
    "\n",
    "    for i in range(n_atoms):\n",
    "        num = min(i, 4) if i < four_start else 4\n",
    "        for j in range(num):\n",
    "            labels.append(f'd_{i}_{j}')\n",
    "    if 'scalar_coupling_constant' in df:\n",
    "        labels.append('scalar_coupling_constant')\n",
    "    return df[labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-08T09:49:09.830100Z",
     "iopub.status.busy": "2021-03-08T09:49:09.829284Z",
     "iopub.status.idle": "2021-03-08T09:49:09.833442Z",
     "shell.execute_reply": "2021-03-08T09:49:09.832827Z"
    },
    "papermill": {
     "duration": 0.03779,
     "end_time": "2021-03-08T09:49:09.833586",
     "exception": false,
     "start_time": "2021-03-08T09:49:09.795796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_nn_model(input_shape):\n",
    "    inp = Input(shape=(input_shape,))\n",
    "    x = Dense(2048, activation=\"relu\")(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    #x = Dropout(0.4)(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    out = Dense(1, activation=\"linear\")(x)  \n",
    "   # out1 = Dense(2, activation=\"linear\")(x)#mulliken charge 2\n",
    "   # out2 = Dense(6, activation=\"linear\")(x)#tensor 6(xx,yy,zz)\n",
    "   # out3 = Dense(12, activation=\"linear\")(x)#tensor 12(others) \n",
    "   # out4 = Dense(1, activation=\"linear\")(x)#scalar_coupling_constant \n",
    "    #model = Model(inputs=inp, outputs=[out,out1,out2,out3,out4])\n",
    "    model = Model(inputs=inp, outputs=[out])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-08T09:49:09.891434Z",
     "iopub.status.busy": "2021-03-08T09:49:09.890548Z",
     "iopub.status.idle": "2021-03-08T09:49:09.894718Z",
     "shell.execute_reply": "2021-03-08T09:49:09.894123Z"
    },
    "papermill": {
     "duration": 0.03603,
     "end_time": "2021-03-08T09:49:09.894864",
     "exception": false,
     "start_time": "2021-03-08T09:49:09.858834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_history(history, label):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Loss for %s' % label)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    _= plt.legend(['Train','Validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-08T09:49:09.953052Z",
     "iopub.status.busy": "2021-03-08T09:49:09.952318Z",
     "iopub.status.idle": "2021-03-08T09:49:13.152096Z",
     "shell.execute_reply": "2021-03-08T09:49:13.151487Z"
    },
    "papermill": {
     "duration": 3.231537,
     "end_time": "2021-03-08T09:49:13.152259",
     "exception": false,
     "start_time": "2021-03-08T09:49:09.920722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up GPU preferences\n",
    "config = tf.compat.v1.ConfigProto( device_count = {'GPU': 1 , 'CPU': 2} ) \n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "sess = tf.compat.v1.Session(config=config) \n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-08T09:49:13.226362Z",
     "iopub.status.busy": "2021-03-08T09:49:13.225062Z",
     "iopub.status.idle": "2021-03-08T14:36:31.149900Z",
     "shell.execute_reply": "2021-03-08T14:36:31.149234Z"
    },
    "papermill": {
     "duration": 17237.970741,
     "end_time": "2021-03-08T14:36:31.150100",
     "exception": false,
     "start_time": "2021-03-08T09:49:13.179359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1JHC out of ['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN']\n",
      "Categories (8, object): ['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN'] \n",
      "\n",
      "Epoch 1/1000\n",
      "312/312 [==============================] - 8s 18ms/step - loss: 81.6885 - val_loss: 19.1820\n",
      "Epoch 2/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 2.0744 - val_loss: 2.3171\n",
      "Epoch 3/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 1.6350 - val_loss: 1.7738\n",
      "Epoch 4/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 1.5261 - val_loss: 1.8540\n",
      "Epoch 5/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 1.4397 - val_loss: 1.4635\n",
      "Epoch 6/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 1.3463 - val_loss: 1.4562\n",
      "Epoch 7/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 1.3142 - val_loss: 1.5465\n",
      "Epoch 8/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 1.2895 - val_loss: 1.6596\n",
      "Epoch 9/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 1.2547 - val_loss: 1.1595\n",
      "Epoch 10/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 1.2231 - val_loss: 1.2624\n",
      "Epoch 11/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 1.2099 - val_loss: 1.2629\n",
      "Epoch 12/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 1.1799 - val_loss: 1.3623\n",
      "Epoch 13/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 1.1699 - val_loss: 1.2626\n",
      "Epoch 14/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 1.1704 - val_loss: 1.1259\n",
      "Epoch 15/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 1.1053 - val_loss: 1.1509\n",
      "Epoch 16/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 1.0967 - val_loss: 1.1451\n",
      "Epoch 17/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 1.0868 - val_loss: 1.2127\n",
      "Epoch 18/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 1.0782 - val_loss: 1.0651\n",
      "Epoch 19/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 1.0528 - val_loss: 1.0469\n",
      "Epoch 20/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 1.0451 - val_loss: 1.1324\n",
      "Epoch 21/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 1.0697 - val_loss: 1.4019\n",
      "Epoch 22/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 1.0461 - val_loss: 1.0843\n",
      "Epoch 23/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 1.0250 - val_loss: 1.1687\n",
      "Epoch 24/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.9991 - val_loss: 1.1541\n",
      "Epoch 25/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.9970 - val_loss: 0.9901\n",
      "Epoch 26/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.9793 - val_loss: 1.1800\n",
      "Epoch 27/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.9740 - val_loss: 1.0646\n",
      "Epoch 28/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.9819 - val_loss: 1.0269\n",
      "Epoch 29/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.9544 - val_loss: 1.1793\n",
      "Epoch 30/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.9720 - val_loss: 1.0243\n",
      "Epoch 31/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.9573 - val_loss: 1.0387\n",
      "Epoch 32/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.9455 - val_loss: 1.0612\n",
      "Epoch 33/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.9234 - val_loss: 1.1184\n",
      "Epoch 34/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.9171 - val_loss: 0.9677\n",
      "Epoch 35/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.9046 - val_loss: 1.0037\n",
      "Epoch 36/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.9184 - val_loss: 0.9451\n",
      "Epoch 37/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.9169 - val_loss: 1.0415\n",
      "Epoch 38/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.8902 - val_loss: 1.0009\n",
      "Epoch 39/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.8739 - val_loss: 0.9811\n",
      "Epoch 40/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.8671 - val_loss: 0.9359\n",
      "Epoch 41/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.8784 - val_loss: 0.9700\n",
      "Epoch 42/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.8448 - val_loss: 1.0285\n",
      "Epoch 43/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.8768 - val_loss: 1.1671\n",
      "Epoch 44/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.8745 - val_loss: 1.0399\n",
      "Epoch 45/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.8714 - val_loss: 0.9950\n",
      "Epoch 46/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.8267 - val_loss: 0.8577\n",
      "Epoch 47/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.8480 - val_loss: 0.9698\n",
      "Epoch 48/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.8335 - val_loss: 0.9644\n",
      "Epoch 49/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.8263 - val_loss: 0.8488\n",
      "Epoch 50/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.8143 - val_loss: 0.9352\n",
      "Epoch 51/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.8260 - val_loss: 0.9106\n",
      "Epoch 52/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.8328 - val_loss: 0.9323\n",
      "Epoch 53/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.8277 - val_loss: 0.9949\n",
      "Epoch 54/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.8274 - val_loss: 0.9161\n",
      "Epoch 55/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.8041 - val_loss: 0.9463\n",
      "Epoch 56/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.7875 - val_loss: 0.9676\n",
      "Epoch 57/1000\n",
      "312/312 [==============================] - 5s 18ms/step - loss: 0.7902 - val_loss: 0.9344\n",
      "Epoch 58/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.8159 - val_loss: 0.9046\n",
      "Epoch 59/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.7904 - val_loss: 0.8556\n",
      "Epoch 60/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.7793 - val_loss: 0.9165\n",
      "Epoch 61/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.7633 - val_loss: 0.8657\n",
      "Epoch 62/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.7901 - val_loss: 0.9150\n",
      "Epoch 63/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.7896 - val_loss: 0.8559\n",
      "Epoch 64/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.7716 - val_loss: 0.8270\n",
      "Epoch 65/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.7550 - val_loss: 0.8151\n",
      "Epoch 66/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.7803 - val_loss: 0.9111\n",
      "Epoch 67/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.7835 - val_loss: 0.7837\n",
      "Epoch 68/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.7461 - val_loss: 0.9974\n",
      "Epoch 69/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.7697 - val_loss: 0.8426\n",
      "Epoch 70/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.7702 - val_loss: 0.9044\n",
      "Epoch 71/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.7567 - val_loss: 0.8283\n",
      "Epoch 72/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.7420 - val_loss: 0.9032\n",
      "Epoch 73/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.7688 - val_loss: 0.8931\n",
      "Epoch 74/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.7333 - val_loss: 0.8367\n",
      "Epoch 75/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.7290 - val_loss: 0.9020\n",
      "Epoch 76/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.7483 - val_loss: 0.9231\n",
      "Epoch 77/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.7566 - val_loss: 0.8885\n",
      "Epoch 78/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.7348 - val_loss: 0.8729\n",
      "Epoch 79/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.7251 - val_loss: 0.8577\n",
      "Epoch 80/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.7453 - val_loss: 0.8014\n",
      "Epoch 81/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.7192 - val_loss: 0.8148\n",
      "Epoch 82/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.7291 - val_loss: 0.9240\n",
      "Epoch 83/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.7080 - val_loss: 0.8454\n",
      "Epoch 84/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.7249 - val_loss: 0.8316\n",
      "Epoch 85/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.7098 - val_loss: 1.0622\n",
      "Epoch 86/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.7172 - val_loss: 0.9935\n",
      "Epoch 87/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.7082 - val_loss: 0.8686\n",
      "Epoch 88/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.7112 - val_loss: 0.8241\n",
      "Epoch 89/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.7081 - val_loss: 0.8158\n",
      "Epoch 90/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.6972 - val_loss: 0.8303\n",
      "Epoch 91/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.6942 - val_loss: 0.8307\n",
      "Epoch 92/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.7024 - val_loss: 0.9143\n",
      "Epoch 93/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.6764 - val_loss: 0.8542\n",
      "Epoch 94/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.6840 - val_loss: 0.9722\n",
      "Epoch 95/1000\n",
      "312/312 [==============================] - 6s 18ms/step - loss: 0.7015 - val_loss: 0.8203\n",
      "Epoch 96/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.7031 - val_loss: 0.9795\n",
      "Epoch 97/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.7101 - val_loss: 0.8670\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 98/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.6169 - val_loss: 0.6990\n",
      "Epoch 99/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5522 - val_loss: 0.6773\n",
      "Epoch 100/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5669 - val_loss: 0.6780\n",
      "Epoch 101/1000\n",
      "312/312 [==============================] - 6s 18ms/step - loss: 0.5725 - val_loss: 0.6710\n",
      "Epoch 102/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5392 - val_loss: 0.6700\n",
      "Epoch 103/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5636 - val_loss: 0.6710\n",
      "Epoch 104/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5696 - val_loss: 0.6660\n",
      "Epoch 105/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5411 - val_loss: 0.6824\n",
      "Epoch 106/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5505 - val_loss: 0.6687\n",
      "Epoch 107/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.5413 - val_loss: 0.6816\n",
      "Epoch 108/1000\n",
      "312/312 [==============================] - 5s 18ms/step - loss: 0.5351 - val_loss: 0.6685\n",
      "Epoch 109/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5350 - val_loss: 0.6777\n",
      "Epoch 110/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.5701 - val_loss: 0.6710\n",
      "Epoch 111/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5430 - val_loss: 0.6648\n",
      "Epoch 112/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5544 - val_loss: 0.6983\n",
      "Epoch 113/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5499 - val_loss: 0.6630\n",
      "Epoch 114/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.5336 - val_loss: 0.6626\n",
      "Epoch 115/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.5394 - val_loss: 0.6635\n",
      "Epoch 116/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5446 - val_loss: 0.6802\n",
      "Epoch 117/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5441 - val_loss: 0.6696\n",
      "Epoch 118/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5329 - val_loss: 0.6581\n",
      "Epoch 119/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.5422 - val_loss: 0.6676\n",
      "Epoch 120/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5434 - val_loss: 0.6693\n",
      "Epoch 121/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.5343 - val_loss: 0.6678\n",
      "Epoch 122/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5359 - val_loss: 0.6851\n",
      "Epoch 123/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5389 - val_loss: 0.6606\n",
      "Epoch 124/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5303 - val_loss: 0.6678\n",
      "Epoch 125/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5281 - val_loss: 0.6652\n",
      "Epoch 126/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5287 - val_loss: 0.6666\n",
      "Epoch 127/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5134 - val_loss: 0.6727\n",
      "Epoch 128/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.5271 - val_loss: 0.6606\n",
      "Epoch 129/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.5299 - val_loss: 0.6832\n",
      "Epoch 130/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5520 - val_loss: 0.6559\n",
      "Epoch 131/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5233 - val_loss: 0.6557\n",
      "Epoch 132/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.5291 - val_loss: 0.6964\n",
      "Epoch 133/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5374 - val_loss: 0.6865\n",
      "Epoch 134/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5293 - val_loss: 0.6658\n",
      "Epoch 135/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.5318 - val_loss: 0.6576\n",
      "Epoch 136/1000\n",
      "312/312 [==============================] - 5s 18ms/step - loss: 0.5384 - val_loss: 0.6769\n",
      "Epoch 137/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5184 - val_loss: 0.6578\n",
      "Epoch 138/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.5227 - val_loss: 0.6558\n",
      "Epoch 139/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5210 - val_loss: 0.6558\n",
      "Epoch 140/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5361 - val_loss: 0.6520\n",
      "Epoch 141/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5057 - val_loss: 0.6610\n",
      "Epoch 142/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.5115 - val_loss: 0.6586\n",
      "Epoch 143/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.5181 - val_loss: 0.6552\n",
      "Epoch 144/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.5203 - val_loss: 0.6639\n",
      "Epoch 145/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5279 - val_loss: 0.6688\n",
      "Epoch 146/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5101 - val_loss: 0.6852\n",
      "Epoch 147/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5234 - val_loss: 0.6847\n",
      "Epoch 148/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5267 - val_loss: 0.6722\n",
      "Epoch 149/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4916 - val_loss: 0.6518\n",
      "Epoch 150/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.5146 - val_loss: 0.6690\n",
      "Epoch 151/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.5351 - val_loss: 0.6496\n",
      "Epoch 152/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5175 - val_loss: 0.6691\n",
      "Epoch 153/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4959 - val_loss: 0.6571\n",
      "Epoch 154/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5150 - val_loss: 0.6565\n",
      "Epoch 155/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4997 - val_loss: 0.6590\n",
      "Epoch 156/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5136 - val_loss: 0.6514\n",
      "Epoch 157/1000\n",
      "312/312 [==============================] - 6s 19ms/step - loss: 0.5167 - val_loss: 0.6541\n",
      "Epoch 158/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5197 - val_loss: 0.6560\n",
      "Epoch 159/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4932 - val_loss: 0.6980\n",
      "Epoch 160/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5258 - val_loss: 0.6692\n",
      "Epoch 161/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5116 - val_loss: 0.6486\n",
      "Epoch 162/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5384 - val_loss: 0.6647\n",
      "Epoch 163/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4946 - val_loss: 0.6616\n",
      "Epoch 164/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5163 - val_loss: 0.6532\n",
      "Epoch 165/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5019 - val_loss: 0.6486\n",
      "Epoch 166/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4927 - val_loss: 0.6603\n",
      "Epoch 167/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5128 - val_loss: 0.6675\n",
      "Epoch 168/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5227 - val_loss: 0.6566\n",
      "Epoch 169/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.5043 - val_loss: 0.6501\n",
      "Epoch 170/1000\n",
      "312/312 [==============================] - 6s 18ms/step - loss: 0.5243 - val_loss: 0.6561\n",
      "Epoch 171/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4930 - val_loss: 0.6538\n",
      "Epoch 172/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5114 - val_loss: 0.6574\n",
      "Epoch 173/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5034 - val_loss: 0.6652\n",
      "Epoch 174/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5042 - val_loss: 0.6498\n",
      "Epoch 175/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5148 - val_loss: 0.6576\n",
      "Epoch 176/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4868 - val_loss: 0.6733\n",
      "Epoch 177/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5108 - val_loss: 0.6552\n",
      "Epoch 178/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4973 - val_loss: 0.6498\n",
      "Epoch 179/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5393 - val_loss: 0.6548\n",
      "Epoch 180/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4893 - val_loss: 0.6542\n",
      "Epoch 181/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5221 - val_loss: 0.6508\n",
      "Epoch 182/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4919 - val_loss: 0.6531\n",
      "Epoch 183/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.5190 - val_loss: 0.6632\n",
      "Epoch 184/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.5440 - val_loss: 0.6533\n",
      "Epoch 185/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5121 - val_loss: 0.6596\n",
      "Epoch 186/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5041 - val_loss: 0.6738\n",
      "Epoch 187/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4834 - val_loss: 0.6516\n",
      "Epoch 188/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4975 - val_loss: 0.6672\n",
      "Epoch 189/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5020 - val_loss: 0.6681\n",
      "Epoch 190/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5146 - val_loss: 0.6688\n",
      "Epoch 191/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5078 - val_loss: 0.6478\n",
      "Epoch 192/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4968 - val_loss: 0.6543\n",
      "Epoch 193/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4828 - val_loss: 0.6658\n",
      "Epoch 194/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.5004 - val_loss: 0.6535\n",
      "Epoch 195/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5087 - val_loss: 0.6565\n",
      "Epoch 196/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4827 - val_loss: 0.6588\n",
      "Epoch 197/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.5092 - val_loss: 0.6648\n",
      "Epoch 198/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.5092 - val_loss: 0.6579\n",
      "Epoch 199/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4750 - val_loss: 0.6621\n",
      "Epoch 200/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4914 - val_loss: 0.6508\n",
      "Epoch 201/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4992 - val_loss: 0.6526\n",
      "Epoch 202/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5037 - val_loss: 0.6491\n",
      "Epoch 203/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4798 - val_loss: 0.6547\n",
      "Epoch 204/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5108 - val_loss: 0.6728\n",
      "Epoch 205/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5128 - val_loss: 0.6452\n",
      "Epoch 206/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4815 - val_loss: 0.6452\n",
      "Epoch 207/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4991 - val_loss: 0.6752\n",
      "Epoch 208/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4991 - val_loss: 0.6611\n",
      "Epoch 209/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4973 - val_loss: 0.6648\n",
      "Epoch 210/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4894 - val_loss: 0.6417\n",
      "Epoch 211/1000\n",
      "312/312 [==============================] - 5s 18ms/step - loss: 0.4688 - val_loss: 0.6564\n",
      "Epoch 212/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5217 - val_loss: 0.6505\n",
      "Epoch 213/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.5010 - val_loss: 0.6498\n",
      "Epoch 214/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4669 - val_loss: 0.6471\n",
      "Epoch 215/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5141 - val_loss: 0.6411\n",
      "Epoch 216/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4871 - val_loss: 0.6612\n",
      "Epoch 217/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4867 - val_loss: 0.6432\n",
      "Epoch 218/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4955 - val_loss: 0.6510\n",
      "Epoch 219/1000\n",
      "312/312 [==============================] - 5s 18ms/step - loss: 0.4937 - val_loss: 0.6545\n",
      "Epoch 220/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4982 - val_loss: 0.6552\n",
      "Epoch 221/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4776 - val_loss: 0.6650\n",
      "Epoch 222/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5285 - val_loss: 0.6488\n",
      "Epoch 223/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4988 - val_loss: 0.6487\n",
      "Epoch 224/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4947 - val_loss: 0.6557\n",
      "Epoch 225/1000\n",
      "312/312 [==============================] - 6s 19ms/step - loss: 0.4789 - val_loss: 0.6769\n",
      "Epoch 226/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4954 - val_loss: 0.6502\n",
      "Epoch 227/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4925 - val_loss: 0.6808\n",
      "Epoch 228/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4892 - val_loss: 0.6743\n",
      "Epoch 229/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5060 - val_loss: 0.6424\n",
      "Epoch 230/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4875 - val_loss: 0.6582\n",
      "Epoch 231/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4817 - val_loss: 0.6689\n",
      "Epoch 232/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4671 - val_loss: 0.6636\n",
      "Epoch 233/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5283 - val_loss: 0.6633\n",
      "Epoch 234/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4745 - val_loss: 0.6485\n",
      "Epoch 235/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4849 - val_loss: 0.6585\n",
      "Epoch 236/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4840 - val_loss: 0.6396\n",
      "Epoch 237/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4738 - val_loss: 0.6425\n",
      "Epoch 238/1000\n",
      "312/312 [==============================] - 6s 19ms/step - loss: 0.4984 - val_loss: 0.6583\n",
      "Epoch 239/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.5101 - val_loss: 0.6405\n",
      "Epoch 240/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4534 - val_loss: 0.6516\n",
      "Epoch 241/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4637 - val_loss: 0.6594\n",
      "Epoch 242/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4884 - val_loss: 0.6641\n",
      "Epoch 243/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4875 - val_loss: 0.6505\n",
      "Epoch 244/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4777 - val_loss: 0.6678\n",
      "Epoch 245/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4750 - val_loss: 0.6381\n",
      "Epoch 246/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5017 - val_loss: 0.6505\n",
      "Epoch 247/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4829 - val_loss: 0.6515\n",
      "Epoch 248/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4856 - val_loss: 0.6536\n",
      "Epoch 249/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4826 - val_loss: 0.6481\n",
      "Epoch 250/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4847 - val_loss: 0.6491\n",
      "Epoch 251/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.5080 - val_loss: 0.6444\n",
      "Epoch 252/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4954 - val_loss: 0.6385\n",
      "Epoch 253/1000\n",
      "312/312 [==============================] - 6s 18ms/step - loss: 0.4776 - val_loss: 0.6618\n",
      "Epoch 254/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4955 - val_loss: 0.6411\n",
      "Epoch 255/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4640 - val_loss: 0.6389\n",
      "Epoch 256/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4838 - val_loss: 0.6531\n",
      "Epoch 257/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4951 - val_loss: 0.6562\n",
      "Epoch 258/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4950 - val_loss: 0.6449\n",
      "Epoch 259/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4707 - val_loss: 0.6729\n",
      "Epoch 260/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5000 - val_loss: 0.6483\n",
      "Epoch 261/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4720 - val_loss: 0.6472\n",
      "Epoch 262/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5107 - val_loss: 0.6479\n",
      "Epoch 263/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4722 - val_loss: 0.6416\n",
      "Epoch 264/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4763 - val_loss: 0.6422\n",
      "Epoch 265/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4720 - val_loss: 0.6485\n",
      "Epoch 266/1000\n",
      "312/312 [==============================] - 6s 18ms/step - loss: 0.4852 - val_loss: 0.6447\n",
      "Epoch 267/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4904 - val_loss: 0.6490\n",
      "Epoch 268/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4766 - val_loss: 0.6420\n",
      "Epoch 269/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4958 - val_loss: 0.6559\n",
      "Epoch 270/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4882 - val_loss: 0.6497\n",
      "Epoch 271/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4820 - val_loss: 0.6766\n",
      "Epoch 272/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4811 - val_loss: 0.6429\n",
      "Epoch 273/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.5013 - val_loss: 0.6486\n",
      "Epoch 274/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4814 - val_loss: 0.6454\n",
      "Epoch 275/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4740 - val_loss: 0.6716\n",
      "\n",
      "Epoch 00275: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 276/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4479 - val_loss: 0.6285\n",
      "Epoch 277/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4545 - val_loss: 0.6289\n",
      "Epoch 278/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4697 - val_loss: 0.6291\n",
      "Epoch 279/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4481 - val_loss: 0.6298\n",
      "Epoch 280/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4701 - val_loss: 0.6279\n",
      "Epoch 281/1000\n",
      "312/312 [==============================] - 6s 19ms/step - loss: 0.4536 - val_loss: 0.6300\n",
      "Epoch 282/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4707 - val_loss: 0.6279\n",
      "Epoch 283/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4453 - val_loss: 0.6288\n",
      "Epoch 284/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4402 - val_loss: 0.6285\n",
      "Epoch 285/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4744 - val_loss: 0.6274\n",
      "Epoch 286/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4489 - val_loss: 0.6303\n",
      "Epoch 287/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4786 - val_loss: 0.6275\n",
      "Epoch 288/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4644 - val_loss: 0.6278\n",
      "Epoch 289/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4329 - val_loss: 0.6267\n",
      "Epoch 290/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4461 - val_loss: 0.6275\n",
      "Epoch 291/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4412 - val_loss: 0.6274\n",
      "Epoch 292/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4504 - val_loss: 0.6274\n",
      "Epoch 293/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4823 - val_loss: 0.6292\n",
      "Epoch 294/1000\n",
      "312/312 [==============================] - 6s 18ms/step - loss: 0.4634 - val_loss: 0.6292\n",
      "Epoch 295/1000\n",
      "312/312 [==============================] - 6s 18ms/step - loss: 0.4460 - val_loss: 0.6275\n",
      "Epoch 296/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4516 - val_loss: 0.6277\n",
      "Epoch 297/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4496 - val_loss: 0.6286\n",
      "Epoch 298/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4550 - val_loss: 0.6294\n",
      "Epoch 299/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4747 - val_loss: 0.6271\n",
      "Epoch 300/1000\n",
      "312/312 [==============================] - 5s 18ms/step - loss: 0.4576 - val_loss: 0.6281\n",
      "Epoch 301/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4552 - val_loss: 0.6283\n",
      "Epoch 302/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4462 - val_loss: 0.6283\n",
      "Epoch 303/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4501 - val_loss: 0.6266\n",
      "Epoch 304/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4851 - val_loss: 0.6285\n",
      "Epoch 305/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4374 - val_loss: 0.6273\n",
      "Epoch 306/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4438 - val_loss: 0.6267\n",
      "Epoch 307/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4560 - val_loss: 0.6288\n",
      "Epoch 308/1000\n",
      "312/312 [==============================] - 6s 18ms/step - loss: 0.4671 - val_loss: 0.6268\n",
      "Epoch 309/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4563 - val_loss: 0.6263\n",
      "Epoch 310/1000\n",
      "312/312 [==============================] - 6s 18ms/step - loss: 0.4348 - val_loss: 0.6282\n",
      "Epoch 311/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4533 - val_loss: 0.6271\n",
      "Epoch 312/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4669 - val_loss: 0.6288\n",
      "Epoch 313/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4556 - val_loss: 0.6266\n",
      "Epoch 314/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4582 - val_loss: 0.6289\n",
      "Epoch 315/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4931 - val_loss: 0.6292\n",
      "Epoch 316/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4730 - val_loss: 0.6309\n",
      "Epoch 317/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4654 - val_loss: 0.6265\n",
      "Epoch 318/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4494 - val_loss: 0.6286\n",
      "Epoch 319/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4395 - val_loss: 0.6280\n",
      "Epoch 320/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4348 - val_loss: 0.6267\n",
      "Epoch 321/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4568 - val_loss: 0.6277\n",
      "Epoch 322/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4531 - val_loss: 0.6265\n",
      "Epoch 323/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4724 - val_loss: 0.6264\n",
      "Epoch 324/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4652 - val_loss: 0.6281\n",
      "Epoch 325/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4491 - val_loss: 0.6272\n",
      "Epoch 326/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4717 - val_loss: 0.6269\n",
      "Epoch 327/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4846 - val_loss: 0.6264\n",
      "Epoch 328/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4789 - val_loss: 0.6274\n",
      "Epoch 329/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4445 - val_loss: 0.6275\n",
      "Epoch 330/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4562 - val_loss: 0.6273\n",
      "Epoch 331/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4503 - val_loss: 0.6275\n",
      "Epoch 332/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4733 - val_loss: 0.6275\n",
      "Epoch 333/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4495 - val_loss: 0.6273\n",
      "Epoch 334/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4459 - val_loss: 0.6294\n",
      "Epoch 335/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4576 - val_loss: 0.6281\n",
      "Epoch 336/1000\n",
      "312/312 [==============================] - 5s 18ms/step - loss: 0.4529 - val_loss: 0.6261\n",
      "Epoch 337/1000\n",
      "312/312 [==============================] - 6s 18ms/step - loss: 0.4446 - val_loss: 0.6263\n",
      "Epoch 338/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4852 - val_loss: 0.6262\n",
      "Epoch 339/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4541 - val_loss: 0.6274\n",
      "Epoch 340/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4415 - val_loss: 0.6273\n",
      "Epoch 341/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4392 - val_loss: 0.6266\n",
      "Epoch 342/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4344 - val_loss: 0.6286\n",
      "Epoch 343/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4605 - val_loss: 0.6268\n",
      "Epoch 344/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4683 - val_loss: 0.6263\n",
      "Epoch 345/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4681 - val_loss: 0.6260\n",
      "Epoch 346/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4525 - val_loss: 0.6267\n",
      "Epoch 347/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4356 - val_loss: 0.6264\n",
      "Epoch 348/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4621 - val_loss: 0.6273\n",
      "Epoch 349/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4482 - val_loss: 0.6259\n",
      "Epoch 350/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4391 - val_loss: 0.6291\n",
      "Epoch 351/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4615 - val_loss: 0.6272\n",
      "Epoch 352/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4315 - val_loss: 0.6261\n",
      "Epoch 353/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4313 - val_loss: 0.6264\n",
      "Epoch 354/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4238 - val_loss: 0.6264\n",
      "Epoch 355/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4855 - val_loss: 0.6274\n",
      "Epoch 356/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4696 - val_loss: 0.6289\n",
      "Epoch 357/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4688 - val_loss: 0.6273\n",
      "Epoch 358/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4444 - val_loss: 0.6304\n",
      "Epoch 359/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4496 - val_loss: 0.6264\n",
      "Epoch 360/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4637 - val_loss: 0.6297\n",
      "Epoch 361/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4293 - val_loss: 0.6280\n",
      "Epoch 362/1000\n",
      "312/312 [==============================] - 6s 20ms/step - loss: 0.4540 - val_loss: 0.6261\n",
      "Epoch 363/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4527 - val_loss: 0.6293\n",
      "Epoch 364/1000\n",
      "312/312 [==============================] - 5s 18ms/step - loss: 0.4523 - val_loss: 0.6265\n",
      "Epoch 365/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4427 - val_loss: 0.6274\n",
      "Epoch 366/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4528 - val_loss: 0.6266\n",
      "Epoch 367/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4837 - val_loss: 0.6270\n",
      "Epoch 368/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4502 - val_loss: 0.6275\n",
      "Epoch 369/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4584 - val_loss: 0.6259\n",
      "Epoch 370/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4464 - val_loss: 0.6259\n",
      "Epoch 371/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4505 - val_loss: 0.6264\n",
      "Epoch 372/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4585 - val_loss: 0.6273\n",
      "Epoch 373/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4398 - val_loss: 0.6281\n",
      "Epoch 374/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4504 - val_loss: 0.6259\n",
      "Epoch 375/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4566 - val_loss: 0.6259\n",
      "Epoch 376/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4595 - val_loss: 0.6272\n",
      "Epoch 377/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4403 - val_loss: 0.6258\n",
      "Epoch 378/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4573 - val_loss: 0.6264\n",
      "Epoch 379/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4491 - val_loss: 0.6270\n",
      "Epoch 380/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4558 - val_loss: 0.6269\n",
      "Epoch 381/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4480 - val_loss: 0.6266\n",
      "Epoch 382/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4557 - val_loss: 0.6275\n",
      "Epoch 383/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4323 - val_loss: 0.6278\n",
      "Epoch 384/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4320 - val_loss: 0.6290\n",
      "Epoch 385/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4471 - val_loss: 0.6311\n",
      "Epoch 386/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4512 - val_loss: 0.6275\n",
      "Epoch 387/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4595 - val_loss: 0.6259\n",
      "Epoch 388/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4686 - val_loss: 0.6300\n",
      "Epoch 389/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4590 - val_loss: 0.6290\n",
      "Epoch 390/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4269 - val_loss: 0.6269\n",
      "Epoch 391/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4531 - val_loss: 0.6258\n",
      "Epoch 392/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4455 - val_loss: 0.6262\n",
      "Epoch 393/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4581 - val_loss: 0.6306\n",
      "Epoch 394/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4680 - val_loss: 0.6278\n",
      "Epoch 395/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4672 - val_loss: 0.6311\n",
      "Epoch 396/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4753 - val_loss: 0.6284\n",
      "Epoch 397/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4627 - val_loss: 0.6263\n",
      "Epoch 398/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4636 - val_loss: 0.6277\n",
      "Epoch 399/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4455 - val_loss: 0.6308\n",
      "Epoch 400/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4466 - val_loss: 0.6298\n",
      "Epoch 401/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4335 - val_loss: 0.6311\n",
      "Epoch 402/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4506 - val_loss: 0.6282\n",
      "Epoch 403/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4406 - val_loss: 0.6259\n",
      "Epoch 404/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4617 - val_loss: 0.6265\n",
      "Epoch 405/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4565 - val_loss: 0.6261\n",
      "Epoch 406/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4451 - val_loss: 0.6294\n",
      "Epoch 407/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4755 - val_loss: 0.6255\n",
      "Epoch 408/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4459 - val_loss: 0.6285\n",
      "Epoch 409/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4656 - val_loss: 0.6277\n",
      "Epoch 410/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4564 - val_loss: 0.6280\n",
      "Epoch 411/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4410 - val_loss: 0.6294\n",
      "Epoch 412/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4537 - val_loss: 0.6261\n",
      "Epoch 413/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4634 - val_loss: 0.6253\n",
      "Epoch 414/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4824 - val_loss: 0.6290\n",
      "Epoch 415/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4677 - val_loss: 0.6298\n",
      "Epoch 416/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4656 - val_loss: 0.6273\n",
      "Epoch 417/1000\n",
      "312/312 [==============================] - 5s 18ms/step - loss: 0.4490 - val_loss: 0.6259\n",
      "Epoch 418/1000\n",
      "312/312 [==============================] - 6s 18ms/step - loss: 0.4631 - val_loss: 0.6255\n",
      "Epoch 419/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4216 - val_loss: 0.6302\n",
      "Epoch 420/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4543 - val_loss: 0.6264\n",
      "Epoch 421/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4593 - val_loss: 0.6287\n",
      "Epoch 422/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4429 - val_loss: 0.6284\n",
      "Epoch 423/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4520 - val_loss: 0.6264\n",
      "Epoch 424/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4462 - val_loss: 0.6252\n",
      "Epoch 425/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4689 - val_loss: 0.6252\n",
      "Epoch 426/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4459 - val_loss: 0.6280\n",
      "Epoch 427/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4710 - val_loss: 0.6254\n",
      "Epoch 428/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4479 - val_loss: 0.6274\n",
      "Epoch 429/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4416 - val_loss: 0.6250\n",
      "Epoch 430/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4635 - val_loss: 0.6258\n",
      "Epoch 431/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4333 - val_loss: 0.6258\n",
      "Epoch 432/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4385 - val_loss: 0.6287\n",
      "Epoch 433/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4543 - val_loss: 0.6259\n",
      "Epoch 434/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4530 - val_loss: 0.6253\n",
      "Epoch 435/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4274 - val_loss: 0.6266\n",
      "Epoch 436/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4509 - val_loss: 0.6251\n",
      "Epoch 437/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4638 - val_loss: 0.6256\n",
      "Epoch 438/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4730 - val_loss: 0.6295\n",
      "Epoch 439/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4452 - val_loss: 0.6252\n",
      "Epoch 440/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4555 - val_loss: 0.6284\n",
      "Epoch 441/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4653 - val_loss: 0.6255\n",
      "Epoch 442/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4477 - val_loss: 0.6253\n",
      "Epoch 443/1000\n",
      "312/312 [==============================] - 6s 18ms/step - loss: 0.4370 - val_loss: 0.6258\n",
      "Epoch 444/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4697 - val_loss: 0.6267\n",
      "Epoch 445/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4892 - val_loss: 0.6254\n",
      "Epoch 446/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4521 - val_loss: 0.6251\n",
      "Epoch 447/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4634 - val_loss: 0.6262\n",
      "Epoch 448/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4690 - val_loss: 0.6250\n",
      "Epoch 449/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4629 - val_loss: 0.6249\n",
      "Epoch 450/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4468 - val_loss: 0.6268\n",
      "Epoch 451/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4497 - val_loss: 0.6262\n",
      "Epoch 452/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4263 - val_loss: 0.6260\n",
      "Epoch 453/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4585 - val_loss: 0.6270\n",
      "Epoch 454/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4453 - val_loss: 0.6260\n",
      "Epoch 455/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4341 - val_loss: 0.6254\n",
      "Epoch 456/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4501 - val_loss: 0.6278\n",
      "Epoch 457/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4479 - val_loss: 0.6288\n",
      "Epoch 458/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4416 - val_loss: 0.6252\n",
      "Epoch 459/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4317 - val_loss: 0.6258\n",
      "Epoch 460/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4503 - val_loss: 0.6275\n",
      "Epoch 461/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4513 - val_loss: 0.6253\n",
      "Epoch 462/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4278 - val_loss: 0.6248\n",
      "Epoch 463/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4554 - val_loss: 0.6256\n",
      "Epoch 464/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4674 - val_loss: 0.6261\n",
      "Epoch 465/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4283 - val_loss: 0.6261\n",
      "Epoch 466/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4576 - val_loss: 0.6265\n",
      "Epoch 467/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4270 - val_loss: 0.6278\n",
      "Epoch 468/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4347 - val_loss: 0.6299\n",
      "Epoch 469/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4557 - val_loss: 0.6273\n",
      "Epoch 470/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4385 - val_loss: 0.6301\n",
      "Epoch 471/1000\n",
      "312/312 [==============================] - 5s 18ms/step - loss: 0.4498 - val_loss: 0.6266\n",
      "Epoch 472/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4549 - val_loss: 0.6249\n",
      "Epoch 473/1000\n",
      "312/312 [==============================] - 6s 18ms/step - loss: 0.4489 - val_loss: 0.6276\n",
      "Epoch 474/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4583 - val_loss: 0.6267\n",
      "Epoch 475/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4528 - val_loss: 0.6257\n",
      "Epoch 476/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4512 - val_loss: 0.6249\n",
      "Epoch 477/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4464 - val_loss: 0.6248\n",
      "Epoch 478/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4489 - val_loss: 0.6264\n",
      "Epoch 479/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4449 - val_loss: 0.6264\n",
      "\n",
      "Epoch 00479: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 480/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4513 - val_loss: 0.6257\n",
      "Epoch 481/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4560 - val_loss: 0.6255\n",
      "Epoch 482/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4693 - val_loss: 0.6258\n",
      "Epoch 483/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4829 - val_loss: 0.6245\n",
      "Epoch 484/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4784 - val_loss: 0.6254\n",
      "Epoch 485/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4384 - val_loss: 0.6253\n",
      "Epoch 486/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4683 - val_loss: 0.6263\n",
      "Epoch 487/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4632 - val_loss: 0.6250\n",
      "Epoch 488/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4649 - val_loss: 0.6261\n",
      "Epoch 489/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4484 - val_loss: 0.6248\n",
      "Epoch 490/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4778 - val_loss: 0.6264\n",
      "Epoch 491/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4755 - val_loss: 0.6256\n",
      "Epoch 492/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4341 - val_loss: 0.6254\n",
      "Epoch 493/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4460 - val_loss: 0.6258\n",
      "Epoch 494/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4389 - val_loss: 0.6247\n",
      "Epoch 495/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4444 - val_loss: 0.6253\n",
      "Epoch 496/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4487 - val_loss: 0.6256\n",
      "Epoch 497/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4523 - val_loss: 0.6248\n",
      "Epoch 498/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4424 - val_loss: 0.6249\n",
      "Epoch 499/1000\n",
      "312/312 [==============================] - 6s 18ms/step - loss: 0.4517 - val_loss: 0.6254\n",
      "Epoch 500/1000\n",
      "312/312 [==============================] - 6s 18ms/step - loss: 0.4621 - val_loss: 0.6255\n",
      "Epoch 501/1000\n",
      "312/312 [==============================] - 5s 18ms/step - loss: 0.4542 - val_loss: 0.6262\n",
      "Epoch 502/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4752 - val_loss: 0.6254\n",
      "Epoch 503/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4562 - val_loss: 0.6245\n",
      "Epoch 504/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4531 - val_loss: 0.6244\n",
      "Epoch 505/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4466 - val_loss: 0.6250\n",
      "Epoch 506/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4488 - val_loss: 0.6253\n",
      "Epoch 507/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4475 - val_loss: 0.6252\n",
      "Epoch 508/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4412 - val_loss: 0.6252\n",
      "Epoch 509/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4481 - val_loss: 0.6251\n",
      "Epoch 510/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4724 - val_loss: 0.6254\n",
      "Epoch 511/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4596 - val_loss: 0.6253\n",
      "Epoch 512/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4455 - val_loss: 0.6253\n",
      "Epoch 513/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4633 - val_loss: 0.6257\n",
      "Epoch 514/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4318 - val_loss: 0.6252\n",
      "Epoch 515/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4401 - val_loss: 0.6248\n",
      "Epoch 516/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4618 - val_loss: 0.6252\n",
      "Epoch 517/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4596 - val_loss: 0.6252\n",
      "Epoch 518/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4380 - val_loss: 0.6251\n",
      "Epoch 519/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4773 - val_loss: 0.6252\n",
      "Epoch 520/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4606 - val_loss: 0.6247\n",
      "Epoch 521/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4411 - val_loss: 0.6257\n",
      "Epoch 522/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4356 - val_loss: 0.6258\n",
      "Epoch 523/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4335 - val_loss: 0.6247\n",
      "Epoch 524/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4434 - val_loss: 0.6255\n",
      "Epoch 525/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4428 - val_loss: 0.6252\n",
      "Epoch 526/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4447 - val_loss: 0.6263\n",
      "Epoch 527/1000\n",
      "312/312 [==============================] - 6s 18ms/step - loss: 0.4380 - val_loss: 0.6257\n",
      "Epoch 528/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4398 - val_loss: 0.6247\n",
      "Epoch 529/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4339 - val_loss: 0.6249\n",
      "Epoch 530/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4523 - val_loss: 0.6247\n",
      "Epoch 531/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4546 - val_loss: 0.6247\n",
      "Epoch 532/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4532 - val_loss: 0.6257\n",
      "Epoch 533/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4495 - val_loss: 0.6248\n",
      "Epoch 534/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4510 - val_loss: 0.6254\n",
      "\n",
      "Epoch 00534: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "Epoch 535/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4493 - val_loss: 0.6256\n",
      "Epoch 536/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4487 - val_loss: 0.6245\n",
      "Epoch 537/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4517 - val_loss: 0.6251\n",
      "Epoch 538/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4347 - val_loss: 0.6251\n",
      "Epoch 539/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4518 - val_loss: 0.6251\n",
      "Epoch 540/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4617 - val_loss: 0.6252\n",
      "Epoch 541/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4433 - val_loss: 0.6245\n",
      "Epoch 542/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4296 - val_loss: 0.6251\n",
      "Epoch 543/1000\n",
      "312/312 [==============================] - 5s 16ms/step - loss: 0.4268 - val_loss: 0.6259\n",
      "Epoch 544/1000\n",
      "312/312 [==============================] - 5s 17ms/step - loss: 0.4740 - val_loss: 0.6249\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00544: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.47094947\n",
      "Training 2JHH out of ['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN']\n",
      "Categories (8, object): ['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN'] \n",
      "\n",
      "Epoch 1/1000\n",
      "167/167 [==============================] - 4s 18ms/step - loss: 7.4768 - val_loss: 0.9932\n",
      "Epoch 2/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.4999 - val_loss: 0.8140\n",
      "Epoch 3/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.4271 - val_loss: 0.5156\n",
      "Epoch 4/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.3747 - val_loss: 0.4742\n",
      "Epoch 5/1000\n",
      "167/167 [==============================] - 3s 18ms/step - loss: 0.3539 - val_loss: 0.5541\n",
      "Epoch 6/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.3441 - val_loss: 0.4818\n",
      "Epoch 7/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.3234 - val_loss: 0.4306\n",
      "Epoch 8/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.2962 - val_loss: 0.3691\n",
      "Epoch 9/1000\n",
      "167/167 [==============================] - 3s 20ms/step - loss: 0.3030 - val_loss: 0.4558\n",
      "Epoch 10/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.3092 - val_loss: 0.3658\n",
      "Epoch 11/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.2840 - val_loss: 0.3244\n",
      "Epoch 12/1000\n",
      "167/167 [==============================] - 3s 18ms/step - loss: 0.2768 - val_loss: 0.2957\n",
      "Epoch 13/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.2737 - val_loss: 0.2677\n",
      "Epoch 14/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.2554 - val_loss: 0.2993\n",
      "Epoch 15/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.2620 - val_loss: 0.3047\n",
      "Epoch 16/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.2574 - val_loss: 0.2816\n",
      "Epoch 17/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.2516 - val_loss: 0.2798\n",
      "Epoch 18/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.2399 - val_loss: 0.3793\n",
      "Epoch 19/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.2353 - val_loss: 0.2713\n",
      "Epoch 20/1000\n",
      "167/167 [==============================] - 3s 18ms/step - loss: 0.2421 - val_loss: 0.2803\n",
      "Epoch 21/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.2296 - val_loss: 0.2749\n",
      "Epoch 22/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.2337 - val_loss: 0.2823\n",
      "Epoch 23/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.2261 - val_loss: 0.2490\n",
      "Epoch 24/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.2289 - val_loss: 0.2745\n",
      "Epoch 25/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.2262 - val_loss: 0.2545\n",
      "Epoch 26/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.2171 - val_loss: 0.2692\n",
      "Epoch 27/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.2181 - val_loss: 0.2456\n",
      "Epoch 28/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.2182 - val_loss: 0.2555\n",
      "Epoch 29/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.2182 - val_loss: 0.2875\n",
      "Epoch 30/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.2114 - val_loss: 0.3283\n",
      "Epoch 31/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.2177 - val_loss: 0.2028\n",
      "Epoch 32/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.2093 - val_loss: 0.2376\n",
      "Epoch 33/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.2012 - val_loss: 0.2874\n",
      "Epoch 34/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.2125 - val_loss: 0.3130\n",
      "Epoch 35/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.2029 - val_loss: 0.2391\n",
      "Epoch 36/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.2005 - val_loss: 0.2440\n",
      "Epoch 37/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.2113 - val_loss: 0.2082\n",
      "Epoch 38/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1868 - val_loss: 0.2115\n",
      "Epoch 39/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1935 - val_loss: 0.2973\n",
      "Epoch 40/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1957 - val_loss: 0.1920\n",
      "Epoch 41/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1975 - val_loss: 0.2418\n",
      "Epoch 42/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1979 - val_loss: 0.2015\n",
      "Epoch 43/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1961 - val_loss: 0.2252\n",
      "Epoch 44/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1901 - val_loss: 0.2058\n",
      "Epoch 45/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1918 - val_loss: 0.2599\n",
      "Epoch 46/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1871 - val_loss: 0.2217\n",
      "Epoch 47/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1977 - val_loss: 0.2248\n",
      "Epoch 48/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1841 - val_loss: 0.2277\n",
      "Epoch 49/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1861 - val_loss: 0.2465\n",
      "Epoch 50/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1885 - val_loss: 0.1865\n",
      "Epoch 51/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1874 - val_loss: 0.2111\n",
      "Epoch 52/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1938 - val_loss: 0.2185\n",
      "Epoch 53/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1786 - val_loss: 0.2029\n",
      "Epoch 54/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1694 - val_loss: 0.1826\n",
      "Epoch 55/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1709 - val_loss: 0.1991\n",
      "Epoch 56/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1759 - val_loss: 0.2139\n",
      "Epoch 57/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1760 - val_loss: 0.2169\n",
      "Epoch 58/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1754 - val_loss: 0.2256\n",
      "Epoch 59/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1750 - val_loss: 0.2048\n",
      "Epoch 60/1000\n",
      "167/167 [==============================] - 3s 18ms/step - loss: 0.1792 - val_loss: 0.1721\n",
      "Epoch 61/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1717 - val_loss: 0.1866\n",
      "Epoch 62/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1751 - val_loss: 0.2439\n",
      "Epoch 63/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1763 - val_loss: 0.1944\n",
      "Epoch 64/1000\n",
      "167/167 [==============================] - 3s 19ms/step - loss: 0.1840 - val_loss: 0.1760\n",
      "Epoch 65/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1667 - val_loss: 0.1851\n",
      "Epoch 66/1000\n",
      "167/167 [==============================] - 3s 19ms/step - loss: 0.1656 - val_loss: 0.1704\n",
      "Epoch 67/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1653 - val_loss: 0.2220\n",
      "Epoch 68/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1588 - val_loss: 0.2187\n",
      "Epoch 69/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1736 - val_loss: 0.2275\n",
      "Epoch 70/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1738 - val_loss: 0.2190\n",
      "Epoch 71/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1635 - val_loss: 0.2069\n",
      "Epoch 72/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1630 - val_loss: 0.1719\n",
      "Epoch 73/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1604 - val_loss: 0.1660\n",
      "Epoch 74/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1642 - val_loss: 0.1821\n",
      "Epoch 75/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1614 - val_loss: 0.1821\n",
      "Epoch 76/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1580 - val_loss: 0.1969\n",
      "Epoch 77/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1719 - val_loss: 0.2034\n",
      "Epoch 78/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1594 - val_loss: 0.2000\n",
      "Epoch 79/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1659 - val_loss: 0.2160\n",
      "Epoch 80/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1666 - val_loss: 0.1679\n",
      "Epoch 81/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1494 - val_loss: 0.1926\n",
      "Epoch 82/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1498 - val_loss: 0.1826\n",
      "Epoch 83/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1602 - val_loss: 0.1802\n",
      "Epoch 84/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1470 - val_loss: 0.2199\n",
      "Epoch 85/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1724 - val_loss: 0.2058\n",
      "Epoch 86/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1587 - val_loss: 0.1797\n",
      "Epoch 87/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1611 - val_loss: 0.2000\n",
      "Epoch 88/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1617 - val_loss: 0.1787\n",
      "Epoch 89/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1542 - val_loss: 0.1720\n",
      "Epoch 90/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1707 - val_loss: 0.2036\n",
      "Epoch 91/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1531 - val_loss: 0.1900\n",
      "Epoch 92/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1447 - val_loss: 0.1823\n",
      "Epoch 93/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1453 - val_loss: 0.1799\n",
      "Epoch 94/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1531 - val_loss: 0.1911\n",
      "Epoch 95/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1645 - val_loss: 0.2200\n",
      "Epoch 96/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1476 - val_loss: 0.1805\n",
      "Epoch 97/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1603 - val_loss: 0.1623\n",
      "Epoch 98/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1547 - val_loss: 0.1597\n",
      "Epoch 99/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1398 - val_loss: 0.1852\n",
      "Epoch 100/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1528 - val_loss: 0.1864\n",
      "Epoch 101/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1508 - val_loss: 0.2124\n",
      "Epoch 102/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1579 - val_loss: 0.2307\n",
      "Epoch 103/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1501 - val_loss: 0.1898\n",
      "Epoch 104/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1501 - val_loss: 0.1884\n",
      "Epoch 105/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1428 - val_loss: 0.1677\n",
      "Epoch 106/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1493 - val_loss: 0.1659\n",
      "Epoch 107/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1486 - val_loss: 0.1878\n",
      "Epoch 108/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1519 - val_loss: 0.1788\n",
      "Epoch 109/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1356 - val_loss: 0.1875\n",
      "Epoch 110/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1553 - val_loss: 0.1886\n",
      "Epoch 111/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1501 - val_loss: 0.1885\n",
      "Epoch 112/1000\n",
      "167/167 [==============================] - 3s 19ms/step - loss: 0.1483 - val_loss: 0.1558\n",
      "Epoch 113/1000\n",
      "167/167 [==============================] - 3s 20ms/step - loss: 0.1493 - val_loss: 0.1883\n",
      "Epoch 114/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1444 - val_loss: 0.1994\n",
      "Epoch 115/1000\n",
      "167/167 [==============================] - 3s 18ms/step - loss: 0.1508 - val_loss: 0.1788\n",
      "Epoch 116/1000\n",
      "167/167 [==============================] - 3s 18ms/step - loss: 0.1437 - val_loss: 0.1676\n",
      "Epoch 117/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1460 - val_loss: 0.1569\n",
      "Epoch 118/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1415 - val_loss: 0.2095\n",
      "Epoch 119/1000\n",
      "167/167 [==============================] - 3s 19ms/step - loss: 0.1675 - val_loss: 0.1807\n",
      "Epoch 120/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1368 - val_loss: 0.1799\n",
      "Epoch 121/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1481 - val_loss: 0.1742\n",
      "Epoch 122/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1546 - val_loss: 0.1560\n",
      "Epoch 123/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1472 - val_loss: 0.1822\n",
      "Epoch 124/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1411 - val_loss: 0.1661\n",
      "Epoch 125/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1458 - val_loss: 0.1961\n",
      "Epoch 126/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1462 - val_loss: 0.1586\n",
      "Epoch 127/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1491 - val_loss: 0.1813\n",
      "Epoch 128/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1401 - val_loss: 0.1857\n",
      "Epoch 129/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1356 - val_loss: 0.1723\n",
      "Epoch 130/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1413 - val_loss: 0.1873\n",
      "Epoch 131/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1451 - val_loss: 0.1785\n",
      "Epoch 132/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1439 - val_loss: 0.1641\n",
      "Epoch 133/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1422 - val_loss: 0.1938\n",
      "Epoch 134/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1517 - val_loss: 0.2408\n",
      "Epoch 135/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1455 - val_loss: 0.1652\n",
      "Epoch 136/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1416 - val_loss: 0.1724\n",
      "Epoch 137/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1289 - val_loss: 0.1830\n",
      "Epoch 138/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1455 - val_loss: 0.1838\n",
      "Epoch 139/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1420 - val_loss: 0.1993\n",
      "Epoch 140/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1495 - val_loss: 0.1641\n",
      "Epoch 141/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1482 - val_loss: 0.1573\n",
      "Epoch 142/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1382 - val_loss: 0.1857\n",
      "\n",
      "Epoch 00142: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 143/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1154 - val_loss: 0.1257\n",
      "Epoch 144/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1080 - val_loss: 0.1261\n",
      "Epoch 145/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1043 - val_loss: 0.1248\n",
      "Epoch 146/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1008 - val_loss: 0.1232\n",
      "Epoch 147/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1057 - val_loss: 0.1255\n",
      "Epoch 148/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1014 - val_loss: 0.1268\n",
      "Epoch 149/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1051 - val_loss: 0.1425\n",
      "Epoch 150/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1058 - val_loss: 0.1288\n",
      "Epoch 151/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0976 - val_loss: 0.1269\n",
      "Epoch 152/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1041 - val_loss: 0.1215\n",
      "Epoch 153/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1042 - val_loss: 0.1255\n",
      "Epoch 154/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0978 - val_loss: 0.1236\n",
      "Epoch 155/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0980 - val_loss: 0.1226\n",
      "Epoch 156/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0942 - val_loss: 0.1201\n",
      "Epoch 157/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0975 - val_loss: 0.1256\n",
      "Epoch 158/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0903 - val_loss: 0.1245\n",
      "Epoch 159/1000\n",
      "167/167 [==============================] - 3s 19ms/step - loss: 0.1005 - val_loss: 0.1197\n",
      "Epoch 160/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0995 - val_loss: 0.1244\n",
      "Epoch 161/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0952 - val_loss: 0.1211\n",
      "Epoch 162/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1002 - val_loss: 0.1229\n",
      "Epoch 163/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1014 - val_loss: 0.1217\n",
      "Epoch 164/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1000 - val_loss: 0.1207\n",
      "Epoch 165/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1050 - val_loss: 0.1359\n",
      "Epoch 166/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1024 - val_loss: 0.1209\n",
      "Epoch 167/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0948 - val_loss: 0.1222\n",
      "Epoch 168/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0998 - val_loss: 0.1213\n",
      "Epoch 169/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0984 - val_loss: 0.1228\n",
      "Epoch 170/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1018 - val_loss: 0.1228\n",
      "Epoch 171/1000\n",
      "167/167 [==============================] - 3s 18ms/step - loss: 0.0921 - val_loss: 0.1209\n",
      "Epoch 172/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1012 - val_loss: 0.1230\n",
      "Epoch 173/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0936 - val_loss: 0.1233\n",
      "Epoch 174/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0910 - val_loss: 0.1246\n",
      "Epoch 175/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.1041 - val_loss: 0.1212\n",
      "Epoch 176/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0890 - val_loss: 0.1187\n",
      "Epoch 177/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0906 - val_loss: 0.1224\n",
      "Epoch 178/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0977 - val_loss: 0.1198\n",
      "Epoch 179/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0908 - val_loss: 0.1258\n",
      "Epoch 180/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1000 - val_loss: 0.1221\n",
      "Epoch 181/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1146 - val_loss: 0.1209\n",
      "Epoch 182/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0957 - val_loss: 0.1278\n",
      "Epoch 183/1000\n",
      "167/167 [==============================] - 3s 18ms/step - loss: 0.0944 - val_loss: 0.1235\n",
      "Epoch 184/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1009 - val_loss: 0.1250\n",
      "Epoch 185/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0960 - val_loss: 0.1252\n",
      "Epoch 186/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0994 - val_loss: 0.1262\n",
      "Epoch 187/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0953 - val_loss: 0.1200\n",
      "Epoch 188/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0929 - val_loss: 0.1267\n",
      "Epoch 189/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1079 - val_loss: 0.1207\n",
      "Epoch 190/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0937 - val_loss: 0.1245\n",
      "Epoch 191/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0955 - val_loss: 0.1197\n",
      "Epoch 192/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0977 - val_loss: 0.1213\n",
      "Epoch 193/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0954 - val_loss: 0.1217\n",
      "Epoch 194/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0964 - val_loss: 0.1217\n",
      "Epoch 195/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0969 - val_loss: 0.1233\n",
      "Epoch 196/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0937 - val_loss: 0.1205\n",
      "Epoch 197/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1010 - val_loss: 0.1283\n",
      "Epoch 198/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0914 - val_loss: 0.1274\n",
      "Epoch 199/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0935 - val_loss: 0.1207\n",
      "Epoch 200/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0938 - val_loss: 0.1216\n",
      "Epoch 201/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0965 - val_loss: 0.1188\n",
      "Epoch 202/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0942 - val_loss: 0.1200\n",
      "Epoch 203/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0973 - val_loss: 0.1217\n",
      "Epoch 204/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0999 - val_loss: 0.1223\n",
      "Epoch 205/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0959 - val_loss: 0.1207\n",
      "Epoch 206/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0928 - val_loss: 0.1264\n",
      "\n",
      "Epoch 00206: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 207/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0865 - val_loss: 0.1167\n",
      "Epoch 208/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0927 - val_loss: 0.1165\n",
      "Epoch 209/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0926 - val_loss: 0.1165\n",
      "Epoch 210/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0897 - val_loss: 0.1161\n",
      "Epoch 211/1000\n",
      "167/167 [==============================] - 3s 18ms/step - loss: 0.0902 - val_loss: 0.1172\n",
      "Epoch 212/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0869 - val_loss: 0.1163\n",
      "Epoch 213/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0863 - val_loss: 0.1160\n",
      "Epoch 214/1000\n",
      "167/167 [==============================] - 3s 18ms/step - loss: 0.0900 - val_loss: 0.1156\n",
      "Epoch 215/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0895 - val_loss: 0.1163\n",
      "Epoch 216/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0946 - val_loss: 0.1162\n",
      "Epoch 217/1000\n",
      "167/167 [==============================] - 3s 19ms/step - loss: 0.0852 - val_loss: 0.1161\n",
      "Epoch 218/1000\n",
      "167/167 [==============================] - 3s 18ms/step - loss: 0.0872 - val_loss: 0.1165\n",
      "Epoch 219/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0926 - val_loss: 0.1157\n",
      "Epoch 220/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0878 - val_loss: 0.1164\n",
      "Epoch 221/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0909 - val_loss: 0.1160\n",
      "Epoch 222/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0889 - val_loss: 0.1168\n",
      "Epoch 223/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0875 - val_loss: 0.1164\n",
      "Epoch 224/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0990 - val_loss: 0.1160\n",
      "Epoch 225/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0889 - val_loss: 0.1160\n",
      "Epoch 226/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0856 - val_loss: 0.1157\n",
      "Epoch 227/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0808 - val_loss: 0.1159\n",
      "Epoch 228/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0908 - val_loss: 0.1157\n",
      "Epoch 229/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0932 - val_loss: 0.1158\n",
      "Epoch 230/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0884 - val_loss: 0.1166\n",
      "Epoch 231/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0820 - val_loss: 0.1164\n",
      "Epoch 232/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0932 - val_loss: 0.1155\n",
      "Epoch 233/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0869 - val_loss: 0.1165\n",
      "Epoch 234/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0900 - val_loss: 0.1161\n",
      "Epoch 235/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0915 - val_loss: 0.1159\n",
      "Epoch 236/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0896 - val_loss: 0.1156\n",
      "Epoch 237/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0909 - val_loss: 0.1156\n",
      "Epoch 238/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0923 - val_loss: 0.1157\n",
      "Epoch 239/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0805 - val_loss: 0.1161\n",
      "Epoch 240/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0912 - val_loss: 0.1161\n",
      "Epoch 241/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0912 - val_loss: 0.1158\n",
      "Epoch 242/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0899 - val_loss: 0.1165\n",
      "Epoch 243/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0834 - val_loss: 0.1163\n",
      "Epoch 244/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0850 - val_loss: 0.1155\n",
      "Epoch 245/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0884 - val_loss: 0.1158\n",
      "Epoch 246/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0878 - val_loss: 0.1161\n",
      "Epoch 247/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0824 - val_loss: 0.1156\n",
      "Epoch 248/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0908 - val_loss: 0.1156\n",
      "Epoch 249/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0960 - val_loss: 0.1157\n",
      "Epoch 250/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0903 - val_loss: 0.1158\n",
      "Epoch 251/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0922 - val_loss: 0.1157\n",
      "Epoch 252/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0867 - val_loss: 0.1160\n",
      "Epoch 253/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0881 - val_loss: 0.1161\n",
      "Epoch 254/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0885 - val_loss: 0.1160\n",
      "Epoch 255/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0875 - val_loss: 0.1168\n",
      "Epoch 256/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0914 - val_loss: 0.1152\n",
      "Epoch 257/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0867 - val_loss: 0.1160\n",
      "Epoch 258/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0887 - val_loss: 0.1159\n",
      "Epoch 259/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0931 - val_loss: 0.1157\n",
      "Epoch 260/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0903 - val_loss: 0.1162\n",
      "Epoch 261/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0866 - val_loss: 0.1156\n",
      "Epoch 262/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0934 - val_loss: 0.1165\n",
      "Epoch 263/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0897 - val_loss: 0.1155\n",
      "Epoch 264/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0885 - val_loss: 0.1154\n",
      "Epoch 265/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0902 - val_loss: 0.1157\n",
      "Epoch 266/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0953 - val_loss: 0.1155\n",
      "Epoch 267/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0858 - val_loss: 0.1153\n",
      "Epoch 268/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0950 - val_loss: 0.1165\n",
      "Epoch 269/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0938 - val_loss: 0.1161\n",
      "Epoch 270/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0816 - val_loss: 0.1158\n",
      "Epoch 271/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0880 - val_loss: 0.1158\n",
      "Epoch 272/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0861 - val_loss: 0.1156\n",
      "Epoch 273/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0916 - val_loss: 0.1155\n",
      "Epoch 274/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0855 - val_loss: 0.1153\n",
      "Epoch 275/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0893 - val_loss: 0.1154\n",
      "Epoch 276/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0869 - val_loss: 0.1158\n",
      "Epoch 277/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0840 - val_loss: 0.1159\n",
      "Epoch 278/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0894 - val_loss: 0.1157\n",
      "Epoch 279/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0791 - val_loss: 0.1157\n",
      "Epoch 280/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0877 - val_loss: 0.1156\n",
      "Epoch 281/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0812 - val_loss: 0.1161\n",
      "Epoch 282/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0909 - val_loss: 0.1157\n",
      "Epoch 283/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0892 - val_loss: 0.1158\n",
      "Epoch 284/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.1019 - val_loss: 0.1159\n",
      "Epoch 285/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0840 - val_loss: 0.1154\n",
      "Epoch 286/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0860 - val_loss: 0.1156\n",
      "\n",
      "Epoch 00286: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 287/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0855 - val_loss: 0.1157\n",
      "Epoch 288/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0830 - val_loss: 0.1152\n",
      "Epoch 289/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0887 - val_loss: 0.1154\n",
      "Epoch 290/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0828 - val_loss: 0.1152\n",
      "Epoch 291/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0834 - val_loss: 0.1154\n",
      "Epoch 292/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0858 - val_loss: 0.1153\n",
      "Epoch 293/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0892 - val_loss: 0.1152\n",
      "Epoch 294/1000\n",
      "167/167 [==============================] - 3s 17ms/step - loss: 0.0853 - val_loss: 0.1155\n",
      "Epoch 295/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0906 - val_loss: 0.1152\n",
      "Epoch 296/1000\n",
      "167/167 [==============================] - 3s 16ms/step - loss: 0.0867 - val_loss: 0.1153\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00296: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.1613662\n",
      "Training 1JHN out of ['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN']\n",
      "Categories (8, object): ['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN'] \n",
      "\n",
      "Epoch 1/1000\n",
      "20/20 [==============================] - 2s 30ms/step - loss: 47.3451 - val_loss: 46.5478\n",
      "Epoch 2/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 46.3835 - val_loss: 44.8295\n",
      "Epoch 3/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 44.8197 - val_loss: 42.1449\n",
      "Epoch 4/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 42.3811 - val_loss: 38.3996\n",
      "Epoch 5/1000\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 38.9229 - val_loss: 33.5097\n",
      "Epoch 6/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 34.4468 - val_loss: 32.0762\n",
      "Epoch 7/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 29.0324 - val_loss: 49.7285\n",
      "Epoch 8/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 22.3671 - val_loss: 21.3446\n",
      "Epoch 9/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 14.6332 - val_loss: 9.9985\n",
      "Epoch 10/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 6.6146 - val_loss: 14.6366\n",
      "Epoch 11/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 1.7475 - val_loss: 10.8868\n",
      "Epoch 12/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 1.3946 - val_loss: 6.7055\n",
      "Epoch 13/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 1.0757 - val_loss: 4.4149\n",
      "Epoch 14/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 1.0380 - val_loss: 3.4311\n",
      "Epoch 15/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.9597 - val_loss: 3.2599\n",
      "Epoch 16/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.9797 - val_loss: 2.4317\n",
      "Epoch 17/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.9130 - val_loss: 2.3673\n",
      "Epoch 18/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.8567 - val_loss: 1.7692\n",
      "Epoch 19/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.8478 - val_loss: 1.4152\n",
      "Epoch 20/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.7517 - val_loss: 0.9958\n",
      "Epoch 21/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.7720 - val_loss: 1.1986\n",
      "Epoch 22/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.7101 - val_loss: 1.6083\n",
      "Epoch 23/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.8518 - val_loss: 1.1970\n",
      "Epoch 24/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.8313 - val_loss: 1.5409\n",
      "Epoch 25/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.6874 - val_loss: 1.1049\n",
      "Epoch 26/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.7612 - val_loss: 1.1412\n",
      "Epoch 27/1000\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 0.7740 - val_loss: 1.0012\n",
      "Epoch 28/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.6720 - val_loss: 1.3102\n",
      "Epoch 29/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.7163 - val_loss: 1.3189\n",
      "Epoch 30/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.7179 - val_loss: 1.1992\n",
      "Epoch 31/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.6513 - val_loss: 1.0875\n",
      "Epoch 32/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.6440 - val_loss: 0.9367\n",
      "Epoch 33/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.6344 - val_loss: 1.0502\n",
      "Epoch 34/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.6210 - val_loss: 1.4063\n",
      "Epoch 35/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.6670 - val_loss: 1.2041\n",
      "Epoch 36/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.6327 - val_loss: 1.1576\n",
      "Epoch 37/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.5879 - val_loss: 1.5199\n",
      "Epoch 38/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.6274 - val_loss: 1.2044\n",
      "Epoch 39/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.6051 - val_loss: 1.1540\n",
      "Epoch 40/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.6512 - val_loss: 0.9518\n",
      "Epoch 41/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.7068 - val_loss: 1.1107\n",
      "Epoch 42/1000\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 0.7072 - val_loss: 1.4173\n",
      "Epoch 43/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.6724 - val_loss: 1.5445\n",
      "Epoch 44/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.5543 - val_loss: 1.1233\n",
      "Epoch 45/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.6220 - val_loss: 1.2634\n",
      "Epoch 46/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.5982 - val_loss: 1.7303\n",
      "Epoch 47/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.6557 - val_loss: 1.2488\n",
      "Epoch 48/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.6922 - val_loss: 1.1334\n",
      "Epoch 49/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.5673 - val_loss: 1.3349\n",
      "Epoch 50/1000\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 0.6493 - val_loss: 1.2406\n",
      "Epoch 51/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.5636 - val_loss: 1.1482\n",
      "Epoch 52/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.5778 - val_loss: 1.3298\n",
      "Epoch 53/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.6364 - val_loss: 1.5595\n",
      "Epoch 54/1000\n",
      "20/20 [==============================] - 1s 26ms/step - loss: 0.6329 - val_loss: 1.1594\n",
      "Epoch 55/1000\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 0.5608 - val_loss: 1.0287\n",
      "Epoch 56/1000\n",
      "20/20 [==============================] - 1s 37ms/step - loss: 0.5200 - val_loss: 0.9756\n",
      "Epoch 57/1000\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 0.5255 - val_loss: 0.8709\n",
      "Epoch 58/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.5443 - val_loss: 0.8821\n",
      "Epoch 59/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.5434 - val_loss: 0.9917\n",
      "Epoch 60/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.6134 - val_loss: 1.2602\n",
      "Epoch 61/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.6845 - val_loss: 1.6005\n",
      "Epoch 62/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.6940 - val_loss: 1.0883\n",
      "Epoch 63/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.5888 - val_loss: 1.3952\n",
      "Epoch 64/1000\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 0.5934 - val_loss: 0.9670\n",
      "Epoch 65/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.6116 - val_loss: 0.9454\n",
      "Epoch 66/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.5781 - val_loss: 1.1065\n",
      "Epoch 67/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.6323 - val_loss: 1.0970\n",
      "Epoch 68/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.6903 - val_loss: 1.0651\n",
      "Epoch 69/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.5371 - val_loss: 1.0225\n",
      "Epoch 70/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.5688 - val_loss: 1.0261\n",
      "Epoch 71/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.5681 - val_loss: 0.9915\n",
      "Epoch 72/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.5408 - val_loss: 0.8198\n",
      "Epoch 73/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.5250 - val_loss: 1.3325\n",
      "Epoch 74/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.5838 - val_loss: 0.9734\n",
      "Epoch 75/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.6252 - val_loss: 1.3320\n",
      "Epoch 76/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.5403 - val_loss: 1.1093\n",
      "Epoch 77/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.5570 - val_loss: 1.2506\n",
      "Epoch 78/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.4737 - val_loss: 1.0275\n",
      "Epoch 79/1000\n",
      "20/20 [==============================] - 0s 25ms/step - loss: 0.5155 - val_loss: 0.8214\n",
      "Epoch 80/1000\n",
      "20/20 [==============================] - 0s 25ms/step - loss: 0.5187 - val_loss: 1.1867\n",
      "Epoch 81/1000\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 0.5945 - val_loss: 1.0610\n",
      "Epoch 82/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.5263 - val_loss: 0.8365\n",
      "Epoch 83/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.4976 - val_loss: 0.7812\n",
      "Epoch 84/1000\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 0.5236 - val_loss: 0.8641\n",
      "Epoch 85/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.4999 - val_loss: 0.7340\n",
      "Epoch 86/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.5434 - val_loss: 0.9941\n",
      "Epoch 87/1000\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 0.4991 - val_loss: 1.3030\n",
      "Epoch 88/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.5862 - val_loss: 0.9961\n",
      "Epoch 89/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.5282 - val_loss: 0.9615\n",
      "Epoch 90/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.5127 - val_loss: 1.2224\n",
      "Epoch 91/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.5548 - val_loss: 0.8115\n",
      "Epoch 92/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.5062 - val_loss: 1.0845\n",
      "Epoch 93/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.4664 - val_loss: 1.0800\n",
      "Epoch 94/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.4730 - val_loss: 0.7523\n",
      "Epoch 95/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.4786 - val_loss: 0.7518\n",
      "Epoch 96/1000\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 0.5523 - val_loss: 0.8608\n",
      "Epoch 97/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.4911 - val_loss: 1.0704\n",
      "Epoch 98/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.5314 - val_loss: 0.9416\n",
      "Epoch 99/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.5619 - val_loss: 0.8955\n",
      "Epoch 100/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.4961 - val_loss: 0.8882\n",
      "Epoch 101/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.4915 - val_loss: 0.8183\n",
      "Epoch 102/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.5305 - val_loss: 0.7044\n",
      "Epoch 103/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.5202 - val_loss: 0.8087\n",
      "Epoch 104/1000\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 0.5522 - val_loss: 0.9020\n",
      "Epoch 105/1000\n",
      "20/20 [==============================] - 1s 25ms/step - loss: 0.5620 - val_loss: 1.1952\n",
      "Epoch 106/1000\n",
      "20/20 [==============================] - 1s 29ms/step - loss: 0.5383 - val_loss: 0.8563\n",
      "Epoch 107/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.5438 - val_loss: 0.7649\n",
      "Epoch 108/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.6078 - val_loss: 1.0160\n",
      "Epoch 109/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.5624 - val_loss: 0.7373\n",
      "Epoch 110/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.5182 - val_loss: 0.9396\n",
      "Epoch 111/1000\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 0.4664 - val_loss: 0.5880\n",
      "Epoch 112/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.5037 - val_loss: 0.6677\n",
      "Epoch 113/1000\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 0.4898 - val_loss: 0.8084\n",
      "Epoch 114/1000\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 0.4609 - val_loss: 1.0642\n",
      "Epoch 115/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.5136 - val_loss: 0.6947\n",
      "Epoch 116/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.5249 - val_loss: 0.6695\n",
      "Epoch 117/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.5629 - val_loss: 1.1554\n",
      "Epoch 118/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.5425 - val_loss: 0.7373\n",
      "Epoch 119/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4862 - val_loss: 0.7613\n",
      "Epoch 120/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4409 - val_loss: 0.7435\n",
      "Epoch 121/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4881 - val_loss: 0.8651\n",
      "Epoch 122/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4881 - val_loss: 0.6551\n",
      "Epoch 123/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4345 - val_loss: 0.7722\n",
      "Epoch 124/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.4716 - val_loss: 0.6304\n",
      "Epoch 125/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4814 - val_loss: 0.6820\n",
      "Epoch 126/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4930 - val_loss: 0.8158\n",
      "Epoch 127/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4884 - val_loss: 0.8263\n",
      "Epoch 128/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4948 - val_loss: 0.7335\n",
      "Epoch 129/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4208 - val_loss: 0.5850\n",
      "Epoch 130/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.5302 - val_loss: 0.6107\n",
      "Epoch 131/1000\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 0.4834 - val_loss: 0.9015\n",
      "Epoch 132/1000\n",
      "20/20 [==============================] - 1s 28ms/step - loss: 0.5168 - val_loss: 0.7915\n",
      "Epoch 133/1000\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 0.4731 - val_loss: 0.9166\n",
      "Epoch 134/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.5030 - val_loss: 0.9802\n",
      "Epoch 135/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4879 - val_loss: 0.6966\n",
      "Epoch 136/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.5008 - val_loss: 0.9895\n",
      "Epoch 137/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4894 - val_loss: 0.7867\n",
      "Epoch 138/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4920 - val_loss: 0.9349\n",
      "Epoch 139/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4929 - val_loss: 0.9058\n",
      "Epoch 140/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.5244 - val_loss: 0.8587\n",
      "Epoch 141/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.5559 - val_loss: 0.7822\n",
      "Epoch 142/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.5621 - val_loss: 0.6528\n",
      "Epoch 143/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4459 - val_loss: 0.8773\n",
      "Epoch 144/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4340 - val_loss: 0.8546\n",
      "Epoch 145/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4244 - val_loss: 0.9670\n",
      "Epoch 146/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.5024 - val_loss: 0.7829\n",
      "Epoch 147/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4796 - val_loss: 0.7926\n",
      "Epoch 148/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4764 - val_loss: 0.7240\n",
      "Epoch 149/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4303 - val_loss: 0.7947\n",
      "Epoch 150/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4616 - val_loss: 0.7535\n",
      "Epoch 151/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4472 - val_loss: 0.8073\n",
      "Epoch 152/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.5124 - val_loss: 0.8258\n",
      "Epoch 153/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4410 - val_loss: 0.5172\n",
      "Epoch 154/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4815 - val_loss: 0.6766\n",
      "Epoch 155/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4474 - val_loss: 0.6191\n",
      "Epoch 156/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.5219 - val_loss: 0.5811\n",
      "Epoch 157/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4926 - val_loss: 0.7564\n",
      "Epoch 158/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4894 - val_loss: 0.7310\n",
      "Epoch 159/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4884 - val_loss: 0.6026\n",
      "Epoch 160/1000\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 0.4945 - val_loss: 0.8131\n",
      "Epoch 161/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.4381 - val_loss: 0.7998\n",
      "Epoch 162/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.4147 - val_loss: 0.8668\n",
      "Epoch 163/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.5873 - val_loss: 0.9012\n",
      "Epoch 164/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4564 - val_loss: 0.9563\n",
      "Epoch 165/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.5337 - val_loss: 1.0695\n",
      "Epoch 166/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4676 - val_loss: 0.8257\n",
      "Epoch 167/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4227 - val_loss: 0.7566\n",
      "Epoch 168/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4659 - val_loss: 0.8215\n",
      "Epoch 169/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4969 - val_loss: 0.8827\n",
      "Epoch 170/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4270 - val_loss: 0.7582\n",
      "Epoch 171/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4654 - val_loss: 0.6447\n",
      "Epoch 172/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4747 - val_loss: 0.7838\n",
      "Epoch 173/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4796 - val_loss: 1.2261\n",
      "Epoch 174/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.5049 - val_loss: 0.9249\n",
      "Epoch 175/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4272 - val_loss: 0.7867\n",
      "Epoch 176/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.5397 - val_loss: 1.0098\n",
      "Epoch 177/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4950 - val_loss: 1.1348\n",
      "Epoch 178/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4389 - val_loss: 0.7412\n",
      "Epoch 179/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3773 - val_loss: 0.7670\n",
      "Epoch 180/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4236 - val_loss: 0.7210\n",
      "Epoch 181/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4479 - val_loss: 0.7359\n",
      "Epoch 182/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4970 - val_loss: 0.8566\n",
      "Epoch 183/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.4834 - val_loss: 0.9121\n",
      "\n",
      "Epoch 00183: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 184/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3933 - val_loss: 0.5530\n",
      "Epoch 185/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3387 - val_loss: 0.4846\n",
      "Epoch 186/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3170 - val_loss: 0.4323\n",
      "Epoch 187/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3013 - val_loss: 0.4145\n",
      "Epoch 188/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3067 - val_loss: 0.4157\n",
      "Epoch 189/1000\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 0.2976 - val_loss: 0.3710\n",
      "Epoch 190/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3254 - val_loss: 0.4016\n",
      "Epoch 191/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3206 - val_loss: 0.3879\n",
      "Epoch 192/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3196 - val_loss: 0.3651\n",
      "Epoch 193/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3042 - val_loss: 0.3658\n",
      "Epoch 194/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3443 - val_loss: 0.3692\n",
      "Epoch 195/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3078 - val_loss: 0.3965\n",
      "Epoch 196/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3189 - val_loss: 0.3676\n",
      "Epoch 197/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3192 - val_loss: 0.4136\n",
      "Epoch 198/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.3228 - val_loss: 0.3584\n",
      "Epoch 199/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3218 - val_loss: 0.3765\n",
      "Epoch 200/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3023 - val_loss: 0.3617\n",
      "Epoch 201/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2908 - val_loss: 0.3975\n",
      "Epoch 202/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2971 - val_loss: 0.3634\n",
      "Epoch 203/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2962 - val_loss: 0.3742\n",
      "Epoch 204/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2820 - val_loss: 0.3656\n",
      "Epoch 205/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3638 - val_loss: 0.3666\n",
      "Epoch 206/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2674 - val_loss: 0.3903\n",
      "Epoch 207/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2842 - val_loss: 0.3654\n",
      "Epoch 208/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3601 - val_loss: 0.3750\n",
      "Epoch 209/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3065 - val_loss: 0.3548\n",
      "Epoch 210/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3478 - val_loss: 0.3574\n",
      "Epoch 211/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3034 - val_loss: 0.3562\n",
      "Epoch 212/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2984 - val_loss: 0.3548\n",
      "Epoch 213/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3472 - val_loss: 0.3717\n",
      "Epoch 214/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3144 - val_loss: 0.3780\n",
      "Epoch 215/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2763 - val_loss: 0.3747\n",
      "Epoch 216/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.3371 - val_loss: 0.3562\n",
      "Epoch 217/1000\n",
      "20/20 [==============================] - 0s 25ms/step - loss: 0.3104 - val_loss: 0.3559\n",
      "Epoch 218/1000\n",
      "20/20 [==============================] - 1s 30ms/step - loss: 0.2743 - val_loss: 0.3713\n",
      "Epoch 219/1000\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 0.3364 - val_loss: 0.3797\n",
      "Epoch 220/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3009 - val_loss: 0.3802\n",
      "Epoch 221/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3267 - val_loss: 0.3527\n",
      "Epoch 222/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2781 - val_loss: 0.3585\n",
      "Epoch 223/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2920 - val_loss: 0.3669\n",
      "Epoch 224/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2943 - val_loss: 0.3515\n",
      "Epoch 225/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2673 - val_loss: 0.3635\n",
      "Epoch 226/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3164 - val_loss: 0.3925\n",
      "Epoch 227/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3406 - val_loss: 0.3627\n",
      "Epoch 228/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3446 - val_loss: 0.3917\n",
      "Epoch 229/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3547 - val_loss: 0.3759\n",
      "Epoch 230/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3052 - val_loss: 0.4538\n",
      "Epoch 231/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2925 - val_loss: 0.3470\n",
      "Epoch 232/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2756 - val_loss: 0.3846\n",
      "Epoch 233/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3422 - val_loss: 0.3610\n",
      "Epoch 234/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2647 - val_loss: 0.3595\n",
      "Epoch 235/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2922 - val_loss: 0.3484\n",
      "Epoch 236/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2664 - val_loss: 0.3619\n",
      "Epoch 237/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3956 - val_loss: 0.3750\n",
      "Epoch 238/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3293 - val_loss: 0.3605\n",
      "Epoch 239/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2763 - val_loss: 0.3757\n",
      "Epoch 240/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3202 - val_loss: 0.3724\n",
      "Epoch 241/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2940 - val_loss: 0.4021\n",
      "Epoch 242/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3181 - val_loss: 0.3630\n",
      "Epoch 243/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2684 - val_loss: 0.3792\n",
      "Epoch 244/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.2903 - val_loss: 0.3588\n",
      "Epoch 245/1000\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 0.3254 - val_loss: 0.3887\n",
      "Epoch 246/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.3540 - val_loss: 0.4066\n",
      "Epoch 247/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2799 - val_loss: 0.3640\n",
      "Epoch 248/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3141 - val_loss: 0.3832\n",
      "Epoch 249/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3045 - val_loss: 0.3896\n",
      "Epoch 250/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3188 - val_loss: 0.4076\n",
      "Epoch 251/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3179 - val_loss: 0.3901\n",
      "Epoch 252/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3056 - val_loss: 0.3877\n",
      "Epoch 253/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2969 - val_loss: 0.3720\n",
      "Epoch 254/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2979 - val_loss: 0.3607\n",
      "Epoch 255/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2954 - val_loss: 0.3766\n",
      "Epoch 256/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3300 - val_loss: 0.3691\n",
      "Epoch 257/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2682 - val_loss: 0.3762\n",
      "Epoch 258/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3249 - val_loss: 0.3850\n",
      "Epoch 259/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3017 - val_loss: 0.3581\n",
      "Epoch 260/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2632 - val_loss: 0.3639\n",
      "Epoch 261/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2667 - val_loss: 0.3868\n",
      "\n",
      "Epoch 00261: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 262/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3637 - val_loss: 0.3527\n",
      "Epoch 263/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2543 - val_loss: 0.3427\n",
      "Epoch 264/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3060 - val_loss: 0.3394\n",
      "Epoch 265/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3032 - val_loss: 0.3377\n",
      "Epoch 266/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2950 - val_loss: 0.3372\n",
      "Epoch 267/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2767 - val_loss: 0.3367\n",
      "Epoch 268/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3016 - val_loss: 0.3363\n",
      "Epoch 269/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2678 - val_loss: 0.3367\n",
      "Epoch 270/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3106 - val_loss: 0.3348\n",
      "Epoch 271/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2853 - val_loss: 0.3373\n",
      "Epoch 272/1000\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 0.2757 - val_loss: 0.3355\n",
      "Epoch 273/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.3647 - val_loss: 0.3354\n",
      "Epoch 274/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3170 - val_loss: 0.3348\n",
      "Epoch 275/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2371 - val_loss: 0.3348\n",
      "Epoch 276/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.2545 - val_loss: 0.3361\n",
      "Epoch 277/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2954 - val_loss: 0.3349\n",
      "Epoch 278/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3162 - val_loss: 0.3353\n",
      "Epoch 279/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3099 - val_loss: 0.3348\n",
      "Epoch 280/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2898 - val_loss: 0.3361\n",
      "Epoch 281/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2926 - val_loss: 0.3396\n",
      "Epoch 282/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3254 - val_loss: 0.3365\n",
      "Epoch 283/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2287 - val_loss: 0.3365\n",
      "Epoch 284/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2854 - val_loss: 0.3364\n",
      "Epoch 285/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3179 - val_loss: 0.3368\n",
      "Epoch 286/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2588 - val_loss: 0.3353\n",
      "Epoch 287/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2950 - val_loss: 0.3333\n",
      "Epoch 288/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2628 - val_loss: 0.3364\n",
      "Epoch 289/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2338 - val_loss: 0.3347\n",
      "Epoch 290/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2865 - val_loss: 0.3344\n",
      "Epoch 291/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2698 - val_loss: 0.3327\n",
      "Epoch 292/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2565 - val_loss: 0.3330\n",
      "Epoch 293/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2571 - val_loss: 0.3344\n",
      "Epoch 294/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2588 - val_loss: 0.3354\n",
      "Epoch 295/1000\n",
      "20/20 [==============================] - 0s 17ms/step - loss: 0.2785 - val_loss: 0.3346\n",
      "Epoch 296/1000\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 0.2965 - val_loss: 0.3357\n",
      "Epoch 297/1000\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 0.2695 - val_loss: 0.3364\n",
      "Epoch 298/1000\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 0.2694 - val_loss: 0.3343\n",
      "Epoch 299/1000\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 0.2427 - val_loss: 0.3359\n",
      "Epoch 300/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2595 - val_loss: 0.3341\n",
      "Epoch 301/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.2733 - val_loss: 0.3366\n",
      "Epoch 302/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3087 - val_loss: 0.3360\n",
      "Epoch 303/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2549 - val_loss: 0.3351\n",
      "Epoch 304/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2988 - val_loss: 0.3366\n",
      "Epoch 305/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2726 - val_loss: 0.3470\n",
      "Epoch 306/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2506 - val_loss: 0.3354\n",
      "Epoch 307/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2803 - val_loss: 0.3346\n",
      "Epoch 308/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3613 - val_loss: 0.3393\n",
      "Epoch 309/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2803 - val_loss: 0.3352\n",
      "Epoch 310/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3232 - val_loss: 0.3367\n",
      "Epoch 311/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3056 - val_loss: 0.3339\n",
      "Epoch 312/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3196 - val_loss: 0.3365\n",
      "Epoch 313/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3299 - val_loss: 0.3348\n",
      "Epoch 314/1000\n",
      "20/20 [==============================] - 0s 19ms/step - loss: 0.3003 - val_loss: 0.3340\n",
      "Epoch 315/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2699 - val_loss: 0.3359\n",
      "Epoch 316/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2713 - val_loss: 0.3350\n",
      "Epoch 317/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2704 - val_loss: 0.3373\n",
      "Epoch 318/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3039 - val_loss: 0.3344\n",
      "Epoch 319/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2589 - val_loss: 0.3332\n",
      "Epoch 320/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2976 - val_loss: 0.3343\n",
      "Epoch 321/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3004 - val_loss: 0.3342\n",
      "\n",
      "Epoch 00321: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 322/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3015 - val_loss: 0.3340\n",
      "Epoch 323/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2568 - val_loss: 0.3335\n",
      "Epoch 324/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2565 - val_loss: 0.3333\n",
      "Epoch 325/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2880 - val_loss: 0.3332\n",
      "Epoch 326/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2501 - val_loss: 0.3336\n",
      "Epoch 327/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.3406 - val_loss: 0.3340\n",
      "Epoch 328/1000\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 0.2427 - val_loss: 0.3346\n",
      "Epoch 329/1000\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 0.2632 - val_loss: 0.3341\n",
      "Epoch 330/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2848 - val_loss: 0.3343\n",
      "Epoch 331/1000\n",
      "20/20 [==============================] - 0s 18ms/step - loss: 0.2801 - val_loss: 0.3342\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00331: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.1003686\n",
      "Training 2JHN out of ['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN']\n",
      "Categories (8, object): ['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN'] \n",
      "\n",
      "Epoch 1/1000\n",
      "53/53 [==============================] - 3s 23ms/step - loss: 2.9049 - val_loss: 2.1905\n",
      "Epoch 2/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.9254 - val_loss: 1.6068\n",
      "Epoch 3/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.4400 - val_loss: 1.3975\n",
      "Epoch 4/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.3814 - val_loss: 1.1870\n",
      "Epoch 5/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.3336 - val_loss: 0.8408\n",
      "Epoch 6/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.3149 - val_loss: 0.6543\n",
      "Epoch 7/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.2903 - val_loss: 0.4792\n",
      "Epoch 8/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.2899 - val_loss: 0.4240\n",
      "Epoch 9/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.2735 - val_loss: 0.3230\n",
      "Epoch 10/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.2706 - val_loss: 0.3181\n",
      "Epoch 11/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.2717 - val_loss: 0.3044\n",
      "Epoch 12/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.2520 - val_loss: 0.3078\n",
      "Epoch 13/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.2377 - val_loss: 0.2698\n",
      "Epoch 14/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.2407 - val_loss: 0.3314\n",
      "Epoch 15/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.2490 - val_loss: 0.4044\n",
      "Epoch 16/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.2461 - val_loss: 0.3079\n",
      "Epoch 17/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.2329 - val_loss: 0.2969\n",
      "Epoch 18/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.2271 - val_loss: 0.3373\n",
      "Epoch 19/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.2248 - val_loss: 0.2483\n",
      "Epoch 20/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.2132 - val_loss: 0.3093\n",
      "Epoch 21/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.2268 - val_loss: 0.2525\n",
      "Epoch 22/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.2304 - val_loss: 0.2846\n",
      "Epoch 23/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.2146 - val_loss: 0.2446\n",
      "Epoch 24/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.2196 - val_loss: 0.2867\n",
      "Epoch 25/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.2220 - val_loss: 0.3100\n",
      "Epoch 26/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.2185 - val_loss: 0.3171\n",
      "Epoch 27/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.2209 - val_loss: 0.2273\n",
      "Epoch 28/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.2082 - val_loss: 0.2125\n",
      "Epoch 29/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.1943 - val_loss: 0.2513\n",
      "Epoch 30/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.2027 - val_loss: 0.2563\n",
      "Epoch 31/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1995 - val_loss: 0.2451\n",
      "Epoch 32/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.2086 - val_loss: 0.2836\n",
      "Epoch 33/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1959 - val_loss: 0.2465\n",
      "Epoch 34/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1979 - val_loss: 0.2359\n",
      "Epoch 35/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1895 - val_loss: 0.2531\n",
      "Epoch 36/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1827 - val_loss: 0.2036\n",
      "Epoch 37/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.2025 - val_loss: 0.2053\n",
      "Epoch 38/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.1957 - val_loss: 0.2144\n",
      "Epoch 39/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1863 - val_loss: 0.2351\n",
      "Epoch 40/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.2002 - val_loss: 0.2295\n",
      "Epoch 41/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1916 - val_loss: 0.1991\n",
      "Epoch 42/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1824 - val_loss: 0.2145\n",
      "Epoch 43/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1723 - val_loss: 0.2417\n",
      "Epoch 44/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1905 - val_loss: 0.2293\n",
      "Epoch 45/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1801 - val_loss: 0.2387\n",
      "Epoch 46/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1874 - val_loss: 0.2528\n",
      "Epoch 47/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1988 - val_loss: 0.2016\n",
      "Epoch 48/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1724 - val_loss: 0.2070\n",
      "Epoch 49/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.1713 - val_loss: 0.2362\n",
      "Epoch 50/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.1761 - val_loss: 0.2024\n",
      "Epoch 51/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1768 - val_loss: 0.2193\n",
      "Epoch 52/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.1766 - val_loss: 0.2053\n",
      "Epoch 53/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.1745 - val_loss: 0.2340\n",
      "Epoch 54/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.1772 - val_loss: 0.1846\n",
      "Epoch 55/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1643 - val_loss: 0.2120\n",
      "Epoch 56/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1697 - val_loss: 0.2155\n",
      "Epoch 57/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1680 - val_loss: 0.2185\n",
      "Epoch 58/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1717 - val_loss: 0.1978\n",
      "Epoch 59/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1570 - val_loss: 0.2169\n",
      "Epoch 60/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1625 - val_loss: 0.2025\n",
      "Epoch 61/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1685 - val_loss: 0.1840\n",
      "Epoch 62/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1768 - val_loss: 0.1891\n",
      "Epoch 63/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1711 - val_loss: 0.1890\n",
      "Epoch 64/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1632 - val_loss: 0.2122\n",
      "Epoch 65/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1842 - val_loss: 0.2203\n",
      "Epoch 66/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1593 - val_loss: 0.1745\n",
      "Epoch 67/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1561 - val_loss: 0.2068\n",
      "Epoch 68/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1586 - val_loss: 0.2143\n",
      "Epoch 69/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1733 - val_loss: 0.1868\n",
      "Epoch 70/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1558 - val_loss: 0.1924\n",
      "Epoch 71/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1539 - val_loss: 0.1922\n",
      "Epoch 72/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1535 - val_loss: 0.2136\n",
      "Epoch 73/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1567 - val_loss: 0.1873\n",
      "Epoch 74/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1598 - val_loss: 0.2040\n",
      "Epoch 75/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.1607 - val_loss: 0.1991\n",
      "Epoch 76/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1589 - val_loss: 0.1888\n",
      "Epoch 77/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1515 - val_loss: 0.1950\n",
      "Epoch 78/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1547 - val_loss: 0.1975\n",
      "Epoch 79/1000\n",
      "53/53 [==============================] - 1s 19ms/step - loss: 0.1573 - val_loss: 0.1784\n",
      "Epoch 80/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1547 - val_loss: 0.1835\n",
      "Epoch 81/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1541 - val_loss: 0.1791\n",
      "Epoch 82/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1684 - val_loss: 0.1851\n",
      "Epoch 83/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1522 - val_loss: 0.1952\n",
      "Epoch 84/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1693 - val_loss: 0.1882\n",
      "Epoch 85/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1529 - val_loss: 0.2017\n",
      "Epoch 86/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1446 - val_loss: 0.1943\n",
      "Epoch 87/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1505 - val_loss: 0.1866\n",
      "Epoch 88/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1645 - val_loss: 0.1976\n",
      "Epoch 89/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1566 - val_loss: 0.2100\n",
      "Epoch 90/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1514 - val_loss: 0.1850\n",
      "Epoch 91/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1555 - val_loss: 0.1839\n",
      "Epoch 92/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1566 - val_loss: 0.1876\n",
      "Epoch 93/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1582 - val_loss: 0.1738\n",
      "Epoch 94/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1581 - val_loss: 0.1818\n",
      "Epoch 95/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1435 - val_loss: 0.2000\n",
      "Epoch 96/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1450 - val_loss: 0.1873\n",
      "Epoch 97/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1368 - val_loss: 0.1765\n",
      "Epoch 98/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1408 - val_loss: 0.1803\n",
      "Epoch 99/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1416 - val_loss: 0.1766\n",
      "Epoch 100/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1466 - val_loss: 0.2058\n",
      "Epoch 101/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1533 - val_loss: 0.1904\n",
      "Epoch 102/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1530 - val_loss: 0.2386\n",
      "Epoch 103/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1590 - val_loss: 0.1820\n",
      "Epoch 104/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1402 - val_loss: 0.1731\n",
      "Epoch 105/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1473 - val_loss: 0.1860\n",
      "Epoch 106/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1383 - val_loss: 0.1730\n",
      "Epoch 107/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1472 - val_loss: 0.1786\n",
      "Epoch 108/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.1567 - val_loss: 0.1762\n",
      "Epoch 109/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1407 - val_loss: 0.1797\n",
      "Epoch 110/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1349 - val_loss: 0.1699\n",
      "Epoch 111/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1363 - val_loss: 0.1624\n",
      "Epoch 112/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1377 - val_loss: 0.1692\n",
      "Epoch 113/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1447 - val_loss: 0.1781\n",
      "Epoch 114/1000\n",
      "53/53 [==============================] - 1s 19ms/step - loss: 0.1362 - val_loss: 0.1813\n",
      "Epoch 115/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1365 - val_loss: 0.2154\n",
      "Epoch 116/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1449 - val_loss: 0.1763\n",
      "Epoch 117/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.1450 - val_loss: 0.1663\n",
      "Epoch 118/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1410 - val_loss: 0.1948\n",
      "Epoch 119/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1572 - val_loss: 0.1776\n",
      "Epoch 120/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1332 - val_loss: 0.1693\n",
      "Epoch 121/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1409 - val_loss: 0.1641\n",
      "Epoch 122/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1323 - val_loss: 0.1737\n",
      "Epoch 123/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1383 - val_loss: 0.1911\n",
      "Epoch 124/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1370 - val_loss: 0.1622\n",
      "Epoch 125/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1347 - val_loss: 0.1685\n",
      "Epoch 126/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1354 - val_loss: 0.1572\n",
      "Epoch 127/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1303 - val_loss: 0.1711\n",
      "Epoch 128/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1406 - val_loss: 0.1605\n",
      "Epoch 129/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1352 - val_loss: 0.1627\n",
      "Epoch 130/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1312 - val_loss: 0.1678\n",
      "Epoch 131/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1354 - val_loss: 0.1593\n",
      "Epoch 132/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1261 - val_loss: 0.1574\n",
      "Epoch 133/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1518 - val_loss: 0.1571\n",
      "Epoch 134/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1235 - val_loss: 0.1656\n",
      "Epoch 135/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1495 - val_loss: 0.1811\n",
      "Epoch 136/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1361 - val_loss: 0.1831\n",
      "Epoch 137/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1407 - val_loss: 0.1542\n",
      "Epoch 138/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.1390 - val_loss: 0.1641\n",
      "Epoch 139/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1411 - val_loss: 0.1747\n",
      "Epoch 140/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1314 - val_loss: 0.1640\n",
      "Epoch 141/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.1338 - val_loss: 0.1747\n",
      "Epoch 142/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1346 - val_loss: 0.1592\n",
      "Epoch 143/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1356 - val_loss: 0.1636\n",
      "Epoch 144/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1436 - val_loss: 0.1599\n",
      "Epoch 145/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1308 - val_loss: 0.1573\n",
      "Epoch 146/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1316 - val_loss: 0.1741\n",
      "Epoch 147/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1490 - val_loss: 0.2077\n",
      "Epoch 148/1000\n",
      "53/53 [==============================] - 1s 19ms/step - loss: 0.1435 - val_loss: 0.1716\n",
      "Epoch 149/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1293 - val_loss: 0.1967\n",
      "Epoch 150/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.1490 - val_loss: 0.1859\n",
      "Epoch 151/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.1320 - val_loss: 0.1630\n",
      "Epoch 152/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.1259 - val_loss: 0.1728\n",
      "Epoch 153/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1361 - val_loss: 0.1529\n",
      "Epoch 154/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1155 - val_loss: 0.2037\n",
      "Epoch 155/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1374 - val_loss: 0.1690\n",
      "Epoch 156/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1252 - val_loss: 0.1604\n",
      "Epoch 157/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1275 - val_loss: 0.1664\n",
      "Epoch 158/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1391 - val_loss: 0.1644\n",
      "Epoch 159/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1347 - val_loss: 0.1698\n",
      "Epoch 160/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1207 - val_loss: 0.1686\n",
      "Epoch 161/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1229 - val_loss: 0.1588\n",
      "Epoch 162/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1192 - val_loss: 0.1621\n",
      "Epoch 163/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1211 - val_loss: 0.1620\n",
      "Epoch 164/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1260 - val_loss: 0.1729\n",
      "Epoch 165/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1246 - val_loss: 0.1624\n",
      "Epoch 166/1000\n",
      "53/53 [==============================] - 1s 23ms/step - loss: 0.1334 - val_loss: 0.1652\n",
      "Epoch 167/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1245 - val_loss: 0.1665\n",
      "Epoch 168/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1243 - val_loss: 0.1761\n",
      "Epoch 169/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1329 - val_loss: 0.1714\n",
      "Epoch 170/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1387 - val_loss: 0.1542\n",
      "Epoch 171/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1322 - val_loss: 0.1553\n",
      "Epoch 172/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1249 - val_loss: 0.1669\n",
      "Epoch 173/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1281 - val_loss: 0.1653\n",
      "Epoch 174/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1265 - val_loss: 0.1517\n",
      "Epoch 175/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1498 - val_loss: 0.1668\n",
      "Epoch 176/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1249 - val_loss: 0.1755\n",
      "Epoch 177/1000\n",
      "53/53 [==============================] - 1s 21ms/step - loss: 0.1237 - val_loss: 0.1514\n",
      "Epoch 178/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1266 - val_loss: 0.1501\n",
      "Epoch 179/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1252 - val_loss: 0.1521\n",
      "Epoch 180/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1313 - val_loss: 0.1584\n",
      "Epoch 181/1000\n",
      "53/53 [==============================] - 1s 19ms/step - loss: 0.1243 - val_loss: 0.1663\n",
      "Epoch 182/1000\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 0.1284 - val_loss: 0.1521\n",
      "Epoch 183/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1209 - val_loss: 0.1677\n",
      "Epoch 184/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1414 - val_loss: 0.1764\n",
      "Epoch 185/1000\n",
      "53/53 [==============================] - 1s 19ms/step - loss: 0.1216 - val_loss: 0.1658\n",
      "Epoch 186/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1275 - val_loss: 0.1523\n",
      "Epoch 187/1000\n",
      "53/53 [==============================] - 1s 21ms/step - loss: 0.1248 - val_loss: 0.1553\n",
      "Epoch 188/1000\n",
      "53/53 [==============================] - 1s 21ms/step - loss: 0.1170 - val_loss: 0.1747\n",
      "Epoch 189/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1186 - val_loss: 0.1682\n",
      "Epoch 190/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1301 - val_loss: 0.1455\n",
      "Epoch 191/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1113 - val_loss: 0.1540\n",
      "Epoch 192/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1293 - val_loss: 0.1531\n",
      "Epoch 193/1000\n",
      "53/53 [==============================] - 1s 19ms/step - loss: 0.1244 - val_loss: 0.1503\n",
      "Epoch 194/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1188 - val_loss: 0.1453\n",
      "Epoch 195/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.1150 - val_loss: 0.1434\n",
      "Epoch 196/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1263 - val_loss: 0.1575\n",
      "Epoch 197/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1138 - val_loss: 0.1536\n",
      "Epoch 198/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1156 - val_loss: 0.1656\n",
      "Epoch 199/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.1171 - val_loss: 0.1469\n",
      "Epoch 200/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1292 - val_loss: 0.2012\n",
      "Epoch 201/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1313 - val_loss: 0.1566\n",
      "Epoch 202/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1256 - val_loss: 0.1803\n",
      "Epoch 203/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1210 - val_loss: 0.1506\n",
      "Epoch 204/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1223 - val_loss: 0.1510\n",
      "Epoch 205/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1188 - val_loss: 0.1586\n",
      "Epoch 206/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1198 - val_loss: 0.1562\n",
      "Epoch 207/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1122 - val_loss: 0.1495\n",
      "Epoch 208/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1166 - val_loss: 0.1668\n",
      "Epoch 209/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1288 - val_loss: 0.1461\n",
      "Epoch 210/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1289 - val_loss: 0.1590\n",
      "Epoch 211/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1132 - val_loss: 0.1577\n",
      "Epoch 212/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1108 - val_loss: 0.1597\n",
      "Epoch 213/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1259 - val_loss: 0.1781\n",
      "Epoch 214/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1246 - val_loss: 0.1597\n",
      "Epoch 215/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1231 - val_loss: 0.1676\n",
      "Epoch 216/1000\n",
      "53/53 [==============================] - 1s 19ms/step - loss: 0.1251 - val_loss: 0.1430\n",
      "Epoch 217/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1129 - val_loss: 0.1575\n",
      "Epoch 218/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.1224 - val_loss: 0.1505\n",
      "Epoch 219/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1167 - val_loss: 0.1380\n",
      "Epoch 220/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1128 - val_loss: 0.1735\n",
      "Epoch 221/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1250 - val_loss: 0.1417\n",
      "Epoch 222/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0993 - val_loss: 0.1551\n",
      "Epoch 223/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.1125 - val_loss: 0.1497\n",
      "Epoch 224/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.1180 - val_loss: 0.1535\n",
      "Epoch 225/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.1180 - val_loss: 0.1447\n",
      "Epoch 226/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1196 - val_loss: 0.1629\n",
      "Epoch 227/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1174 - val_loss: 0.1546\n",
      "Epoch 228/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.1117 - val_loss: 0.1618\n",
      "Epoch 229/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1041 - val_loss: 0.1418\n",
      "Epoch 230/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1220 - val_loss: 0.1479\n",
      "Epoch 231/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1128 - val_loss: 0.1435\n",
      "Epoch 232/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1212 - val_loss: 0.1556\n",
      "Epoch 233/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1266 - val_loss: 0.1451\n",
      "Epoch 234/1000\n",
      "53/53 [==============================] - 1s 19ms/step - loss: 0.1110 - val_loss: 0.1435\n",
      "Epoch 235/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1164 - val_loss: 0.1462\n",
      "Epoch 236/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1077 - val_loss: 0.1605\n",
      "Epoch 237/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1161 - val_loss: 0.1442\n",
      "Epoch 238/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1049 - val_loss: 0.1379\n",
      "Epoch 239/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1133 - val_loss: 0.1459\n",
      "Epoch 240/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1216 - val_loss: 0.1415\n",
      "Epoch 241/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1125 - val_loss: 0.1591\n",
      "Epoch 242/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1195 - val_loss: 0.1565\n",
      "Epoch 243/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1091 - val_loss: 0.1506\n",
      "Epoch 244/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1102 - val_loss: 0.1425\n",
      "Epoch 245/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1224 - val_loss: 0.1602\n",
      "Epoch 246/1000\n",
      "53/53 [==============================] - 1s 19ms/step - loss: 0.1223 - val_loss: 0.1615\n",
      "Epoch 247/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1219 - val_loss: 0.1650\n",
      "Epoch 248/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1319 - val_loss: 0.1399\n",
      "Epoch 249/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1098 - val_loss: 0.1595\n",
      "Epoch 250/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1166 - val_loss: 0.1433\n",
      "Epoch 251/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1094 - val_loss: 0.1595\n",
      "Epoch 252/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1235 - val_loss: 0.1465\n",
      "Epoch 253/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1150 - val_loss: 0.1644\n",
      "Epoch 254/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1230 - val_loss: 0.1408\n",
      "Epoch 255/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1112 - val_loss: 0.1511\n",
      "Epoch 256/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1279 - val_loss: 0.1594\n",
      "Epoch 257/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1184 - val_loss: 0.1378\n",
      "Epoch 258/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.1035 - val_loss: 0.1575\n",
      "Epoch 259/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1130 - val_loss: 0.1479\n",
      "Epoch 260/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1043 - val_loss: 0.1934\n",
      "Epoch 261/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1160 - val_loss: 0.1543\n",
      "Epoch 262/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1150 - val_loss: 0.1431\n",
      "Epoch 263/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1124 - val_loss: 0.1428\n",
      "Epoch 264/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1197 - val_loss: 0.1529\n",
      "Epoch 265/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1064 - val_loss: 0.1436\n",
      "Epoch 266/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1076 - val_loss: 0.1479\n",
      "Epoch 267/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1213 - val_loss: 0.1472\n",
      "Epoch 268/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1156 - val_loss: 0.1534\n",
      "\n",
      "Epoch 00268: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 269/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1116 - val_loss: 0.1213\n",
      "Epoch 270/1000\n",
      "53/53 [==============================] - 1s 19ms/step - loss: 0.0867 - val_loss: 0.1201\n",
      "Epoch 271/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0923 - val_loss: 0.1209\n",
      "Epoch 272/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0847 - val_loss: 0.1198\n",
      "Epoch 273/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0712 - val_loss: 0.1180\n",
      "Epoch 274/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0881 - val_loss: 0.1182\n",
      "Epoch 275/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0830 - val_loss: 0.1177\n",
      "Epoch 276/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0778 - val_loss: 0.1190\n",
      "Epoch 277/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0813 - val_loss: 0.1172\n",
      "Epoch 278/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1001 - val_loss: 0.1184\n",
      "Epoch 279/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0972 - val_loss: 0.1184\n",
      "Epoch 280/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0797 - val_loss: 0.1175\n",
      "Epoch 281/1000\n",
      "53/53 [==============================] - 1s 19ms/step - loss: 0.0778 - val_loss: 0.1168\n",
      "Epoch 282/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0794 - val_loss: 0.1161\n",
      "Epoch 283/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0751 - val_loss: 0.1165\n",
      "Epoch 284/1000\n",
      "53/53 [==============================] - 1s 19ms/step - loss: 0.0882 - val_loss: 0.1171\n",
      "Epoch 285/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0927 - val_loss: 0.1171\n",
      "Epoch 286/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0839 - val_loss: 0.1209\n",
      "Epoch 287/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0817 - val_loss: 0.1180\n",
      "Epoch 288/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0887 - val_loss: 0.1182\n",
      "Epoch 289/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0870 - val_loss: 0.1205\n",
      "Epoch 290/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0974 - val_loss: 0.1192\n",
      "Epoch 291/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0918 - val_loss: 0.1155\n",
      "Epoch 292/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0748 - val_loss: 0.1174\n",
      "Epoch 293/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0719 - val_loss: 0.1158\n",
      "Epoch 294/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0786 - val_loss: 0.1200\n",
      "Epoch 295/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0786 - val_loss: 0.1177\n",
      "Epoch 296/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0930 - val_loss: 0.1187\n",
      "Epoch 297/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0838 - val_loss: 0.1180\n",
      "Epoch 298/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.1001 - val_loss: 0.1178\n",
      "Epoch 299/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0742 - val_loss: 0.1195\n",
      "Epoch 300/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0843 - val_loss: 0.1187\n",
      "Epoch 301/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0714 - val_loss: 0.1183\n",
      "Epoch 302/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0829 - val_loss: 0.1165\n",
      "Epoch 303/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0844 - val_loss: 0.1151\n",
      "Epoch 304/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0954 - val_loss: 0.1171\n",
      "Epoch 305/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0777 - val_loss: 0.1177\n",
      "Epoch 306/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0724 - val_loss: 0.1180\n",
      "Epoch 307/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0798 - val_loss: 0.1156\n",
      "Epoch 308/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.0825 - val_loss: 0.1153\n",
      "Epoch 309/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.0825 - val_loss: 0.1169\n",
      "Epoch 310/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0827 - val_loss: 0.1150\n",
      "Epoch 311/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0784 - val_loss: 0.1168\n",
      "Epoch 312/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0759 - val_loss: 0.1160\n",
      "Epoch 313/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0832 - val_loss: 0.1192\n",
      "Epoch 314/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0801 - val_loss: 0.1152\n",
      "Epoch 315/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.0778 - val_loss: 0.1180\n",
      "Epoch 316/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0848 - val_loss: 0.1151\n",
      "Epoch 317/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0722 - val_loss: 0.1159\n",
      "Epoch 318/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0827 - val_loss: 0.1168\n",
      "Epoch 319/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0743 - val_loss: 0.1172\n",
      "Epoch 320/1000\n",
      "53/53 [==============================] - 1s 19ms/step - loss: 0.0825 - val_loss: 0.1161\n",
      "Epoch 321/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0817 - val_loss: 0.1164\n",
      "Epoch 322/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0880 - val_loss: 0.1162\n",
      "Epoch 323/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0783 - val_loss: 0.1150\n",
      "Epoch 324/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0736 - val_loss: 0.1161\n",
      "Epoch 325/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0864 - val_loss: 0.1185\n",
      "Epoch 326/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0820 - val_loss: 0.1151\n",
      "Epoch 327/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0873 - val_loss: 0.1161\n",
      "Epoch 328/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0778 - val_loss: 0.1161\n",
      "Epoch 329/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0778 - val_loss: 0.1152\n",
      "Epoch 330/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0873 - val_loss: 0.1163\n",
      "Epoch 331/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0861 - val_loss: 0.1157\n",
      "Epoch 332/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0872 - val_loss: 0.1192\n",
      "Epoch 333/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0815 - val_loss: 0.1188\n",
      "Epoch 334/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0729 - val_loss: 0.1151\n",
      "Epoch 335/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0736 - val_loss: 0.1157\n",
      "Epoch 336/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0808 - val_loss: 0.1151\n",
      "Epoch 337/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.0635 - val_loss: 0.1154\n",
      "Epoch 338/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0845 - val_loss: 0.1158\n",
      "Epoch 339/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0794 - val_loss: 0.1196\n",
      "Epoch 340/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0794 - val_loss: 0.1208\n",
      "\n",
      "Epoch 00340: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 341/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0827 - val_loss: 0.1144\n",
      "Epoch 342/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0847 - val_loss: 0.1137\n",
      "Epoch 343/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0706 - val_loss: 0.1133\n",
      "Epoch 344/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0766 - val_loss: 0.1134\n",
      "Epoch 345/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0758 - val_loss: 0.1131\n",
      "Epoch 346/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0587 - val_loss: 0.1130\n",
      "Epoch 347/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0766 - val_loss: 0.1128\n",
      "Epoch 348/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0766 - val_loss: 0.1128\n",
      "Epoch 349/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0956 - val_loss: 0.1128\n",
      "Epoch 350/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.0744 - val_loss: 0.1128\n",
      "Epoch 351/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0700 - val_loss: 0.1129\n",
      "Epoch 352/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0816 - val_loss: 0.1129\n",
      "Epoch 353/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0856 - val_loss: 0.1125\n",
      "Epoch 354/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.0702 - val_loss: 0.1127\n",
      "Epoch 355/1000\n",
      "53/53 [==============================] - 1s 19ms/step - loss: 0.0698 - val_loss: 0.1126\n",
      "Epoch 356/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0936 - val_loss: 0.1129\n",
      "Epoch 357/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0772 - val_loss: 0.1126\n",
      "Epoch 358/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0731 - val_loss: 0.1129\n",
      "Epoch 359/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0682 - val_loss: 0.1127\n",
      "Epoch 360/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0746 - val_loss: 0.1127\n",
      "Epoch 361/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0836 - val_loss: 0.1128\n",
      "Epoch 362/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0659 - val_loss: 0.1126\n",
      "Epoch 363/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0680 - val_loss: 0.1129\n",
      "Epoch 364/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0694 - val_loss: 0.1128\n",
      "Epoch 365/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0749 - val_loss: 0.1127\n",
      "Epoch 366/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0806 - val_loss: 0.1127\n",
      "Epoch 367/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0784 - val_loss: 0.1128\n",
      "Epoch 368/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0930 - val_loss: 0.1125\n",
      "Epoch 369/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0758 - val_loss: 0.1126\n",
      "Epoch 370/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0812 - val_loss: 0.1126\n",
      "Epoch 371/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0746 - val_loss: 0.1124\n",
      "Epoch 372/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0775 - val_loss: 0.1126\n",
      "Epoch 373/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0749 - val_loss: 0.1127\n",
      "Epoch 374/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.0843 - val_loss: 0.1124\n",
      "Epoch 375/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0700 - val_loss: 0.1126\n",
      "Epoch 376/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0656 - val_loss: 0.1124\n",
      "Epoch 377/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0679 - val_loss: 0.1130\n",
      "Epoch 378/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0719 - val_loss: 0.1125\n",
      "Epoch 379/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0823 - val_loss: 0.1123\n",
      "Epoch 380/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0680 - val_loss: 0.1124\n",
      "Epoch 381/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0768 - val_loss: 0.1123\n",
      "Epoch 382/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0807 - val_loss: 0.1124\n",
      "Epoch 383/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0651 - val_loss: 0.1123\n",
      "Epoch 384/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.0894 - val_loss: 0.1128\n",
      "Epoch 385/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0755 - val_loss: 0.1128\n",
      "Epoch 386/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0796 - val_loss: 0.1122\n",
      "Epoch 387/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0768 - val_loss: 0.1123\n",
      "Epoch 388/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0690 - val_loss: 0.1122\n",
      "Epoch 389/1000\n",
      "53/53 [==============================] - 1s 19ms/step - loss: 0.0729 - val_loss: 0.1122\n",
      "Epoch 390/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0720 - val_loss: 0.1123\n",
      "Epoch 391/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0644 - val_loss: 0.1125\n",
      "Epoch 392/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0803 - val_loss: 0.1122\n",
      "Epoch 393/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0770 - val_loss: 0.1123\n",
      "Epoch 394/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0708 - val_loss: 0.1126\n",
      "Epoch 395/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0850 - val_loss: 0.1121\n",
      "Epoch 396/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0878 - val_loss: 0.1125\n",
      "Epoch 397/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0698 - val_loss: 0.1123\n",
      "Epoch 398/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0760 - val_loss: 0.1127\n",
      "Epoch 399/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0722 - val_loss: 0.1124\n",
      "Epoch 400/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0667 - val_loss: 0.1127\n",
      "Epoch 401/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0730 - val_loss: 0.1123\n",
      "Epoch 402/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0813 - val_loss: 0.1122\n",
      "Epoch 403/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0747 - val_loss: 0.1126\n",
      "Epoch 404/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.0787 - val_loss: 0.1121\n",
      "Epoch 405/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0846 - val_loss: 0.1124\n",
      "Epoch 406/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0805 - val_loss: 0.1121\n",
      "Epoch 407/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0736 - val_loss: 0.1121\n",
      "Epoch 408/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0672 - val_loss: 0.1124\n",
      "Epoch 409/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0807 - val_loss: 0.1122\n",
      "Epoch 410/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0940 - val_loss: 0.1123\n",
      "Epoch 411/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0820 - val_loss: 0.1123\n",
      "Epoch 412/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0733 - val_loss: 0.1122\n",
      "Epoch 413/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.0830 - val_loss: 0.1121\n",
      "Epoch 414/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.0651 - val_loss: 0.1123\n",
      "Epoch 415/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.0645 - val_loss: 0.1122\n",
      "Epoch 416/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.0954 - val_loss: 0.1124\n",
      "Epoch 417/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0756 - val_loss: 0.1121\n",
      "Epoch 418/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0600 - val_loss: 0.1120\n",
      "Epoch 419/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0779 - val_loss: 0.1120\n",
      "Epoch 420/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0776 - val_loss: 0.1122\n",
      "Epoch 421/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0628 - val_loss: 0.1121\n",
      "Epoch 422/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0846 - val_loss: 0.1122\n",
      "Epoch 423/1000\n",
      "53/53 [==============================] - 1s 19ms/step - loss: 0.0848 - val_loss: 0.1121\n",
      "Epoch 424/1000\n",
      "53/53 [==============================] - 1s 19ms/step - loss: 0.0776 - val_loss: 0.1121\n",
      "Epoch 425/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0704 - val_loss: 0.1121\n",
      "Epoch 426/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.0800 - val_loss: 0.1121\n",
      "Epoch 427/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0722 - val_loss: 0.1121\n",
      "Epoch 428/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0696 - val_loss: 0.1121\n",
      "Epoch 429/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0741 - val_loss: 0.1123\n",
      "Epoch 430/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0744 - val_loss: 0.1123\n",
      "Epoch 431/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0704 - val_loss: 0.1120\n",
      "Epoch 432/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0801 - val_loss: 0.1119\n",
      "Epoch 433/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0787 - val_loss: 0.1122\n",
      "Epoch 434/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0858 - val_loss: 0.1122\n",
      "Epoch 435/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0757 - val_loss: 0.1122\n",
      "Epoch 436/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0843 - val_loss: 0.1121\n",
      "Epoch 437/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0757 - val_loss: 0.1121\n",
      "Epoch 438/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0782 - val_loss: 0.1122\n",
      "Epoch 439/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0811 - val_loss: 0.1123\n",
      "Epoch 440/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.0786 - val_loss: 0.1123\n",
      "Epoch 441/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0872 - val_loss: 0.1122\n",
      "Epoch 442/1000\n",
      "53/53 [==============================] - 1s 19ms/step - loss: 0.0688 - val_loss: 0.1121\n",
      "Epoch 443/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.0824 - val_loss: 0.1121\n",
      "Epoch 444/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.0701 - val_loss: 0.1123\n",
      "Epoch 445/1000\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.0761 - val_loss: 0.1122\n",
      "Epoch 446/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0799 - val_loss: 0.1121\n",
      "Epoch 447/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0604 - val_loss: 0.1122\n",
      "Epoch 448/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0641 - val_loss: 0.1124\n",
      "Epoch 449/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0675 - val_loss: 0.1120\n",
      "Epoch 450/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0694 - val_loss: 0.1123\n",
      "Epoch 451/1000\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0759 - val_loss: 0.1121\n",
      "Epoch 452/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0615 - val_loss: 0.1122\n",
      "Epoch 453/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0780 - val_loss: 0.1122\n",
      "Epoch 454/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0737 - val_loss: 0.1120\n",
      "Epoch 455/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0690 - val_loss: 0.1123\n",
      "Epoch 456/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0753 - val_loss: 0.1119\n",
      "Epoch 457/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0686 - val_loss: 0.1123\n",
      "Epoch 458/1000\n",
      "53/53 [==============================] - 1s 19ms/step - loss: 0.0696 - val_loss: 0.1121\n",
      "Epoch 459/1000\n",
      "53/53 [==============================] - 2s 30ms/step - loss: 0.0727 - val_loss: 0.1121\n",
      "Epoch 460/1000\n",
      "53/53 [==============================] - 1s 19ms/step - loss: 0.0818 - val_loss: 0.1120\n",
      "Epoch 461/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0778 - val_loss: 0.1122\n",
      "Epoch 462/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0721 - val_loss: 0.1124\n",
      "\n",
      "Epoch 00462: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 463/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0693 - val_loss: 0.1120\n",
      "Epoch 464/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0746 - val_loss: 0.1120\n",
      "Epoch 465/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0863 - val_loss: 0.1120\n",
      "Epoch 466/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0739 - val_loss: 0.1119\n",
      "Epoch 467/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0809 - val_loss: 0.1119\n",
      "Epoch 468/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0769 - val_loss: 0.1118\n",
      "Epoch 469/1000\n",
      "53/53 [==============================] - 1s 21ms/step - loss: 0.0774 - val_loss: 0.1118\n",
      "Epoch 470/1000\n",
      "53/53 [==============================] - 1s 19ms/step - loss: 0.0837 - val_loss: 0.1118\n",
      "Epoch 471/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0703 - val_loss: 0.1118\n",
      "Epoch 472/1000\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0871 - val_loss: 0.1119\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00472: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.1901865\n",
      "Training 2JHC out of ['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN']\n",
      "Categories (8, object): ['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN'] \n",
      "\n",
      "Epoch 1/1000\n",
      "502/502 [==============================] - 10s 17ms/step - loss: 1.1019 - val_loss: 0.7833\n",
      "Epoch 2/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.6116 - val_loss: 0.5917\n",
      "Epoch 3/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.5463 - val_loss: 0.5236\n",
      "Epoch 4/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.5038 - val_loss: 0.5305\n",
      "Epoch 5/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.4751 - val_loss: 0.4815\n",
      "Epoch 6/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.4525 - val_loss: 0.4805\n",
      "Epoch 7/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.4321 - val_loss: 0.4183\n",
      "Epoch 8/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.4201 - val_loss: 0.4107\n",
      "Epoch 9/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.4038 - val_loss: 0.4349\n",
      "Epoch 10/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.3934 - val_loss: 0.3945\n",
      "Epoch 11/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.3824 - val_loss: 0.3802\n",
      "Epoch 12/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.3742 - val_loss: 0.3763\n",
      "Epoch 13/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.3628 - val_loss: 0.3859\n",
      "Epoch 14/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.3557 - val_loss: 0.3861\n",
      "Epoch 15/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.3510 - val_loss: 0.3602\n",
      "Epoch 16/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.3386 - val_loss: 0.3587\n",
      "Epoch 17/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.3311 - val_loss: 0.3434\n",
      "Epoch 18/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.3263 - val_loss: 0.3391\n",
      "Epoch 19/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.3214 - val_loss: 0.3337\n",
      "Epoch 20/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.3154 - val_loss: 0.3337\n",
      "Epoch 21/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.3115 - val_loss: 0.3352\n",
      "Epoch 22/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.3077 - val_loss: 0.3273\n",
      "Epoch 23/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.3001 - val_loss: 0.3314\n",
      "Epoch 24/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.2964 - val_loss: 0.3157\n",
      "Epoch 25/1000\n",
      "502/502 [==============================] - 9s 18ms/step - loss: 0.2894 - val_loss: 0.3112\n",
      "Epoch 26/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.2886 - val_loss: 0.3180\n",
      "Epoch 27/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.2850 - val_loss: 0.3137\n",
      "Epoch 28/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2795 - val_loss: 0.3079\n",
      "Epoch 29/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.2784 - val_loss: 0.2996\n",
      "Epoch 30/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2697 - val_loss: 0.3040\n",
      "Epoch 31/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2685 - val_loss: 0.2951\n",
      "Epoch 32/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2665 - val_loss: 0.3079\n",
      "Epoch 33/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2671 - val_loss: 0.2951\n",
      "Epoch 34/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2623 - val_loss: 0.2955\n",
      "Epoch 35/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2603 - val_loss: 0.2887\n",
      "Epoch 36/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2565 - val_loss: 0.2914\n",
      "Epoch 37/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2575 - val_loss: 0.2906\n",
      "Epoch 38/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2538 - val_loss: 0.2882\n",
      "Epoch 39/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2491 - val_loss: 0.2914\n",
      "Epoch 40/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2482 - val_loss: 0.2861\n",
      "Epoch 41/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2449 - val_loss: 0.2894\n",
      "Epoch 42/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2424 - val_loss: 0.2800\n",
      "Epoch 43/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2415 - val_loss: 0.2785\n",
      "Epoch 44/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2412 - val_loss: 0.2894\n",
      "Epoch 45/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2396 - val_loss: 0.2777\n",
      "Epoch 46/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2343 - val_loss: 0.2805\n",
      "Epoch 47/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2349 - val_loss: 0.2775\n",
      "Epoch 48/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2341 - val_loss: 0.2920\n",
      "Epoch 49/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2313 - val_loss: 0.2789\n",
      "Epoch 50/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2304 - val_loss: 0.2844\n",
      "Epoch 51/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2314 - val_loss: 0.2757\n",
      "Epoch 52/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2268 - val_loss: 0.2722\n",
      "Epoch 53/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2253 - val_loss: 0.2721\n",
      "Epoch 54/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2251 - val_loss: 0.2731\n",
      "Epoch 55/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2221 - val_loss: 0.2658\n",
      "Epoch 56/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2229 - val_loss: 0.2667\n",
      "Epoch 57/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.2228 - val_loss: 0.2709\n",
      "Epoch 58/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.2219 - val_loss: 0.2690\n",
      "Epoch 59/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.2171 - val_loss: 0.2714\n",
      "Epoch 60/1000\n",
      "502/502 [==============================] - 9s 18ms/step - loss: 0.2155 - val_loss: 0.2656\n",
      "Epoch 61/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.2151 - val_loss: 0.2600\n",
      "Epoch 62/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2163 - val_loss: 0.2716\n",
      "Epoch 63/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2131 - val_loss: 0.2633\n",
      "Epoch 64/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2141 - val_loss: 0.2617\n",
      "Epoch 65/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2102 - val_loss: 0.2614\n",
      "Epoch 66/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2086 - val_loss: 0.2720\n",
      "Epoch 67/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2075 - val_loss: 0.2595\n",
      "Epoch 68/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2100 - val_loss: 0.2682\n",
      "Epoch 69/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2085 - val_loss: 0.2595\n",
      "Epoch 70/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2074 - val_loss: 0.2612\n",
      "Epoch 71/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2076 - val_loss: 0.2605\n",
      "Epoch 72/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2050 - val_loss: 0.2605\n",
      "Epoch 73/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2050 - val_loss: 0.2561\n",
      "Epoch 74/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2034 - val_loss: 0.2584\n",
      "Epoch 75/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2048 - val_loss: 0.2625\n",
      "Epoch 76/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.2045 - val_loss: 0.2567\n",
      "Epoch 77/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1999 - val_loss: 0.2539\n",
      "Epoch 78/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.2042 - val_loss: 0.2610\n",
      "Epoch 79/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1984 - val_loss: 0.2573\n",
      "Epoch 80/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1999 - val_loss: 0.2540\n",
      "Epoch 81/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1990 - val_loss: 0.2547\n",
      "Epoch 82/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1980 - val_loss: 0.2597\n",
      "Epoch 83/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1977 - val_loss: 0.2566\n",
      "Epoch 84/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1985 - val_loss: 0.2528\n",
      "Epoch 85/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1983 - val_loss: 0.2533\n",
      "Epoch 86/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1931 - val_loss: 0.2521\n",
      "Epoch 87/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1943 - val_loss: 0.2528\n",
      "Epoch 88/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1922 - val_loss: 0.2553\n",
      "Epoch 89/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1944 - val_loss: 0.2483\n",
      "Epoch 90/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1897 - val_loss: 0.2591\n",
      "Epoch 91/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1925 - val_loss: 0.2464\n",
      "Epoch 92/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1914 - val_loss: 0.2473\n",
      "Epoch 93/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1923 - val_loss: 0.2477\n",
      "Epoch 94/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1960 - val_loss: 0.2478\n",
      "Epoch 95/1000\n",
      "502/502 [==============================] - 9s 18ms/step - loss: 0.1909 - val_loss: 0.2466\n",
      "Epoch 96/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1905 - val_loss: 0.2481\n",
      "Epoch 97/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1878 - val_loss: 0.2523\n",
      "Epoch 98/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1834 - val_loss: 0.2446\n",
      "Epoch 99/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1850 - val_loss: 0.2417\n",
      "Epoch 100/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1864 - val_loss: 0.2432\n",
      "Epoch 101/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1863 - val_loss: 0.2479\n",
      "Epoch 102/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1857 - val_loss: 0.2450\n",
      "Epoch 103/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1852 - val_loss: 0.2428\n",
      "Epoch 104/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1841 - val_loss: 0.2559\n",
      "Epoch 105/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1867 - val_loss: 0.2415\n",
      "Epoch 106/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1836 - val_loss: 0.2459\n",
      "Epoch 107/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1805 - val_loss: 0.2402\n",
      "Epoch 108/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1810 - val_loss: 0.2445\n",
      "Epoch 109/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1795 - val_loss: 0.2419\n",
      "Epoch 110/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1826 - val_loss: 0.2396\n",
      "Epoch 111/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1805 - val_loss: 0.2410\n",
      "Epoch 112/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1850 - val_loss: 0.2411\n",
      "Epoch 113/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1797 - val_loss: 0.2443\n",
      "Epoch 114/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1801 - val_loss: 0.2388\n",
      "Epoch 115/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1798 - val_loss: 0.2447\n",
      "Epoch 116/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1790 - val_loss: 0.2423\n",
      "Epoch 117/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1760 - val_loss: 0.2399\n",
      "Epoch 118/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1762 - val_loss: 0.2393\n",
      "Epoch 119/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1798 - val_loss: 0.2431\n",
      "Epoch 120/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1777 - val_loss: 0.2456\n",
      "Epoch 121/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1800 - val_loss: 0.2504\n",
      "Epoch 122/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1754 - val_loss: 0.2434\n",
      "Epoch 123/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1787 - val_loss: 0.2397\n",
      "Epoch 124/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1742 - val_loss: 0.2385\n",
      "Epoch 125/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1743 - val_loss: 0.2418\n",
      "Epoch 126/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1746 - val_loss: 0.2383\n",
      "Epoch 127/1000\n",
      "502/502 [==============================] - 9s 18ms/step - loss: 0.1738 - val_loss: 0.2419\n",
      "Epoch 128/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1719 - val_loss: 0.2402\n",
      "Epoch 129/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1725 - val_loss: 0.2380\n",
      "Epoch 130/1000\n",
      "502/502 [==============================] - 9s 18ms/step - loss: 0.1780 - val_loss: 0.2379\n",
      "Epoch 131/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1717 - val_loss: 0.2373\n",
      "Epoch 132/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1743 - val_loss: 0.2363\n",
      "Epoch 133/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1735 - val_loss: 0.2367\n",
      "Epoch 134/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1719 - val_loss: 0.2398\n",
      "Epoch 135/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1725 - val_loss: 0.2358\n",
      "Epoch 136/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1743 - val_loss: 0.2398\n",
      "Epoch 137/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1678 - val_loss: 0.2458\n",
      "Epoch 138/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1728 - val_loss: 0.2413\n",
      "Epoch 139/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1706 - val_loss: 0.2370\n",
      "Epoch 140/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1666 - val_loss: 0.2436\n",
      "Epoch 141/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1753 - val_loss: 0.2416\n",
      "Epoch 142/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1693 - val_loss: 0.2397\n",
      "Epoch 143/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1672 - val_loss: 0.2347\n",
      "Epoch 144/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1687 - val_loss: 0.2366\n",
      "Epoch 145/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1680 - val_loss: 0.2327\n",
      "Epoch 146/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1653 - val_loss: 0.2368\n",
      "Epoch 147/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1694 - val_loss: 0.2331\n",
      "Epoch 148/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1675 - val_loss: 0.2366\n",
      "Epoch 149/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1680 - val_loss: 0.2365\n",
      "Epoch 150/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1687 - val_loss: 0.2319\n",
      "Epoch 151/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1650 - val_loss: 0.2376\n",
      "Epoch 152/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1675 - val_loss: 0.2357\n",
      "Epoch 153/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1660 - val_loss: 0.2360\n",
      "Epoch 154/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1667 - val_loss: 0.2352\n",
      "Epoch 155/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1641 - val_loss: 0.2395\n",
      "Epoch 156/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1635 - val_loss: 0.2393\n",
      "Epoch 157/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1660 - val_loss: 0.2380\n",
      "Epoch 158/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1642 - val_loss: 0.2349\n",
      "Epoch 159/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1653 - val_loss: 0.2352\n",
      "Epoch 160/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1620 - val_loss: 0.2320\n",
      "Epoch 161/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1628 - val_loss: 0.2345\n",
      "Epoch 162/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1627 - val_loss: 0.2382\n",
      "Epoch 163/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1656 - val_loss: 0.2338\n",
      "Epoch 164/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1613 - val_loss: 0.2359\n",
      "Epoch 165/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1622 - val_loss: 0.2370\n",
      "Epoch 166/1000\n",
      "502/502 [==============================] - 9s 18ms/step - loss: 0.1675 - val_loss: 0.2340\n",
      "Epoch 167/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1641 - val_loss: 0.2323\n",
      "Epoch 168/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1594 - val_loss: 0.2335\n",
      "Epoch 169/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1590 - val_loss: 0.2322\n",
      "Epoch 170/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1609 - val_loss: 0.2325\n",
      "Epoch 171/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1620 - val_loss: 0.2313\n",
      "Epoch 172/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1602 - val_loss: 0.2361\n",
      "Epoch 173/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1593 - val_loss: 0.2334\n",
      "Epoch 174/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1631 - val_loss: 0.2298\n",
      "Epoch 175/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1616 - val_loss: 0.2329\n",
      "Epoch 176/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1578 - val_loss: 0.2345\n",
      "Epoch 177/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1634 - val_loss: 0.2310\n",
      "Epoch 178/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1592 - val_loss: 0.2291\n",
      "Epoch 179/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1569 - val_loss: 0.2296\n",
      "Epoch 180/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1623 - val_loss: 0.2310\n",
      "Epoch 181/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1587 - val_loss: 0.2297\n",
      "Epoch 182/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1587 - val_loss: 0.2306\n",
      "Epoch 183/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1579 - val_loss: 0.2346\n",
      "Epoch 184/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1587 - val_loss: 0.2328\n",
      "Epoch 185/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1585 - val_loss: 0.2316\n",
      "Epoch 186/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1612 - val_loss: 0.2328\n",
      "Epoch 187/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1643 - val_loss: 0.2321\n",
      "Epoch 188/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1604 - val_loss: 0.2320\n",
      "Epoch 189/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1585 - val_loss: 0.2316\n",
      "Epoch 190/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1560 - val_loss: 0.2324\n",
      "Epoch 191/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1587 - val_loss: 0.2309\n",
      "Epoch 192/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1547 - val_loss: 0.2299\n",
      "Epoch 193/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1567 - val_loss: 0.2322\n",
      "Epoch 194/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1549 - val_loss: 0.2335\n",
      "Epoch 195/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1583 - val_loss: 0.2311\n",
      "Epoch 196/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1585 - val_loss: 0.2369\n",
      "Epoch 197/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1531 - val_loss: 0.2298\n",
      "Epoch 198/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1552 - val_loss: 0.2320\n",
      "Epoch 199/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1563 - val_loss: 0.2289\n",
      "Epoch 200/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1539 - val_loss: 0.2289\n",
      "Epoch 201/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1579 - val_loss: 0.2327\n",
      "Epoch 202/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1539 - val_loss: 0.2301\n",
      "Epoch 203/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1546 - val_loss: 0.2258\n",
      "Epoch 204/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1516 - val_loss: 0.2249\n",
      "Epoch 205/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1503 - val_loss: 0.2279\n",
      "Epoch 206/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1540 - val_loss: 0.2310\n",
      "Epoch 207/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1548 - val_loss: 0.2267\n",
      "Epoch 208/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1515 - val_loss: 0.2269\n",
      "Epoch 209/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1516 - val_loss: 0.2269\n",
      "Epoch 210/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1555 - val_loss: 0.2295\n",
      "Epoch 211/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1594 - val_loss: 0.2312\n",
      "Epoch 212/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1573 - val_loss: 0.2297\n",
      "Epoch 213/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1511 - val_loss: 0.2260\n",
      "Epoch 214/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1496 - val_loss: 0.2250\n",
      "Epoch 215/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1550 - val_loss: 0.2318\n",
      "Epoch 216/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1578 - val_loss: 0.2296\n",
      "Epoch 217/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1513 - val_loss: 0.2277\n",
      "Epoch 218/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1519 - val_loss: 0.2293\n",
      "Epoch 219/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1548 - val_loss: 0.2326\n",
      "Epoch 220/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1519 - val_loss: 0.2465\n",
      "Epoch 221/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1553 - val_loss: 0.2317\n",
      "Epoch 222/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1556 - val_loss: 0.2279\n",
      "Epoch 223/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1530 - val_loss: 0.2259\n",
      "Epoch 224/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1538 - val_loss: 0.2280\n",
      "Epoch 225/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1550 - val_loss: 0.2281\n",
      "Epoch 226/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1520 - val_loss: 0.2276\n",
      "Epoch 227/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1515 - val_loss: 0.2273\n",
      "Epoch 228/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1485 - val_loss: 0.2260\n",
      "Epoch 229/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1503 - val_loss: 0.2278\n",
      "Epoch 230/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1525 - val_loss: 0.2295\n",
      "Epoch 231/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1502 - val_loss: 0.2247\n",
      "Epoch 232/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1516 - val_loss: 0.2267\n",
      "Epoch 233/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1533 - val_loss: 0.2256\n",
      "Epoch 234/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1521 - val_loss: 0.2337\n",
      "Epoch 235/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1516 - val_loss: 0.2242\n",
      "Epoch 236/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1525 - val_loss: 0.2271\n",
      "Epoch 237/1000\n",
      "502/502 [==============================] - 9s 18ms/step - loss: 0.1499 - val_loss: 0.2272\n",
      "Epoch 238/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1465 - val_loss: 0.2278\n",
      "Epoch 239/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1506 - val_loss: 0.2295\n",
      "Epoch 240/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1498 - val_loss: 0.2346\n",
      "Epoch 241/1000\n",
      "502/502 [==============================] - 9s 18ms/step - loss: 0.1522 - val_loss: 0.2242\n",
      "Epoch 242/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1494 - val_loss: 0.2239\n",
      "Epoch 243/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1519 - val_loss: 0.2238\n",
      "Epoch 244/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1486 - val_loss: 0.2273\n",
      "Epoch 245/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1539 - val_loss: 0.2283\n",
      "Epoch 246/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1525 - val_loss: 0.2269\n",
      "Epoch 247/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1507 - val_loss: 0.2246\n",
      "Epoch 248/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1493 - val_loss: 0.2235\n",
      "Epoch 249/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1489 - val_loss: 0.2251\n",
      "Epoch 250/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1532 - val_loss: 0.2264\n",
      "Epoch 251/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1525 - val_loss: 0.2264\n",
      "Epoch 252/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1488 - val_loss: 0.2252\n",
      "Epoch 253/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1513 - val_loss: 0.2239\n",
      "Epoch 254/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1504 - val_loss: 0.2283\n",
      "Epoch 255/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1488 - val_loss: 0.2219\n",
      "Epoch 256/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1480 - val_loss: 0.2243\n",
      "Epoch 257/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1471 - val_loss: 0.2350\n",
      "Epoch 258/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1523 - val_loss: 0.2222\n",
      "Epoch 259/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1472 - val_loss: 0.2256\n",
      "Epoch 260/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1462 - val_loss: 0.2254\n",
      "Epoch 261/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1446 - val_loss: 0.2265\n",
      "Epoch 262/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1437 - val_loss: 0.2252\n",
      "Epoch 263/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1462 - val_loss: 0.2248\n",
      "Epoch 264/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1499 - val_loss: 0.2260\n",
      "Epoch 265/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1473 - val_loss: 0.2256\n",
      "Epoch 266/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1527 - val_loss: 0.2264\n",
      "Epoch 267/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1486 - val_loss: 0.2225\n",
      "Epoch 268/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1465 - val_loss: 0.2414\n",
      "Epoch 269/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1458 - val_loss: 0.2223\n",
      "Epoch 270/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1409 - val_loss: 0.2253\n",
      "Epoch 271/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1440 - val_loss: 0.2247\n",
      "Epoch 272/1000\n",
      "502/502 [==============================] - 9s 18ms/step - loss: 0.1428 - val_loss: 0.2234\n",
      "Epoch 273/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1435 - val_loss: 0.2255\n",
      "Epoch 274/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1427 - val_loss: 0.2225\n",
      "Epoch 275/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1441 - val_loss: 0.2226\n",
      "Epoch 276/1000\n",
      "502/502 [==============================] - 9s 18ms/step - loss: 0.1480 - val_loss: 0.2267\n",
      "Epoch 277/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1475 - val_loss: 0.2284\n",
      "Epoch 278/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1430 - val_loss: 0.2319\n",
      "Epoch 279/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1458 - val_loss: 0.2227\n",
      "Epoch 280/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1444 - val_loss: 0.2203\n",
      "Epoch 281/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1443 - val_loss: 0.2262\n",
      "Epoch 282/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1459 - val_loss: 0.2228\n",
      "Epoch 283/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1390 - val_loss: 0.2301\n",
      "Epoch 284/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1474 - val_loss: 0.2206\n",
      "Epoch 285/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1443 - val_loss: 0.2245\n",
      "Epoch 286/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1423 - val_loss: 0.2240\n",
      "Epoch 287/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1487 - val_loss: 0.2235\n",
      "Epoch 288/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1415 - val_loss: 0.2233\n",
      "Epoch 289/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1421 - val_loss: 0.2286\n",
      "Epoch 290/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1479 - val_loss: 0.2207\n",
      "Epoch 291/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1422 - val_loss: 0.2256\n",
      "Epoch 292/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1459 - val_loss: 0.2209\n",
      "Epoch 293/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1449 - val_loss: 0.2293\n",
      "Epoch 294/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1453 - val_loss: 0.2213\n",
      "Epoch 295/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1447 - val_loss: 0.2224\n",
      "Epoch 296/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1412 - val_loss: 0.2251\n",
      "Epoch 297/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1466 - val_loss: 0.2228\n",
      "Epoch 298/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1421 - val_loss: 0.2292\n",
      "Epoch 299/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1440 - val_loss: 0.2234\n",
      "Epoch 300/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1454 - val_loss: 0.2237\n",
      "Epoch 301/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1403 - val_loss: 0.2234\n",
      "Epoch 302/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1407 - val_loss: 0.2241\n",
      "Epoch 303/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1414 - val_loss: 0.2252\n",
      "Epoch 304/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1402 - val_loss: 0.2204\n",
      "Epoch 305/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1429 - val_loss: 0.2216\n",
      "Epoch 306/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1431 - val_loss: 0.2266\n",
      "Epoch 307/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1419 - val_loss: 0.2242\n",
      "Epoch 308/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1414 - val_loss: 0.2299\n",
      "Epoch 309/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1399 - val_loss: 0.2216\n",
      "Epoch 310/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1393 - val_loss: 0.2218\n",
      "\n",
      "Epoch 00310: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 311/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1277 - val_loss: 0.2069\n",
      "Epoch 312/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1235 - val_loss: 0.2059\n",
      "Epoch 313/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1245 - val_loss: 0.2053\n",
      "Epoch 314/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1198 - val_loss: 0.2045\n",
      "Epoch 315/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1182 - val_loss: 0.2053\n",
      "Epoch 316/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1166 - val_loss: 0.2040\n",
      "Epoch 317/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1217 - val_loss: 0.2042\n",
      "Epoch 318/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1153 - val_loss: 0.2042\n",
      "Epoch 319/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1174 - val_loss: 0.2047\n",
      "Epoch 320/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1165 - val_loss: 0.2036\n",
      "Epoch 321/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1200 - val_loss: 0.2036\n",
      "Epoch 322/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1209 - val_loss: 0.2033\n",
      "Epoch 323/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1178 - val_loss: 0.2033\n",
      "Epoch 324/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1190 - val_loss: 0.2036\n",
      "Epoch 325/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1105 - val_loss: 0.2034\n",
      "Epoch 326/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1167 - val_loss: 0.2032\n",
      "Epoch 327/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1162 - val_loss: 0.2039\n",
      "Epoch 328/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1164 - val_loss: 0.2035\n",
      "Epoch 329/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1169 - val_loss: 0.2033\n",
      "Epoch 330/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1173 - val_loss: 0.2032\n",
      "Epoch 331/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1180 - val_loss: 0.2028\n",
      "Epoch 332/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1155 - val_loss: 0.2039\n",
      "Epoch 333/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1155 - val_loss: 0.2031\n",
      "Epoch 334/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1132 - val_loss: 0.2032\n",
      "Epoch 335/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1141 - val_loss: 0.2026\n",
      "Epoch 336/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1147 - val_loss: 0.2031\n",
      "Epoch 337/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1152 - val_loss: 0.2029\n",
      "Epoch 338/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1159 - val_loss: 0.2027\n",
      "Epoch 339/1000\n",
      "502/502 [==============================] - 9s 18ms/step - loss: 0.1161 - val_loss: 0.2027\n",
      "Epoch 340/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1156 - val_loss: 0.2027\n",
      "Epoch 341/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1151 - val_loss: 0.2033\n",
      "Epoch 342/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1169 - val_loss: 0.2032\n",
      "Epoch 343/1000\n",
      "502/502 [==============================] - 9s 18ms/step - loss: 0.1140 - val_loss: 0.2028\n",
      "Epoch 344/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1170 - val_loss: 0.2028\n",
      "Epoch 345/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1146 - val_loss: 0.2031\n",
      "Epoch 346/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1136 - val_loss: 0.2032\n",
      "Epoch 347/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1162 - val_loss: 0.2026\n",
      "Epoch 348/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1169 - val_loss: 0.2025\n",
      "Epoch 349/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1146 - val_loss: 0.2037\n",
      "Epoch 350/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1143 - val_loss: 0.2034\n",
      "Epoch 351/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1162 - val_loss: 0.2025\n",
      "Epoch 352/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1133 - val_loss: 0.2026\n",
      "Epoch 353/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1123 - val_loss: 0.2029\n",
      "Epoch 354/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1169 - val_loss: 0.2027\n",
      "Epoch 355/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1133 - val_loss: 0.2024\n",
      "Epoch 356/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1166 - val_loss: 0.2027\n",
      "Epoch 357/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1191 - val_loss: 0.2025\n",
      "Epoch 358/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1215 - val_loss: 0.2027\n",
      "Epoch 359/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1111 - val_loss: 0.2024\n",
      "Epoch 360/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1148 - val_loss: 0.2026\n",
      "Epoch 361/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1140 - val_loss: 0.2029\n",
      "Epoch 362/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1173 - val_loss: 0.2025\n",
      "Epoch 363/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1126 - val_loss: 0.2030\n",
      "Epoch 364/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1195 - val_loss: 0.2025\n",
      "Epoch 365/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1109 - val_loss: 0.2027\n",
      "Epoch 366/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1135 - val_loss: 0.2026\n",
      "Epoch 367/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1135 - val_loss: 0.2024\n",
      "Epoch 368/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1107 - val_loss: 0.2027\n",
      "Epoch 369/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1124 - val_loss: 0.2025\n",
      "Epoch 370/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1125 - val_loss: 0.2021\n",
      "Epoch 371/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1097 - val_loss: 0.2023\n",
      "Epoch 372/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1109 - val_loss: 0.2024\n",
      "Epoch 373/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1125 - val_loss: 0.2024\n",
      "Epoch 374/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1127 - val_loss: 0.2025\n",
      "Epoch 375/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1174 - val_loss: 0.2021\n",
      "Epoch 376/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1112 - val_loss: 0.2024\n",
      "Epoch 377/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1182 - val_loss: 0.2020\n",
      "Epoch 378/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1119 - val_loss: 0.2021\n",
      "Epoch 379/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1158 - val_loss: 0.2024\n",
      "Epoch 380/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1128 - val_loss: 0.2024\n",
      "Epoch 381/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1085 - val_loss: 0.2026\n",
      "Epoch 382/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1135 - val_loss: 0.2024\n",
      "Epoch 383/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1150 - val_loss: 0.2021\n",
      "Epoch 384/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1108 - val_loss: 0.2021\n",
      "Epoch 385/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1098 - val_loss: 0.2024\n",
      "Epoch 386/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1127 - val_loss: 0.2027\n",
      "Epoch 387/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1163 - val_loss: 0.2025\n",
      "Epoch 388/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1161 - val_loss: 0.2023\n",
      "Epoch 389/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1112 - val_loss: 0.2020\n",
      "Epoch 390/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1137 - val_loss: 0.2023\n",
      "Epoch 391/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1147 - val_loss: 0.2022\n",
      "Epoch 392/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1110 - val_loss: 0.2023\n",
      "Epoch 393/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1108 - val_loss: 0.2021\n",
      "Epoch 394/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1132 - val_loss: 0.2028\n",
      "Epoch 395/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1126 - val_loss: 0.2023\n",
      "Epoch 396/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1146 - val_loss: 0.2020\n",
      "Epoch 397/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1130 - val_loss: 0.2028\n",
      "Epoch 398/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1107 - val_loss: 0.2019\n",
      "Epoch 399/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1155 - val_loss: 0.2022\n",
      "Epoch 400/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1105 - val_loss: 0.2021\n",
      "Epoch 401/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1172 - val_loss: 0.2023\n",
      "Epoch 402/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1115 - val_loss: 0.2020\n",
      "Epoch 403/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1101 - val_loss: 0.2021\n",
      "Epoch 404/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1136 - val_loss: 0.2021\n",
      "Epoch 405/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1103 - val_loss: 0.2022\n",
      "Epoch 406/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1111 - val_loss: 0.2024\n",
      "Epoch 407/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1157 - val_loss: 0.2024\n",
      "Epoch 408/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1105 - val_loss: 0.2023\n",
      "Epoch 409/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1106 - val_loss: 0.2021\n",
      "Epoch 410/1000\n",
      "502/502 [==============================] - 9s 18ms/step - loss: 0.1068 - val_loss: 0.2026\n",
      "Epoch 411/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1171 - val_loss: 0.2021\n",
      "Epoch 412/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1106 - val_loss: 0.2020\n",
      "Epoch 413/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1191 - val_loss: 0.2023\n",
      "Epoch 414/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1095 - val_loss: 0.2021\n",
      "Epoch 415/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1127 - val_loss: 0.2019\n",
      "Epoch 416/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1138 - val_loss: 0.2021\n",
      "Epoch 417/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1088 - val_loss: 0.2026\n",
      "Epoch 418/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1115 - val_loss: 0.2032\n",
      "Epoch 419/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1058 - val_loss: 0.2021\n",
      "\n",
      "Epoch 00419: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 420/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1094 - val_loss: 0.2010\n",
      "Epoch 421/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1105 - val_loss: 0.2009\n",
      "Epoch 422/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1089 - val_loss: 0.2008\n",
      "Epoch 423/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1165 - val_loss: 0.2009\n",
      "Epoch 424/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1074 - val_loss: 0.2009\n",
      "Epoch 425/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1099 - val_loss: 0.2007\n",
      "Epoch 426/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1096 - val_loss: 0.2007\n",
      "Epoch 427/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1106 - val_loss: 0.2007\n",
      "Epoch 428/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1109 - val_loss: 0.2008\n",
      "Epoch 429/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1084 - val_loss: 0.2007\n",
      "Epoch 430/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1098 - val_loss: 0.2006\n",
      "Epoch 431/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1120 - val_loss: 0.2010\n",
      "Epoch 432/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1118 - val_loss: 0.2006\n",
      "Epoch 433/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1043 - val_loss: 0.2011\n",
      "Epoch 434/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1065 - val_loss: 0.2005\n",
      "Epoch 435/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1111 - val_loss: 0.2004\n",
      "Epoch 436/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1106 - val_loss: 0.2005\n",
      "Epoch 437/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1119 - val_loss: 0.2005\n",
      "Epoch 438/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1089 - val_loss: 0.2005\n",
      "Epoch 439/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1093 - val_loss: 0.2005\n",
      "Epoch 440/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1107 - val_loss: 0.2005\n",
      "Epoch 441/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1094 - val_loss: 0.2005\n",
      "Epoch 442/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1017 - val_loss: 0.2006\n",
      "Epoch 443/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1137 - val_loss: 0.2005\n",
      "Epoch 444/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1055 - val_loss: 0.2005\n",
      "Epoch 445/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1132 - val_loss: 0.2004\n",
      "Epoch 446/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1109 - val_loss: 0.2004\n",
      "Epoch 447/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1123 - val_loss: 0.2006\n",
      "Epoch 448/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1058 - val_loss: 0.2004\n",
      "Epoch 449/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1088 - val_loss: 0.2006\n",
      "Epoch 450/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1066 - val_loss: 0.2004\n",
      "Epoch 451/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1128 - val_loss: 0.2004\n",
      "Epoch 452/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1064 - val_loss: 0.2004\n",
      "Epoch 453/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1120 - val_loss: 0.2004\n",
      "Epoch 454/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1138 - val_loss: 0.2004\n",
      "Epoch 455/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1158 - val_loss: 0.2004\n",
      "Epoch 456/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1093 - val_loss: 0.2005\n",
      "Epoch 457/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1072 - val_loss: 0.2005\n",
      "Epoch 458/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1073 - val_loss: 0.2004\n",
      "Epoch 459/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1086 - val_loss: 0.2004\n",
      "Epoch 460/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1024 - val_loss: 0.2005\n",
      "Epoch 461/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1083 - val_loss: 0.2004\n",
      "Epoch 462/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1097 - val_loss: 0.2005\n",
      "Epoch 463/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1092 - val_loss: 0.2005\n",
      "Epoch 464/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1092 - val_loss: 0.2008\n",
      "Epoch 465/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1091 - val_loss: 0.2003\n",
      "Epoch 466/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1113 - val_loss: 0.2005\n",
      "Epoch 467/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1116 - val_loss: 0.2005\n",
      "Epoch 468/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1087 - val_loss: 0.2004\n",
      "Epoch 469/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1099 - val_loss: 0.2003\n",
      "Epoch 470/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1053 - val_loss: 0.2004\n",
      "Epoch 471/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1080 - val_loss: 0.2005\n",
      "Epoch 472/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1189 - val_loss: 0.2006\n",
      "Epoch 473/1000\n",
      "502/502 [==============================] - 9s 18ms/step - loss: 0.1069 - val_loss: 0.2003\n",
      "Epoch 474/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1088 - val_loss: 0.2004\n",
      "Epoch 475/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1078 - val_loss: 0.2007\n",
      "Epoch 476/1000\n",
      "502/502 [==============================] - 9s 18ms/step - loss: 0.1104 - val_loss: 0.2006\n",
      "Epoch 477/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1130 - val_loss: 0.2004\n",
      "Epoch 478/1000\n",
      "502/502 [==============================] - 8s 17ms/step - loss: 0.1101 - val_loss: 0.2005\n",
      "Epoch 479/1000\n",
      "502/502 [==============================] - 9s 17ms/step - loss: 0.1129 - val_loss: 0.2004\n",
      "Epoch 480/1000\n",
      "502/502 [==============================] - 9s 18ms/step - loss: 0.1131 - val_loss: 0.2004\n",
      "Epoch 481/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1096 - val_loss: 0.2006\n",
      "Epoch 482/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1072 - val_loss: 0.2006\n",
      "Epoch 483/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1108 - val_loss: 0.2004\n",
      "Epoch 484/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1095 - val_loss: 0.2005\n",
      "Epoch 485/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1096 - val_loss: 0.2004\n",
      "Epoch 486/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1108 - val_loss: 0.2005\n",
      "Epoch 487/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1101 - val_loss: 0.2005\n",
      "Epoch 488/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1066 - val_loss: 0.2003\n",
      "Epoch 489/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1076 - val_loss: 0.2004\n",
      "Epoch 490/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1112 - val_loss: 0.2004\n",
      "Epoch 491/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1073 - val_loss: 0.2006\n",
      "Epoch 492/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1049 - val_loss: 0.2006\n",
      "Epoch 493/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1079 - val_loss: 0.2007\n",
      "Epoch 494/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1069 - val_loss: 0.2003\n",
      "Epoch 495/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1092 - val_loss: 0.2003\n",
      "\n",
      "Epoch 00495: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 496/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1077 - val_loss: 0.2005\n",
      "Epoch 497/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1087 - val_loss: 0.2003\n",
      "Epoch 498/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1095 - val_loss: 0.2003\n",
      "Epoch 499/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1109 - val_loss: 0.2004\n",
      "Epoch 500/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1102 - val_loss: 0.2003\n",
      "Epoch 501/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1138 - val_loss: 0.2006\n",
      "Epoch 502/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1067 - val_loss: 0.2003\n",
      "Epoch 503/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1091 - val_loss: 0.2005\n",
      "Epoch 504/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1078 - val_loss: 0.2005\n",
      "Epoch 505/1000\n",
      "502/502 [==============================] - 8s 16ms/step - loss: 0.1096 - val_loss: 0.2004\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00505: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.607858\n",
      "Training 3JHH out of ['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN']\n",
      "Categories (8, object): ['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN'] \n",
      "\n",
      "Epoch 1/1000\n",
      "260/260 [==============================] - 6s 17ms/step - loss: 2.6837 - val_loss: 1.8561\n",
      "Epoch 2/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.4582 - val_loss: 0.4498\n",
      "Epoch 3/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.4005 - val_loss: 0.4329\n",
      "Epoch 4/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.3551 - val_loss: 0.4084\n",
      "Epoch 5/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.3418 - val_loss: 0.3724\n",
      "Epoch 6/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.3227 - val_loss: 0.3779\n",
      "Epoch 7/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.3129 - val_loss: 0.3546\n",
      "Epoch 8/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.3056 - val_loss: 0.3045\n",
      "Epoch 9/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.2938 - val_loss: 0.2948\n",
      "Epoch 10/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.2853 - val_loss: 0.3183\n",
      "Epoch 11/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.2800 - val_loss: 0.2894\n",
      "Epoch 12/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.2731 - val_loss: 0.3024\n",
      "Epoch 13/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.2684 - val_loss: 0.2621\n",
      "Epoch 14/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.2574 - val_loss: 0.3001\n",
      "Epoch 15/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.2606 - val_loss: 0.2552\n",
      "Epoch 16/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.2488 - val_loss: 0.2474\n",
      "Epoch 17/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.2458 - val_loss: 0.2515\n",
      "Epoch 18/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.2451 - val_loss: 0.2435\n",
      "Epoch 19/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.2434 - val_loss: 0.2604\n",
      "Epoch 20/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.2299 - val_loss: 0.2609\n",
      "Epoch 21/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.2341 - val_loss: 0.2376\n",
      "Epoch 22/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.2287 - val_loss: 0.2388\n",
      "Epoch 23/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.2263 - val_loss: 0.2383\n",
      "Epoch 24/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.2234 - val_loss: 0.2423\n",
      "Epoch 25/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.2231 - val_loss: 0.2620\n",
      "Epoch 26/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.2216 - val_loss: 0.2409\n",
      "Epoch 27/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.2156 - val_loss: 0.2156\n",
      "Epoch 28/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.2133 - val_loss: 0.2201\n",
      "Epoch 29/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.2126 - val_loss: 0.2240\n",
      "Epoch 30/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.2110 - val_loss: 0.2146\n",
      "Epoch 31/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.2106 - val_loss: 0.2139\n",
      "Epoch 32/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.2036 - val_loss: 0.2167\n",
      "Epoch 33/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.2066 - val_loss: 0.2157\n",
      "Epoch 34/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.2017 - val_loss: 0.2478\n",
      "Epoch 35/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.2001 - val_loss: 0.2124\n",
      "Epoch 36/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.2001 - val_loss: 0.2026\n",
      "Epoch 37/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1959 - val_loss: 0.2159\n",
      "Epoch 38/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1870 - val_loss: 0.2128\n",
      "Epoch 39/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1971 - val_loss: 0.1891\n",
      "Epoch 40/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1923 - val_loss: 0.1954\n",
      "Epoch 41/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1904 - val_loss: 0.1993\n",
      "Epoch 42/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1902 - val_loss: 0.1950\n",
      "Epoch 43/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1820 - val_loss: 0.1942\n",
      "Epoch 44/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1839 - val_loss: 0.2032\n",
      "Epoch 45/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1831 - val_loss: 0.2053\n",
      "Epoch 46/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1804 - val_loss: 0.1977\n",
      "Epoch 47/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1854 - val_loss: 0.2091\n",
      "Epoch 48/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1795 - val_loss: 0.1864\n",
      "Epoch 49/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1820 - val_loss: 0.1917\n",
      "Epoch 50/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1719 - val_loss: 0.2030\n",
      "Epoch 51/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1769 - val_loss: 0.2026\n",
      "Epoch 52/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1750 - val_loss: 0.1996\n",
      "Epoch 53/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1757 - val_loss: 0.2041\n",
      "Epoch 54/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.1743 - val_loss: 0.1846\n",
      "Epoch 55/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1763 - val_loss: 0.1872\n",
      "Epoch 56/1000\n",
      "260/260 [==============================] - 5s 17ms/step - loss: 0.1685 - val_loss: 0.1911\n",
      "Epoch 57/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.1677 - val_loss: 0.1905\n",
      "Epoch 58/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.1719 - val_loss: 0.1896\n",
      "Epoch 59/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1624 - val_loss: 0.1828\n",
      "Epoch 60/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1604 - val_loss: 0.1715\n",
      "Epoch 61/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.1656 - val_loss: 0.1781\n",
      "Epoch 62/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1610 - val_loss: 0.1962\n",
      "Epoch 63/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.1639 - val_loss: 0.1759\n",
      "Epoch 64/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.1578 - val_loss: 0.1866\n",
      "Epoch 65/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1677 - val_loss: 0.1809\n",
      "Epoch 66/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.1591 - val_loss: 0.1766\n",
      "Epoch 67/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1666 - val_loss: 0.1954\n",
      "Epoch 68/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.1601 - val_loss: 0.1690\n",
      "Epoch 69/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1579 - val_loss: 0.1709\n",
      "Epoch 70/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1546 - val_loss: 0.1808\n",
      "Epoch 71/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1589 - val_loss: 0.1968\n",
      "Epoch 72/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1527 - val_loss: 0.1814\n",
      "Epoch 73/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1608 - val_loss: 0.1752\n",
      "Epoch 74/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1507 - val_loss: 0.1754\n",
      "Epoch 75/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1524 - val_loss: 0.1973\n",
      "Epoch 76/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1509 - val_loss: 0.1641\n",
      "Epoch 77/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1481 - val_loss: 0.1738\n",
      "Epoch 78/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1500 - val_loss: 0.1747\n",
      "Epoch 79/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1482 - val_loss: 0.1709\n",
      "Epoch 80/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1553 - val_loss: 0.1684\n",
      "Epoch 81/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1471 - val_loss: 0.1650\n",
      "Epoch 82/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1509 - val_loss: 0.1785\n",
      "Epoch 83/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1537 - val_loss: 0.1864\n",
      "Epoch 84/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1485 - val_loss: 0.1670\n",
      "Epoch 85/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1482 - val_loss: 0.1661\n",
      "Epoch 86/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1545 - val_loss: 0.1738\n",
      "Epoch 87/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1451 - val_loss: 0.1635\n",
      "Epoch 88/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1402 - val_loss: 0.2014\n",
      "Epoch 89/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1461 - val_loss: 0.1677\n",
      "Epoch 90/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1450 - val_loss: 0.1589\n",
      "Epoch 91/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1467 - val_loss: 0.1684\n",
      "Epoch 92/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1441 - val_loss: 0.1981\n",
      "Epoch 93/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1429 - val_loss: 0.1604\n",
      "Epoch 94/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.1402 - val_loss: 0.1662\n",
      "Epoch 95/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1437 - val_loss: 0.1704\n",
      "Epoch 96/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1485 - val_loss: 0.1625\n",
      "Epoch 97/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1462 - val_loss: 0.1685\n",
      "Epoch 98/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1491 - val_loss: 0.1606\n",
      "Epoch 99/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1403 - val_loss: 0.1605\n",
      "Epoch 100/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1419 - val_loss: 0.1638\n",
      "Epoch 101/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1441 - val_loss: 0.1696\n",
      "Epoch 102/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.1355 - val_loss: 0.1640\n",
      "Epoch 103/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1335 - val_loss: 0.1708\n",
      "Epoch 104/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1389 - val_loss: 0.1759\n",
      "Epoch 105/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1472 - val_loss: 0.1653\n",
      "Epoch 106/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1418 - val_loss: 0.1679\n",
      "Epoch 107/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1357 - val_loss: 0.1592\n",
      "Epoch 108/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1397 - val_loss: 0.1636\n",
      "Epoch 109/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1412 - val_loss: 0.1694\n",
      "Epoch 110/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.1355 - val_loss: 0.1577\n",
      "Epoch 111/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1418 - val_loss: 0.1603\n",
      "Epoch 112/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1325 - val_loss: 0.1576\n",
      "Epoch 113/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1390 - val_loss: 0.1686\n",
      "Epoch 114/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1351 - val_loss: 0.1643\n",
      "Epoch 115/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1387 - val_loss: 0.1661\n",
      "Epoch 116/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1400 - val_loss: 0.1670\n",
      "Epoch 117/1000\n",
      "260/260 [==============================] - 5s 17ms/step - loss: 0.1357 - val_loss: 0.1631\n",
      "Epoch 118/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1375 - val_loss: 0.1563\n",
      "Epoch 119/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1340 - val_loss: 0.1526\n",
      "Epoch 120/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1311 - val_loss: 0.1661\n",
      "Epoch 121/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1341 - val_loss: 0.1552\n",
      "Epoch 122/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1366 - val_loss: 0.1674\n",
      "Epoch 123/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1312 - val_loss: 0.1570\n",
      "Epoch 124/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1289 - val_loss: 0.1710\n",
      "Epoch 125/1000\n",
      "260/260 [==============================] - 5s 17ms/step - loss: 0.1340 - val_loss: 0.1688\n",
      "Epoch 126/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1321 - val_loss: 0.1574\n",
      "Epoch 127/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1332 - val_loss: 0.1648\n",
      "Epoch 128/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1400 - val_loss: 0.1570\n",
      "Epoch 129/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1339 - val_loss: 0.1675\n",
      "Epoch 130/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1420 - val_loss: 0.1604\n",
      "Epoch 131/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1394 - val_loss: 0.2022\n",
      "Epoch 132/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1304 - val_loss: 0.1834\n",
      "Epoch 133/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.1348 - val_loss: 0.1662\n",
      "Epoch 134/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1292 - val_loss: 0.1658\n",
      "Epoch 135/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1296 - val_loss: 0.1579\n",
      "Epoch 136/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1312 - val_loss: 0.1574\n",
      "Epoch 137/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1281 - val_loss: 0.1860\n",
      "Epoch 138/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1324 - val_loss: 0.1641\n",
      "Epoch 139/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1266 - val_loss: 0.1706\n",
      "Epoch 140/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1289 - val_loss: 0.1659\n",
      "Epoch 141/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.1265 - val_loss: 0.1554\n",
      "Epoch 142/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1314 - val_loss: 0.1616\n",
      "Epoch 143/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1272 - val_loss: 0.1803\n",
      "Epoch 144/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1347 - val_loss: 0.1517\n",
      "Epoch 145/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1329 - val_loss: 0.1732\n",
      "Epoch 146/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1328 - val_loss: 0.2070\n",
      "Epoch 147/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1302 - val_loss: 0.1563\n",
      "Epoch 148/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1232 - val_loss: 0.1489\n",
      "Epoch 149/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1277 - val_loss: 0.1610\n",
      "Epoch 150/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1284 - val_loss: 0.1551\n",
      "Epoch 151/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1281 - val_loss: 0.1560\n",
      "Epoch 152/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1230 - val_loss: 0.1455\n",
      "Epoch 153/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1285 - val_loss: 0.1932\n",
      "Epoch 154/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1277 - val_loss: 0.1567\n",
      "Epoch 155/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1218 - val_loss: 0.1455\n",
      "Epoch 156/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1255 - val_loss: 0.1642\n",
      "Epoch 157/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1256 - val_loss: 0.2049\n",
      "Epoch 158/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1290 - val_loss: 0.1487\n",
      "Epoch 159/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1315 - val_loss: 0.1599\n",
      "Epoch 160/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1226 - val_loss: 0.1658\n",
      "Epoch 161/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1262 - val_loss: 0.1493\n",
      "Epoch 162/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1291 - val_loss: 0.1882\n",
      "Epoch 163/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1272 - val_loss: 0.1559\n",
      "Epoch 164/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1260 - val_loss: 0.1551\n",
      "Epoch 165/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1229 - val_loss: 0.1737\n",
      "Epoch 166/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1318 - val_loss: 0.1473\n",
      "Epoch 167/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1197 - val_loss: 0.1539\n",
      "Epoch 168/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1330 - val_loss: 0.1748\n",
      "Epoch 169/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1256 - val_loss: 0.1467\n",
      "Epoch 170/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1229 - val_loss: 0.1446\n",
      "Epoch 171/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1143 - val_loss: 0.1520\n",
      "Epoch 172/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1207 - val_loss: 0.1582\n",
      "Epoch 173/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1212 - val_loss: 0.1523\n",
      "Epoch 174/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1213 - val_loss: 0.1477\n",
      "Epoch 175/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1183 - val_loss: 0.1582\n",
      "Epoch 176/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1217 - val_loss: 0.1501\n",
      "Epoch 177/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1232 - val_loss: 0.1610\n",
      "Epoch 178/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1363 - val_loss: 0.1505\n",
      "Epoch 179/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1192 - val_loss: 0.1443\n",
      "Epoch 180/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1203 - val_loss: 0.1504\n",
      "Epoch 181/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1275 - val_loss: 0.1514\n",
      "Epoch 182/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1181 - val_loss: 0.1552\n",
      "Epoch 183/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1277 - val_loss: 0.1512\n",
      "Epoch 184/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1212 - val_loss: 0.1491\n",
      "Epoch 185/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.1235 - val_loss: 0.1514\n",
      "Epoch 186/1000\n",
      "260/260 [==============================] - 5s 19ms/step - loss: 0.1261 - val_loss: 0.1460\n",
      "Epoch 187/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.1145 - val_loss: 0.1524\n",
      "Epoch 188/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1211 - val_loss: 0.1611\n",
      "Epoch 189/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1221 - val_loss: 0.1525\n",
      "Epoch 190/1000\n",
      "260/260 [==============================] - 5s 17ms/step - loss: 0.1273 - val_loss: 0.1434\n",
      "Epoch 191/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1266 - val_loss: 0.1547\n",
      "Epoch 192/1000\n",
      "260/260 [==============================] - 5s 17ms/step - loss: 0.1159 - val_loss: 0.1538\n",
      "Epoch 193/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.1210 - val_loss: 0.1627\n",
      "Epoch 194/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1233 - val_loss: 0.1548\n",
      "Epoch 195/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1139 - val_loss: 0.1667\n",
      "Epoch 196/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1217 - val_loss: 0.1474\n",
      "Epoch 197/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.1185 - val_loss: 0.1517\n",
      "Epoch 198/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1171 - val_loss: 0.1445\n",
      "Epoch 199/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.1159 - val_loss: 0.1432\n",
      "Epoch 200/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.1207 - val_loss: 0.1444\n",
      "Epoch 201/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1188 - val_loss: 0.1544\n",
      "Epoch 202/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.1200 - val_loss: 0.1814\n",
      "Epoch 203/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1153 - val_loss: 0.1424\n",
      "Epoch 204/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1186 - val_loss: 0.1438\n",
      "Epoch 205/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1176 - val_loss: 0.1482\n",
      "Epoch 206/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1189 - val_loss: 0.1661\n",
      "Epoch 207/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1233 - val_loss: 0.1441\n",
      "Epoch 208/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1158 - val_loss: 0.1750\n",
      "Epoch 209/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1146 - val_loss: 0.1443\n",
      "Epoch 210/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1167 - val_loss: 0.1456\n",
      "Epoch 211/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1182 - val_loss: 0.1443\n",
      "Epoch 212/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1208 - val_loss: 0.1511\n",
      "Epoch 213/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1131 - val_loss: 0.1436\n",
      "Epoch 214/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1181 - val_loss: 0.1599\n",
      "Epoch 215/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1135 - val_loss: 0.1425\n",
      "Epoch 216/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1140 - val_loss: 0.1613\n",
      "Epoch 217/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1135 - val_loss: 0.1832\n",
      "Epoch 218/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1227 - val_loss: 0.1566\n",
      "Epoch 219/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1108 - val_loss: 0.2118\n",
      "Epoch 220/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1250 - val_loss: 0.1411\n",
      "Epoch 221/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1121 - val_loss: 0.1435\n",
      "Epoch 222/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1104 - val_loss: 0.1433\n",
      "Epoch 223/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1142 - val_loss: 0.1692\n",
      "Epoch 224/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1188 - val_loss: 0.1464\n",
      "Epoch 225/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1151 - val_loss: 0.1434\n",
      "Epoch 226/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1156 - val_loss: 0.1452\n",
      "Epoch 227/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1123 - val_loss: 0.1487\n",
      "Epoch 228/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1148 - val_loss: 0.1682\n",
      "Epoch 229/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1130 - val_loss: 0.1389\n",
      "Epoch 230/1000\n",
      "260/260 [==============================] - 5s 17ms/step - loss: 0.1156 - val_loss: 0.1415\n",
      "Epoch 231/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1159 - val_loss: 0.1528\n",
      "Epoch 232/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1142 - val_loss: 0.1476\n",
      "Epoch 233/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1142 - val_loss: 0.1441\n",
      "Epoch 234/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1103 - val_loss: 0.1470\n",
      "Epoch 235/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1140 - val_loss: 0.1463\n",
      "Epoch 236/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1144 - val_loss: 0.1403\n",
      "Epoch 237/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1128 - val_loss: 0.1578\n",
      "Epoch 238/1000\n",
      "260/260 [==============================] - 5s 17ms/step - loss: 0.1069 - val_loss: 0.1620\n",
      "Epoch 239/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1179 - val_loss: 0.1413\n",
      "Epoch 240/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1144 - val_loss: 0.1379\n",
      "Epoch 241/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1141 - val_loss: 0.1383\n",
      "Epoch 242/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1143 - val_loss: 0.1645\n",
      "Epoch 243/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1244 - val_loss: 0.1446\n",
      "Epoch 244/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1112 - val_loss: 0.1426\n",
      "Epoch 245/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1100 - val_loss: 0.1511\n",
      "Epoch 246/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1077 - val_loss: 0.1422\n",
      "Epoch 247/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1171 - val_loss: 0.1380\n",
      "Epoch 248/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1080 - val_loss: 0.1467\n",
      "Epoch 249/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1222 - val_loss: 0.1426\n",
      "Epoch 250/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1156 - val_loss: 0.1526\n",
      "Epoch 251/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1188 - val_loss: 0.1425\n",
      "Epoch 252/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1120 - val_loss: 0.1369\n",
      "Epoch 253/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1094 - val_loss: 0.1504\n",
      "Epoch 254/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1123 - val_loss: 0.1370\n",
      "Epoch 255/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1122 - val_loss: 0.1528\n",
      "Epoch 256/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1152 - val_loss: 0.1402\n",
      "Epoch 257/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1122 - val_loss: 0.1694\n",
      "Epoch 258/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1142 - val_loss: 0.1511\n",
      "Epoch 259/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1036 - val_loss: 0.1403\n",
      "Epoch 260/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1115 - val_loss: 0.1450\n",
      "Epoch 261/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1147 - val_loss: 0.1392\n",
      "Epoch 262/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1123 - val_loss: 0.1449\n",
      "Epoch 263/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1060 - val_loss: 0.1573\n",
      "Epoch 264/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1128 - val_loss: 0.1896\n",
      "Epoch 265/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1162 - val_loss: 0.1537\n",
      "Epoch 266/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1106 - val_loss: 0.1835\n",
      "Epoch 267/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1134 - val_loss: 0.1486\n",
      "Epoch 268/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1052 - val_loss: 0.1592\n",
      "Epoch 269/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1099 - val_loss: 0.1448\n",
      "Epoch 270/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1124 - val_loss: 0.1426\n",
      "Epoch 271/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1194 - val_loss: 0.1409\n",
      "Epoch 272/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1016 - val_loss: 0.1437\n",
      "Epoch 273/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1183 - val_loss: 0.1447\n",
      "Epoch 274/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1057 - val_loss: 0.1369\n",
      "Epoch 275/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1072 - val_loss: 0.1484\n",
      "Epoch 276/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1173 - val_loss: 0.1573\n",
      "Epoch 277/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1161 - val_loss: 0.1443\n",
      "Epoch 278/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1096 - val_loss: 0.1442\n",
      "Epoch 279/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1062 - val_loss: 0.1540\n",
      "Epoch 280/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.1083 - val_loss: 0.1394\n",
      "Epoch 281/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1131 - val_loss: 0.1352\n",
      "Epoch 282/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1088 - val_loss: 0.1353\n",
      "Epoch 283/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1084 - val_loss: 0.1420\n",
      "Epoch 284/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1131 - val_loss: 0.1402\n",
      "Epoch 285/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1142 - val_loss: 0.1393\n",
      "Epoch 286/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1078 - val_loss: 0.1572\n",
      "Epoch 287/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1092 - val_loss: 0.1457\n",
      "Epoch 288/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1087 - val_loss: 0.1399\n",
      "Epoch 289/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1191 - val_loss: 0.1421\n",
      "Epoch 290/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1042 - val_loss: 0.1394\n",
      "Epoch 291/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1119 - val_loss: 0.1403\n",
      "Epoch 292/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1109 - val_loss: 0.1356\n",
      "Epoch 293/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1062 - val_loss: 0.1438\n",
      "Epoch 294/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1110 - val_loss: 0.1436\n",
      "Epoch 295/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1104 - val_loss: 0.1544\n",
      "Epoch 296/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1108 - val_loss: 0.1477\n",
      "Epoch 297/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1120 - val_loss: 0.1482\n",
      "Epoch 298/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1127 - val_loss: 0.1534\n",
      "Epoch 299/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1194 - val_loss: 0.1577\n",
      "Epoch 300/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1144 - val_loss: 0.1494\n",
      "Epoch 301/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1129 - val_loss: 0.1418\n",
      "Epoch 302/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1049 - val_loss: 0.1454\n",
      "Epoch 303/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1123 - val_loss: 0.1409\n",
      "Epoch 304/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1042 - val_loss: 0.1392\n",
      "Epoch 305/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1131 - val_loss: 0.1401\n",
      "Epoch 306/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1050 - val_loss: 0.1496\n",
      "Epoch 307/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1079 - val_loss: 0.1391\n",
      "Epoch 308/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1067 - val_loss: 0.1411\n",
      "Epoch 309/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1105 - val_loss: 0.1418\n",
      "Epoch 310/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1035 - val_loss: 0.1534\n",
      "Epoch 311/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.1071 - val_loss: 0.1443\n",
      "\n",
      "Epoch 00311: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 312/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0934 - val_loss: 0.1265\n",
      "Epoch 313/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0901 - val_loss: 0.1244\n",
      "Epoch 314/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0834 - val_loss: 0.1278\n",
      "Epoch 315/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0846 - val_loss: 0.1220\n",
      "Epoch 316/1000\n",
      "260/260 [==============================] - 5s 17ms/step - loss: 0.0897 - val_loss: 0.1239\n",
      "Epoch 317/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0848 - val_loss: 0.1220\n",
      "Epoch 318/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0818 - val_loss: 0.1214\n",
      "Epoch 319/1000\n",
      "260/260 [==============================] - 5s 17ms/step - loss: 0.0860 - val_loss: 0.1221\n",
      "Epoch 320/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0870 - val_loss: 0.1205\n",
      "Epoch 321/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0907 - val_loss: 0.1203\n",
      "Epoch 322/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0842 - val_loss: 0.1212\n",
      "Epoch 323/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.0801 - val_loss: 0.1213\n",
      "Epoch 324/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.0810 - val_loss: 0.1223\n",
      "Epoch 325/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0872 - val_loss: 0.1206\n",
      "Epoch 326/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.0869 - val_loss: 0.1217\n",
      "Epoch 327/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0897 - val_loss: 0.1239\n",
      "Epoch 328/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0881 - val_loss: 0.1221\n",
      "Epoch 329/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0896 - val_loss: 0.1204\n",
      "Epoch 330/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.0832 - val_loss: 0.1205\n",
      "Epoch 331/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.0832 - val_loss: 0.1194\n",
      "Epoch 332/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0806 - val_loss: 0.1242\n",
      "Epoch 333/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0826 - val_loss: 0.1208\n",
      "Epoch 334/1000\n",
      "260/260 [==============================] - 5s 17ms/step - loss: 0.0842 - val_loss: 0.1201\n",
      "Epoch 335/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0838 - val_loss: 0.1197\n",
      "Epoch 336/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0831 - val_loss: 0.1244\n",
      "Epoch 337/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0831 - val_loss: 0.1238\n",
      "Epoch 338/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0828 - val_loss: 0.1201\n",
      "Epoch 339/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0826 - val_loss: 0.1221\n",
      "Epoch 340/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0802 - val_loss: 0.1211\n",
      "Epoch 341/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0816 - val_loss: 0.1217\n",
      "Epoch 342/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0858 - val_loss: 0.1200\n",
      "Epoch 343/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0841 - val_loss: 0.1209\n",
      "Epoch 344/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0810 - val_loss: 0.1200\n",
      "Epoch 345/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0837 - val_loss: 0.1207\n",
      "Epoch 346/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0806 - val_loss: 0.1195\n",
      "Epoch 347/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0876 - val_loss: 0.1199\n",
      "Epoch 348/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0837 - val_loss: 0.1196\n",
      "Epoch 349/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0819 - val_loss: 0.1194\n",
      "Epoch 350/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0782 - val_loss: 0.1196\n",
      "Epoch 351/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0882 - val_loss: 0.1196\n",
      "Epoch 352/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0888 - val_loss: 0.1198\n",
      "Epoch 353/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0867 - val_loss: 0.1203\n",
      "Epoch 354/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0814 - val_loss: 0.1192\n",
      "Epoch 355/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0849 - val_loss: 0.1192\n",
      "Epoch 356/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0858 - val_loss: 0.1219\n",
      "Epoch 357/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0851 - val_loss: 0.1202\n",
      "Epoch 358/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0860 - val_loss: 0.1194\n",
      "Epoch 359/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0827 - val_loss: 0.1201\n",
      "Epoch 360/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0888 - val_loss: 0.1203\n",
      "Epoch 361/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0840 - val_loss: 0.1215\n",
      "Epoch 362/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0848 - val_loss: 0.1198\n",
      "Epoch 363/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0852 - val_loss: 0.1209\n",
      "Epoch 364/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0819 - val_loss: 0.1192\n",
      "Epoch 365/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0815 - val_loss: 0.1202\n",
      "Epoch 366/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0784 - val_loss: 0.1208\n",
      "Epoch 367/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0834 - val_loss: 0.1301\n",
      "Epoch 368/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0827 - val_loss: 0.1197\n",
      "Epoch 369/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0818 - val_loss: 0.1191\n",
      "Epoch 370/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0830 - val_loss: 0.1200\n",
      "Epoch 371/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0798 - val_loss: 0.1201\n",
      "Epoch 372/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0847 - val_loss: 0.1247\n",
      "Epoch 373/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0823 - val_loss: 0.1200\n",
      "Epoch 374/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0791 - val_loss: 0.1190\n",
      "Epoch 375/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0790 - val_loss: 0.1205\n",
      "Epoch 376/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0814 - val_loss: 0.1220\n",
      "Epoch 377/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0809 - val_loss: 0.1198\n",
      "Epoch 378/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0804 - val_loss: 0.1206\n",
      "Epoch 379/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0853 - val_loss: 0.1221\n",
      "Epoch 380/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0886 - val_loss: 0.1212\n",
      "Epoch 381/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0835 - val_loss: 0.1212\n",
      "Epoch 382/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0827 - val_loss: 0.1191\n",
      "Epoch 383/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0868 - val_loss: 0.1191\n",
      "Epoch 384/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0810 - val_loss: 0.1189\n",
      "Epoch 385/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0843 - val_loss: 0.1202\n",
      "Epoch 386/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0822 - val_loss: 0.1219\n",
      "Epoch 387/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0793 - val_loss: 0.1213\n",
      "Epoch 388/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0876 - val_loss: 0.1213\n",
      "Epoch 389/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0790 - val_loss: 0.1194\n",
      "Epoch 390/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0750 - val_loss: 0.1198\n",
      "Epoch 391/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0772 - val_loss: 0.1247\n",
      "Epoch 392/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0782 - val_loss: 0.1186\n",
      "Epoch 393/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0825 - val_loss: 0.1203\n",
      "Epoch 394/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0810 - val_loss: 0.1192\n",
      "Epoch 395/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0733 - val_loss: 0.1207\n",
      "Epoch 396/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0841 - val_loss: 0.1202\n",
      "Epoch 397/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0830 - val_loss: 0.1193\n",
      "Epoch 398/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0841 - val_loss: 0.1197\n",
      "Epoch 399/1000\n",
      "260/260 [==============================] - 5s 17ms/step - loss: 0.0825 - val_loss: 0.1188\n",
      "Epoch 400/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0817 - val_loss: 0.1234\n",
      "Epoch 401/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0808 - val_loss: 0.1196\n",
      "Epoch 402/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0844 - val_loss: 0.1189\n",
      "Epoch 403/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0816 - val_loss: 0.1190\n",
      "Epoch 404/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0835 - val_loss: 0.1189\n",
      "Epoch 405/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0749 - val_loss: 0.1189\n",
      "Epoch 406/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0786 - val_loss: 0.1193\n",
      "Epoch 407/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0798 - val_loss: 0.1194\n",
      "Epoch 408/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0762 - val_loss: 0.1188\n",
      "Epoch 409/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0744 - val_loss: 0.1187\n",
      "Epoch 410/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0812 - val_loss: 0.1198\n",
      "Epoch 411/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0773 - val_loss: 0.1194\n",
      "Epoch 412/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0792 - val_loss: 0.1187\n",
      "Epoch 413/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0845 - val_loss: 0.1250\n",
      "Epoch 414/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0841 - val_loss: 0.1191\n",
      "Epoch 415/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0814 - val_loss: 0.1187\n",
      "Epoch 416/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0828 - val_loss: 0.1208\n",
      "Epoch 417/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0813 - val_loss: 0.1190\n",
      "Epoch 418/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0898 - val_loss: 0.1187\n",
      "Epoch 419/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0776 - val_loss: 0.1196\n",
      "Epoch 420/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0811 - val_loss: 0.1191\n",
      "Epoch 421/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0827 - val_loss: 0.1184\n",
      "Epoch 422/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0755 - val_loss: 0.1203\n",
      "Epoch 423/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0801 - val_loss: 0.1191\n",
      "Epoch 424/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0759 - val_loss: 0.1232\n",
      "Epoch 425/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0792 - val_loss: 0.1187\n",
      "Epoch 426/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0820 - val_loss: 0.1184\n",
      "Epoch 427/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0835 - val_loss: 0.1204\n",
      "Epoch 428/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0862 - val_loss: 0.1233\n",
      "Epoch 429/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0781 - val_loss: 0.1190\n",
      "Epoch 430/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0742 - val_loss: 0.1186\n",
      "Epoch 431/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0814 - val_loss: 0.1193\n",
      "Epoch 432/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0812 - val_loss: 0.1190\n",
      "Epoch 433/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0848 - val_loss: 0.1202\n",
      "Epoch 434/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0784 - val_loss: 0.1187\n",
      "Epoch 435/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0799 - val_loss: 0.1205\n",
      "Epoch 436/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0830 - val_loss: 0.1217\n",
      "Epoch 437/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0844 - val_loss: 0.1213\n",
      "Epoch 438/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0798 - val_loss: 0.1220\n",
      "Epoch 439/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0797 - val_loss: 0.1189\n",
      "Epoch 440/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0743 - val_loss: 0.1202\n",
      "Epoch 441/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0922 - val_loss: 0.1197\n",
      "Epoch 442/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0765 - val_loss: 0.1183\n",
      "Epoch 443/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0850 - val_loss: 0.1187\n",
      "Epoch 444/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0817 - val_loss: 0.1195\n",
      "Epoch 445/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0786 - val_loss: 0.1182\n",
      "Epoch 446/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0854 - val_loss: 0.1193\n",
      "Epoch 447/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0790 - val_loss: 0.1218\n",
      "Epoch 448/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0766 - val_loss: 0.1222\n",
      "Epoch 449/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0839 - val_loss: 0.1189\n",
      "Epoch 450/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0834 - val_loss: 0.1187\n",
      "Epoch 451/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.0802 - val_loss: 0.1258\n",
      "Epoch 452/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.0785 - val_loss: 0.1194\n",
      "Epoch 453/1000\n",
      "260/260 [==============================] - 5s 17ms/step - loss: 0.0841 - val_loss: 0.1197\n",
      "Epoch 454/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0835 - val_loss: 0.1185\n",
      "Epoch 455/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0848 - val_loss: 0.1195\n",
      "Epoch 456/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.0883 - val_loss: 0.1191\n",
      "Epoch 457/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0820 - val_loss: 0.1196\n",
      "Epoch 458/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.0849 - val_loss: 0.1221\n",
      "Epoch 459/1000\n",
      "260/260 [==============================] - 5s 19ms/step - loss: 0.0816 - val_loss: 0.1188\n",
      "Epoch 460/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.0850 - val_loss: 0.1201\n",
      "Epoch 461/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0775 - val_loss: 0.1206\n",
      "Epoch 462/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0795 - val_loss: 0.1233\n",
      "Epoch 463/1000\n",
      "260/260 [==============================] - 5s 17ms/step - loss: 0.0849 - val_loss: 0.1199\n",
      "Epoch 464/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0779 - val_loss: 0.1188\n",
      "Epoch 465/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.0841 - val_loss: 0.1192\n",
      "Epoch 466/1000\n",
      "260/260 [==============================] - 5s 19ms/step - loss: 0.0824 - val_loss: 0.1184\n",
      "Epoch 467/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0808 - val_loss: 0.1202\n",
      "Epoch 468/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.0776 - val_loss: 0.1225\n",
      "Epoch 469/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0736 - val_loss: 0.1184\n",
      "Epoch 470/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.0766 - val_loss: 0.1199\n",
      "Epoch 471/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0845 - val_loss: 0.1185\n",
      "Epoch 472/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0810 - val_loss: 0.1194\n",
      "\n",
      "Epoch 00472: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 473/1000\n",
      "260/260 [==============================] - 5s 18ms/step - loss: 0.0826 - val_loss: 0.1173\n",
      "Epoch 474/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0742 - val_loss: 0.1173\n",
      "Epoch 475/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0806 - val_loss: 0.1172\n",
      "Epoch 476/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0822 - val_loss: 0.1171\n",
      "Epoch 477/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0834 - val_loss: 0.1172\n",
      "Epoch 478/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0814 - val_loss: 0.1172\n",
      "Epoch 479/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0745 - val_loss: 0.1171\n",
      "Epoch 480/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0750 - val_loss: 0.1170\n",
      "Epoch 481/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0790 - val_loss: 0.1170\n",
      "Epoch 482/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0743 - val_loss: 0.1170\n",
      "Epoch 483/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0809 - val_loss: 0.1170\n",
      "Epoch 484/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0796 - val_loss: 0.1171\n",
      "Epoch 485/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0835 - val_loss: 0.1170\n",
      "Epoch 486/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0697 - val_loss: 0.1169\n",
      "Epoch 487/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0746 - val_loss: 0.1170\n",
      "Epoch 488/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0757 - val_loss: 0.1171\n",
      "Epoch 489/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0742 - val_loss: 0.1170\n",
      "Epoch 490/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0761 - val_loss: 0.1169\n",
      "Epoch 491/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0805 - val_loss: 0.1168\n",
      "Epoch 492/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0774 - val_loss: 0.1170\n",
      "Epoch 493/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0781 - val_loss: 0.1169\n",
      "Epoch 494/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0798 - val_loss: 0.1170\n",
      "Epoch 495/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0737 - val_loss: 0.1171\n",
      "Epoch 496/1000\n",
      "260/260 [==============================] - 5s 17ms/step - loss: 0.0741 - val_loss: 0.1170\n",
      "Epoch 497/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0747 - val_loss: 0.1170\n",
      "Epoch 498/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0740 - val_loss: 0.1171\n",
      "Epoch 499/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0770 - val_loss: 0.1169\n",
      "Epoch 500/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0804 - val_loss: 0.1171\n",
      "Epoch 501/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0800 - val_loss: 0.1168\n",
      "Epoch 502/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0799 - val_loss: 0.1168\n",
      "Epoch 503/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0772 - val_loss: 0.1169\n",
      "Epoch 504/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0717 - val_loss: 0.1171\n",
      "Epoch 505/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0784 - val_loss: 0.1168\n",
      "Epoch 506/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0734 - val_loss: 0.1169\n",
      "Epoch 507/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0761 - val_loss: 0.1169\n",
      "Epoch 508/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0809 - val_loss: 0.1169\n",
      "Epoch 509/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0698 - val_loss: 0.1171\n",
      "Epoch 510/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0802 - val_loss: 0.1170\n",
      "Epoch 511/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0795 - val_loss: 0.1169\n",
      "Epoch 512/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0758 - val_loss: 0.1171\n",
      "Epoch 513/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0862 - val_loss: 0.1169\n",
      "Epoch 514/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0829 - val_loss: 0.1169\n",
      "Epoch 515/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0772 - val_loss: 0.1168\n",
      "Epoch 516/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0840 - val_loss: 0.1170\n",
      "Epoch 517/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0795 - val_loss: 0.1168\n",
      "Epoch 518/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0750 - val_loss: 0.1169\n",
      "Epoch 519/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0751 - val_loss: 0.1168\n",
      "Epoch 520/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0758 - val_loss: 0.1169\n",
      "Epoch 521/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0745 - val_loss: 0.1167\n",
      "Epoch 522/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0825 - val_loss: 0.1168\n",
      "Epoch 523/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0776 - val_loss: 0.1170\n",
      "Epoch 524/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0818 - val_loss: 0.1169\n",
      "Epoch 525/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0714 - val_loss: 0.1168\n",
      "Epoch 526/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0877 - val_loss: 0.1167\n",
      "Epoch 527/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0699 - val_loss: 0.1168\n",
      "Epoch 528/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0745 - val_loss: 0.1171\n",
      "Epoch 529/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0778 - val_loss: 0.1168\n",
      "Epoch 530/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0770 - val_loss: 0.1168\n",
      "Epoch 531/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0778 - val_loss: 0.1168\n",
      "Epoch 532/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0809 - val_loss: 0.1169\n",
      "Epoch 533/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0698 - val_loss: 0.1170\n",
      "Epoch 534/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0749 - val_loss: 0.1169\n",
      "Epoch 535/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0866 - val_loss: 0.1168\n",
      "Epoch 536/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0795 - val_loss: 0.1168\n",
      "Epoch 537/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0832 - val_loss: 0.1171\n",
      "Epoch 538/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0769 - val_loss: 0.1171\n",
      "Epoch 539/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0848 - val_loss: 0.1170\n",
      "Epoch 540/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0732 - val_loss: 0.1167\n",
      "Epoch 541/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0834 - val_loss: 0.1168\n",
      "Epoch 542/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0809 - val_loss: 0.1168\n",
      "Epoch 543/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0759 - val_loss: 0.1168\n",
      "Epoch 544/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0757 - val_loss: 0.1169\n",
      "Epoch 545/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0819 - val_loss: 0.1169\n",
      "Epoch 546/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0817 - val_loss: 0.1168\n",
      "Epoch 547/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0726 - val_loss: 0.1168\n",
      "Epoch 548/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0751 - val_loss: 0.1171\n",
      "Epoch 549/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0756 - val_loss: 0.1167\n",
      "Epoch 550/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0712 - val_loss: 0.1167\n",
      "Epoch 551/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0791 - val_loss: 0.1169\n",
      "\n",
      "Epoch 00551: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 552/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0892 - val_loss: 0.1167\n",
      "Epoch 553/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0861 - val_loss: 0.1167\n",
      "Epoch 554/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0786 - val_loss: 0.1167\n",
      "Epoch 555/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0801 - val_loss: 0.1167\n",
      "Epoch 556/1000\n",
      "260/260 [==============================] - 4s 17ms/step - loss: 0.0824 - val_loss: 0.1168\n",
      "Epoch 557/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0769 - val_loss: 0.1167\n",
      "Epoch 558/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0737 - val_loss: 0.1167\n",
      "Epoch 559/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0786 - val_loss: 0.1168\n",
      "Epoch 560/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0768 - val_loss: 0.1167\n",
      "Epoch 561/1000\n",
      "260/260 [==============================] - 4s 16ms/step - loss: 0.0758 - val_loss: 0.1167\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00561: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.1478362\n",
      "Training 3JHC out of ['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN']\n",
      "Categories (8, object): ['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN'] \n",
      "\n",
      "Epoch 1/1000\n",
      "665/665 [==============================] - 14s 17ms/step - loss: 1.5223 - val_loss: 0.7641\n",
      "Epoch 2/1000\n",
      "665/665 [==============================] - 12s 17ms/step - loss: 0.6137 - val_loss: 0.6165\n",
      "Epoch 3/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.5450 - val_loss: 0.5302\n",
      "Epoch 4/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.5060 - val_loss: 0.5059\n",
      "Epoch 5/1000\n",
      "665/665 [==============================] - 12s 18ms/step - loss: 0.4755 - val_loss: 0.4876\n",
      "Epoch 6/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.4527 - val_loss: 0.4570\n",
      "Epoch 7/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.4379 - val_loss: 0.4446\n",
      "Epoch 8/1000\n",
      "665/665 [==============================] - 12s 18ms/step - loss: 0.4220 - val_loss: 0.4420\n",
      "Epoch 9/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.4122 - val_loss: 0.4273\n",
      "Epoch 10/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.4003 - val_loss: 0.4369\n",
      "Epoch 11/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.3884 - val_loss: 0.4091\n",
      "Epoch 12/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.3779 - val_loss: 0.4072\n",
      "Epoch 13/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.3697 - val_loss: 0.4008\n",
      "Epoch 14/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.3636 - val_loss: 0.3937\n",
      "Epoch 15/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.3573 - val_loss: 0.3957\n",
      "Epoch 16/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.3503 - val_loss: 0.3679\n",
      "Epoch 17/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.3447 - val_loss: 0.3724\n",
      "Epoch 18/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.3382 - val_loss: 0.3571\n",
      "Epoch 19/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.3324 - val_loss: 0.3551\n",
      "Epoch 20/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.3272 - val_loss: 0.3601\n",
      "Epoch 21/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.3263 - val_loss: 0.3377\n",
      "Epoch 22/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.3186 - val_loss: 0.3567\n",
      "Epoch 23/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.3125 - val_loss: 0.3472\n",
      "Epoch 24/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.3101 - val_loss: 0.3353\n",
      "Epoch 25/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.3067 - val_loss: 0.3354\n",
      "Epoch 26/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.3038 - val_loss: 0.3385\n",
      "Epoch 27/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.3003 - val_loss: 0.3277\n",
      "Epoch 28/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2968 - val_loss: 0.3338\n",
      "Epoch 29/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2931 - val_loss: 0.3243\n",
      "Epoch 30/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2882 - val_loss: 0.3280\n",
      "Epoch 31/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2867 - val_loss: 0.3287\n",
      "Epoch 32/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2859 - val_loss: 0.3192\n",
      "Epoch 33/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2801 - val_loss: 0.3171\n",
      "Epoch 34/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.2796 - val_loss: 0.3206\n",
      "Epoch 35/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2741 - val_loss: 0.3095\n",
      "Epoch 36/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2750 - val_loss: 0.3158\n",
      "Epoch 37/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2720 - val_loss: 0.3088\n",
      "Epoch 38/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2685 - val_loss: 0.3027\n",
      "Epoch 39/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2668 - val_loss: 0.3085\n",
      "Epoch 40/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2662 - val_loss: 0.3084\n",
      "Epoch 41/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2621 - val_loss: 0.3096\n",
      "Epoch 42/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2604 - val_loss: 0.3391\n",
      "Epoch 43/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2594 - val_loss: 0.3118\n",
      "Epoch 44/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2586 - val_loss: 0.3052\n",
      "Epoch 45/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2561 - val_loss: 0.2987\n",
      "Epoch 46/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2555 - val_loss: 0.2946\n",
      "Epoch 47/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2500 - val_loss: 0.2982\n",
      "Epoch 48/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2486 - val_loss: 0.3203\n",
      "Epoch 49/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2505 - val_loss: 0.2958\n",
      "Epoch 50/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2482 - val_loss: 0.2966\n",
      "Epoch 51/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2485 - val_loss: 0.2921\n",
      "Epoch 52/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2449 - val_loss: 0.2999\n",
      "Epoch 53/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2425 - val_loss: 0.2892\n",
      "Epoch 54/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2422 - val_loss: 0.3058\n",
      "Epoch 55/1000\n",
      "665/665 [==============================] - 12s 18ms/step - loss: 0.2404 - val_loss: 0.2901\n",
      "Epoch 56/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.2404 - val_loss: 0.2900\n",
      "Epoch 57/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.2414 - val_loss: 0.2822\n",
      "Epoch 58/1000\n",
      "665/665 [==============================] - 12s 18ms/step - loss: 0.2363 - val_loss: 0.2974\n",
      "Epoch 59/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.2348 - val_loss: 0.2842\n",
      "Epoch 60/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.2365 - val_loss: 0.2909\n",
      "Epoch 61/1000\n",
      "665/665 [==============================] - 12s 18ms/step - loss: 0.2318 - val_loss: 0.2825\n",
      "Epoch 62/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.2327 - val_loss: 0.2882\n",
      "Epoch 63/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.2307 - val_loss: 0.2825\n",
      "Epoch 64/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.2320 - val_loss: 0.2830\n",
      "Epoch 65/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2301 - val_loss: 0.2950\n",
      "Epoch 66/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2286 - val_loss: 0.2844\n",
      "Epoch 67/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2253 - val_loss: 0.2770\n",
      "Epoch 68/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2245 - val_loss: 0.2841\n",
      "Epoch 69/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2259 - val_loss: 0.2872\n",
      "Epoch 70/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2218 - val_loss: 0.2801\n",
      "Epoch 71/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2248 - val_loss: 0.2752\n",
      "Epoch 72/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2203 - val_loss: 0.2912\n",
      "Epoch 73/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2217 - val_loss: 0.2766\n",
      "Epoch 74/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2188 - val_loss: 0.2845\n",
      "Epoch 75/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2197 - val_loss: 0.2756\n",
      "Epoch 76/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2173 - val_loss: 0.2792\n",
      "Epoch 77/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2173 - val_loss: 0.2758\n",
      "Epoch 78/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2166 - val_loss: 0.2738\n",
      "Epoch 79/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2143 - val_loss: 0.2836\n",
      "Epoch 80/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2149 - val_loss: 0.2750\n",
      "Epoch 81/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2138 - val_loss: 0.2708\n",
      "Epoch 82/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2123 - val_loss: 0.2779\n",
      "Epoch 83/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2116 - val_loss: 0.2811\n",
      "Epoch 84/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2105 - val_loss: 0.2768\n",
      "Epoch 85/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2090 - val_loss: 0.2821\n",
      "Epoch 86/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2085 - val_loss: 0.2723\n",
      "Epoch 87/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2114 - val_loss: 0.2740\n",
      "Epoch 88/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2099 - val_loss: 0.2744\n",
      "Epoch 89/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2072 - val_loss: 0.2687\n",
      "Epoch 90/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2059 - val_loss: 0.2735\n",
      "Epoch 91/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2087 - val_loss: 0.2762\n",
      "Epoch 92/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2071 - val_loss: 0.2702\n",
      "Epoch 93/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2055 - val_loss: 0.2715\n",
      "Epoch 94/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2035 - val_loss: 0.2757\n",
      "Epoch 95/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2048 - val_loss: 0.2760\n",
      "Epoch 96/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2026 - val_loss: 0.2709\n",
      "Epoch 97/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2030 - val_loss: 0.2732\n",
      "Epoch 98/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2059 - val_loss: 0.2800\n",
      "Epoch 99/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2043 - val_loss: 0.2669\n",
      "Epoch 100/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2017 - val_loss: 0.2738\n",
      "Epoch 101/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2003 - val_loss: 0.2665\n",
      "Epoch 102/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2006 - val_loss: 0.2799\n",
      "Epoch 103/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.2000 - val_loss: 0.2658\n",
      "Epoch 104/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1974 - val_loss: 0.2653\n",
      "Epoch 105/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1972 - val_loss: 0.2701\n",
      "Epoch 106/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1982 - val_loss: 0.2649\n",
      "Epoch 107/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1980 - val_loss: 0.2807\n",
      "Epoch 108/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1976 - val_loss: 0.2645\n",
      "Epoch 109/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1999 - val_loss: 0.2653\n",
      "Epoch 110/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1965 - val_loss: 0.2630\n",
      "Epoch 111/1000\n",
      "665/665 [==============================] - 12s 18ms/step - loss: 0.1957 - val_loss: 0.2805\n",
      "Epoch 112/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1946 - val_loss: 0.2747\n",
      "Epoch 113/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1953 - val_loss: 0.2693\n",
      "Epoch 114/1000\n",
      "665/665 [==============================] - 12s 17ms/step - loss: 0.1953 - val_loss: 0.2652\n",
      "Epoch 115/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1926 - val_loss: 0.2670\n",
      "Epoch 116/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1945 - val_loss: 0.2785\n",
      "Epoch 117/1000\n",
      "665/665 [==============================] - 12s 18ms/step - loss: 0.1940 - val_loss: 0.2633\n",
      "Epoch 118/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1941 - val_loss: 0.2745\n",
      "Epoch 119/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1932 - val_loss: 0.2611\n",
      "Epoch 120/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1915 - val_loss: 0.2700\n",
      "Epoch 121/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1908 - val_loss: 0.2598\n",
      "Epoch 122/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1917 - val_loss: 0.2657\n",
      "Epoch 123/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1914 - val_loss: 0.2679\n",
      "Epoch 124/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1865 - val_loss: 0.2646\n",
      "Epoch 125/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1890 - val_loss: 0.2588\n",
      "Epoch 126/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1905 - val_loss: 0.2717\n",
      "Epoch 127/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1882 - val_loss: 0.2655\n",
      "Epoch 128/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1887 - val_loss: 0.2592\n",
      "Epoch 129/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1878 - val_loss: 0.2584\n",
      "Epoch 130/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1859 - val_loss: 0.2637\n",
      "Epoch 131/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1857 - val_loss: 0.2602\n",
      "Epoch 132/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1848 - val_loss: 0.2645\n",
      "Epoch 133/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1873 - val_loss: 0.2638\n",
      "Epoch 134/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1872 - val_loss: 0.2662\n",
      "Epoch 135/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1872 - val_loss: 0.2605\n",
      "Epoch 136/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1844 - val_loss: 0.2641\n",
      "Epoch 137/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1865 - val_loss: 0.2654\n",
      "Epoch 138/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1840 - val_loss: 0.2633\n",
      "Epoch 139/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1844 - val_loss: 0.2605\n",
      "Epoch 140/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1823 - val_loss: 0.2624\n",
      "Epoch 141/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1823 - val_loss: 0.2637\n",
      "Epoch 142/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1804 - val_loss: 0.2595\n",
      "Epoch 143/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1825 - val_loss: 0.2589\n",
      "Epoch 144/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1820 - val_loss: 0.2612\n",
      "Epoch 145/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1807 - val_loss: 0.2700\n",
      "Epoch 146/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1823 - val_loss: 0.2594\n",
      "Epoch 147/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1822 - val_loss: 0.2577\n",
      "Epoch 148/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1820 - val_loss: 0.2622\n",
      "Epoch 149/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1819 - val_loss: 0.2601\n",
      "Epoch 150/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1798 - val_loss: 0.2608\n",
      "Epoch 151/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1795 - val_loss: 0.2534\n",
      "Epoch 152/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1776 - val_loss: 0.2561\n",
      "Epoch 153/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1804 - val_loss: 0.2675\n",
      "Epoch 154/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1804 - val_loss: 0.2545\n",
      "Epoch 155/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1784 - val_loss: 0.2602\n",
      "Epoch 156/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1793 - val_loss: 0.2609\n",
      "Epoch 157/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1782 - val_loss: 0.2569\n",
      "Epoch 158/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1794 - val_loss: 0.2736\n",
      "Epoch 159/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1807 - val_loss: 0.2641\n",
      "Epoch 160/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1793 - val_loss: 0.2574\n",
      "Epoch 161/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1767 - val_loss: 0.2546\n",
      "Epoch 162/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1779 - val_loss: 0.2532\n",
      "Epoch 163/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1761 - val_loss: 0.2567\n",
      "Epoch 164/1000\n",
      "665/665 [==============================] - 12s 18ms/step - loss: 0.1745 - val_loss: 0.2600\n",
      "Epoch 165/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1789 - val_loss: 0.2595\n",
      "Epoch 166/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1759 - val_loss: 0.2563\n",
      "Epoch 167/1000\n",
      "665/665 [==============================] - 12s 18ms/step - loss: 0.1753 - val_loss: 0.2568\n",
      "Epoch 168/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1748 - val_loss: 0.2578\n",
      "Epoch 169/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1764 - val_loss: 0.2551\n",
      "Epoch 170/1000\n",
      "665/665 [==============================] - 12s 18ms/step - loss: 0.1746 - val_loss: 0.2559\n",
      "Epoch 171/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1731 - val_loss: 0.2618\n",
      "Epoch 172/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1743 - val_loss: 0.2604\n",
      "Epoch 173/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1712 - val_loss: 0.2564\n",
      "Epoch 174/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1736 - val_loss: 0.2640\n",
      "Epoch 175/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1739 - val_loss: 0.2574\n",
      "Epoch 176/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1734 - val_loss: 0.2587\n",
      "Epoch 177/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1735 - val_loss: 0.2515\n",
      "Epoch 178/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1709 - val_loss: 0.2540\n",
      "Epoch 179/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1706 - val_loss: 0.2629\n",
      "Epoch 180/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1738 - val_loss: 0.2522\n",
      "Epoch 181/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1712 - val_loss: 0.2523\n",
      "Epoch 182/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1681 - val_loss: 0.2516\n",
      "Epoch 183/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1710 - val_loss: 0.2498\n",
      "Epoch 184/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1709 - val_loss: 0.2557\n",
      "Epoch 185/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1697 - val_loss: 0.2549\n",
      "Epoch 186/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1723 - val_loss: 0.2539\n",
      "Epoch 187/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1715 - val_loss: 0.2535\n",
      "Epoch 188/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1742 - val_loss: 0.2562\n",
      "Epoch 189/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1684 - val_loss: 0.2538\n",
      "Epoch 190/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1682 - val_loss: 0.2570\n",
      "Epoch 191/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1693 - val_loss: 0.2516\n",
      "Epoch 192/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1678 - val_loss: 0.2536\n",
      "Epoch 193/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1682 - val_loss: 0.2502\n",
      "Epoch 194/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1681 - val_loss: 0.2545\n",
      "Epoch 195/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1695 - val_loss: 0.2525\n",
      "Epoch 196/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1706 - val_loss: 0.2499\n",
      "Epoch 197/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1654 - val_loss: 0.2556\n",
      "Epoch 198/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1657 - val_loss: 0.2522\n",
      "Epoch 199/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1680 - val_loss: 0.2532\n",
      "Epoch 200/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1674 - val_loss: 0.2586\n",
      "Epoch 201/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1678 - val_loss: 0.2568\n",
      "Epoch 202/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1677 - val_loss: 0.2531\n",
      "Epoch 203/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1669 - val_loss: 0.2477\n",
      "Epoch 204/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1643 - val_loss: 0.2558\n",
      "Epoch 205/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1658 - val_loss: 0.2488\n",
      "Epoch 206/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1652 - val_loss: 0.2515\n",
      "Epoch 207/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1674 - val_loss: 0.2484\n",
      "Epoch 208/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1642 - val_loss: 0.2562\n",
      "Epoch 209/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1642 - val_loss: 0.2566\n",
      "Epoch 210/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1656 - val_loss: 0.2510\n",
      "Epoch 211/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1640 - val_loss: 0.2523\n",
      "Epoch 212/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1646 - val_loss: 0.2486\n",
      "Epoch 213/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1668 - val_loss: 0.2505\n",
      "Epoch 214/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1635 - val_loss: 0.2520\n",
      "Epoch 215/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1639 - val_loss: 0.2518\n",
      "Epoch 216/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1662 - val_loss: 0.2526\n",
      "Epoch 217/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1661 - val_loss: 0.2624\n",
      "Epoch 218/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1646 - val_loss: 0.2542\n",
      "Epoch 219/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1636 - val_loss: 0.2538\n",
      "Epoch 220/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1622 - val_loss: 0.2515\n",
      "Epoch 221/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1643 - val_loss: 0.2508\n",
      "Epoch 222/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1675 - val_loss: 0.2564\n",
      "Epoch 223/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1675 - val_loss: 0.2510\n",
      "Epoch 224/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1626 - val_loss: 0.2492\n",
      "Epoch 225/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1633 - val_loss: 0.2541\n",
      "Epoch 226/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1633 - val_loss: 0.2604\n",
      "Epoch 227/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1609 - val_loss: 0.2470\n",
      "Epoch 228/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1619 - val_loss: 0.2503\n",
      "Epoch 229/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1598 - val_loss: 0.2519\n",
      "Epoch 230/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1615 - val_loss: 0.2525\n",
      "Epoch 231/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1615 - val_loss: 0.2564\n",
      "Epoch 232/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1595 - val_loss: 0.2517\n",
      "Epoch 233/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1624 - val_loss: 0.2515\n",
      "Epoch 234/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1624 - val_loss: 0.2485\n",
      "Epoch 235/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1597 - val_loss: 0.2492\n",
      "Epoch 236/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1611 - val_loss: 0.2447\n",
      "Epoch 237/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1602 - val_loss: 0.2514\n",
      "Epoch 238/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1601 - val_loss: 0.2501\n",
      "Epoch 239/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1581 - val_loss: 0.2517\n",
      "Epoch 240/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1598 - val_loss: 0.2457\n",
      "Epoch 241/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1575 - val_loss: 0.2570\n",
      "Epoch 242/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1603 - val_loss: 0.2471\n",
      "Epoch 243/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1580 - val_loss: 0.2501\n",
      "Epoch 244/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1573 - val_loss: 0.2577\n",
      "Epoch 245/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1576 - val_loss: 0.2457\n",
      "Epoch 246/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1586 - val_loss: 0.2483\n",
      "Epoch 247/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1566 - val_loss: 0.2496\n",
      "Epoch 248/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1584 - val_loss: 0.2520\n",
      "Epoch 249/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1603 - val_loss: 0.2445\n",
      "Epoch 250/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1589 - val_loss: 0.2463\n",
      "Epoch 251/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1576 - val_loss: 0.2460\n",
      "Epoch 252/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1567 - val_loss: 0.2430\n",
      "Epoch 253/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1567 - val_loss: 0.2510\n",
      "Epoch 254/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1596 - val_loss: 0.2525\n",
      "Epoch 255/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1561 - val_loss: 0.2502\n",
      "Epoch 256/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1577 - val_loss: 0.2458\n",
      "Epoch 257/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1569 - val_loss: 0.2450\n",
      "Epoch 258/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1552 - val_loss: 0.2472\n",
      "Epoch 259/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1563 - val_loss: 0.2471\n",
      "Epoch 260/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1582 - val_loss: 0.2502\n",
      "Epoch 261/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1551 - val_loss: 0.2433\n",
      "Epoch 262/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1584 - val_loss: 0.2484\n",
      "Epoch 263/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1575 - val_loss: 0.2512\n",
      "Epoch 264/1000\n",
      "665/665 [==============================] - 12s 18ms/step - loss: 0.1555 - val_loss: 0.2493\n",
      "Epoch 265/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1591 - val_loss: 0.2441\n",
      "Epoch 266/1000\n",
      "665/665 [==============================] - 12s 17ms/step - loss: 0.1549 - val_loss: 0.2458\n",
      "Epoch 267/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1555 - val_loss: 0.2441\n",
      "Epoch 268/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1572 - val_loss: 0.2442\n",
      "Epoch 269/1000\n",
      "665/665 [==============================] - 12s 18ms/step - loss: 0.1530 - val_loss: 0.2478\n",
      "Epoch 270/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1568 - val_loss: 0.2475\n",
      "Epoch 271/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1558 - val_loss: 0.2440\n",
      "Epoch 272/1000\n",
      "665/665 [==============================] - 12s 18ms/step - loss: 0.1553 - val_loss: 0.2490\n",
      "Epoch 273/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1570 - val_loss: 0.2451\n",
      "Epoch 274/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1559 - val_loss: 0.2441\n",
      "Epoch 275/1000\n",
      "665/665 [==============================] - 12s 18ms/step - loss: 0.1536 - val_loss: 0.2468\n",
      "Epoch 276/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1538 - val_loss: 0.2425\n",
      "Epoch 277/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1560 - val_loss: 0.2463\n",
      "Epoch 278/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1530 - val_loss: 0.2467\n",
      "Epoch 279/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1531 - val_loss: 0.2467\n",
      "Epoch 280/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1539 - val_loss: 0.2472\n",
      "Epoch 281/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1550 - val_loss: 0.2432\n",
      "Epoch 282/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1543 - val_loss: 0.2443\n",
      "Epoch 283/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1520 - val_loss: 0.2491\n",
      "Epoch 284/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1553 - val_loss: 0.2452\n",
      "Epoch 285/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1515 - val_loss: 0.2473\n",
      "Epoch 286/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1548 - val_loss: 0.2470\n",
      "Epoch 287/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1529 - val_loss: 0.2465\n",
      "Epoch 288/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1539 - val_loss: 0.2477\n",
      "Epoch 289/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1515 - val_loss: 0.2446\n",
      "Epoch 290/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1525 - val_loss: 0.2471\n",
      "Epoch 291/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1508 - val_loss: 0.2437\n",
      "Epoch 292/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1521 - val_loss: 0.2437\n",
      "Epoch 293/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1537 - val_loss: 0.2445\n",
      "Epoch 294/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1522 - val_loss: 0.2453\n",
      "Epoch 295/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1546 - val_loss: 0.2448\n",
      "Epoch 296/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1519 - val_loss: 0.2428\n",
      "Epoch 297/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1540 - val_loss: 0.2459\n",
      "Epoch 298/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1538 - val_loss: 0.2433\n",
      "Epoch 299/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1535 - val_loss: 0.2410\n",
      "Epoch 300/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1521 - val_loss: 0.2437\n",
      "Epoch 301/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1493 - val_loss: 0.2484\n",
      "Epoch 302/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1539 - val_loss: 0.2463\n",
      "Epoch 303/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1505 - val_loss: 0.2442\n",
      "Epoch 304/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1478 - val_loss: 0.2475\n",
      "Epoch 305/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1535 - val_loss: 0.2409\n",
      "Epoch 306/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1505 - val_loss: 0.2493\n",
      "Epoch 307/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1523 - val_loss: 0.2434\n",
      "Epoch 308/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1522 - val_loss: 0.2495\n",
      "Epoch 309/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1581 - val_loss: 0.2433\n",
      "Epoch 310/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1495 - val_loss: 0.2420\n",
      "Epoch 311/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1512 - val_loss: 0.2429\n",
      "Epoch 312/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1519 - val_loss: 0.2423\n",
      "Epoch 313/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1513 - val_loss: 0.2416\n",
      "Epoch 314/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1478 - val_loss: 0.2451\n",
      "Epoch 315/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1496 - val_loss: 0.2487\n",
      "Epoch 316/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1509 - val_loss: 0.2432\n",
      "Epoch 317/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1508 - val_loss: 0.2405\n",
      "Epoch 318/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1482 - val_loss: 0.2478\n",
      "Epoch 319/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1523 - val_loss: 0.2425\n",
      "Epoch 320/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1486 - val_loss: 0.2450\n",
      "Epoch 321/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1466 - val_loss: 0.2429\n",
      "Epoch 322/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1500 - val_loss: 0.2415\n",
      "Epoch 323/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1480 - val_loss: 0.2432\n",
      "Epoch 324/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1475 - val_loss: 0.2407\n",
      "Epoch 325/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1491 - val_loss: 0.2417\n",
      "Epoch 326/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1460 - val_loss: 0.2429\n",
      "Epoch 327/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1487 - val_loss: 0.2409\n",
      "Epoch 328/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1499 - val_loss: 0.2415\n",
      "Epoch 329/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1490 - val_loss: 0.2482\n",
      "Epoch 330/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1472 - val_loss: 0.2406\n",
      "Epoch 331/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1496 - val_loss: 0.2477\n",
      "Epoch 332/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1481 - val_loss: 0.2437\n",
      "Epoch 333/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1481 - val_loss: 0.2412\n",
      "Epoch 334/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1482 - val_loss: 0.2480\n",
      "Epoch 335/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1452 - val_loss: 0.2437\n",
      "Epoch 336/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1478 - val_loss: 0.2467\n",
      "Epoch 337/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1498 - val_loss: 0.2466\n",
      "Epoch 338/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1472 - val_loss: 0.2431\n",
      "Epoch 339/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1474 - val_loss: 0.2396\n",
      "Epoch 340/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1461 - val_loss: 0.2386\n",
      "Epoch 341/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1457 - val_loss: 0.2401\n",
      "Epoch 342/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1462 - val_loss: 0.2502\n",
      "Epoch 343/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1477 - val_loss: 0.2419\n",
      "Epoch 344/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1478 - val_loss: 0.2432\n",
      "Epoch 345/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1484 - val_loss: 0.2450\n",
      "Epoch 346/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1447 - val_loss: 0.2433\n",
      "Epoch 347/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1460 - val_loss: 0.2489\n",
      "Epoch 348/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1486 - val_loss: 0.2420\n",
      "Epoch 349/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1428 - val_loss: 0.2414\n",
      "Epoch 350/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1474 - val_loss: 0.2466\n",
      "Epoch 351/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1439 - val_loss: 0.2415\n",
      "Epoch 352/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1479 - val_loss: 0.2401\n",
      "Epoch 353/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1466 - val_loss: 0.2465\n",
      "Epoch 354/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1465 - val_loss: 0.2421\n",
      "Epoch 355/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1466 - val_loss: 0.2389\n",
      "Epoch 356/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1447 - val_loss: 0.2401\n",
      "Epoch 357/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1464 - val_loss: 0.2410\n",
      "Epoch 358/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1461 - val_loss: 0.2398\n",
      "Epoch 359/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1471 - val_loss: 0.2421\n",
      "Epoch 360/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1464 - val_loss: 0.2495\n",
      "Epoch 361/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1488 - val_loss: 0.2389\n",
      "Epoch 362/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1455 - val_loss: 0.2476\n",
      "Epoch 363/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1461 - val_loss: 0.2471\n",
      "Epoch 364/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1459 - val_loss: 0.2389\n",
      "Epoch 365/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1433 - val_loss: 0.2396\n",
      "Epoch 366/1000\n",
      "665/665 [==============================] - 12s 17ms/step - loss: 0.1457 - val_loss: 0.2388\n",
      "Epoch 367/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1445 - val_loss: 0.2445\n",
      "Epoch 368/1000\n",
      "665/665 [==============================] - 12s 17ms/step - loss: 0.1405 - val_loss: 0.2393\n",
      "Epoch 369/1000\n",
      "665/665 [==============================] - 12s 18ms/step - loss: 0.1432 - val_loss: 0.2396\n",
      "Epoch 370/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1434 - val_loss: 0.2431\n",
      "\n",
      "Epoch 00370: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 371/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1294 - val_loss: 0.2269\n",
      "Epoch 372/1000\n",
      "665/665 [==============================] - 12s 18ms/step - loss: 0.1212 - val_loss: 0.2247\n",
      "Epoch 373/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1199 - val_loss: 0.2249\n",
      "Epoch 374/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1198 - val_loss: 0.2236\n",
      "Epoch 375/1000\n",
      "665/665 [==============================] - 12s 18ms/step - loss: 0.1192 - val_loss: 0.2238\n",
      "Epoch 376/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1164 - val_loss: 0.2238\n",
      "Epoch 377/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1192 - val_loss: 0.2230\n",
      "Epoch 378/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1164 - val_loss: 0.2233\n",
      "Epoch 379/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1164 - val_loss: 0.2237\n",
      "Epoch 380/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1146 - val_loss: 0.2230\n",
      "Epoch 381/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1150 - val_loss: 0.2227\n",
      "Epoch 382/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1176 - val_loss: 0.2228\n",
      "Epoch 383/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1137 - val_loss: 0.2226\n",
      "Epoch 384/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1154 - val_loss: 0.2245\n",
      "Epoch 385/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1159 - val_loss: 0.2226\n",
      "Epoch 386/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1164 - val_loss: 0.2232\n",
      "Epoch 387/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1144 - val_loss: 0.2221\n",
      "Epoch 388/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1145 - val_loss: 0.2227\n",
      "Epoch 389/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1149 - val_loss: 0.2228\n",
      "Epoch 390/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1167 - val_loss: 0.2233\n",
      "Epoch 391/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1130 - val_loss: 0.2228\n",
      "Epoch 392/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1158 - val_loss: 0.2249\n",
      "Epoch 393/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1152 - val_loss: 0.2222\n",
      "Epoch 394/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1141 - val_loss: 0.2230\n",
      "Epoch 395/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1128 - val_loss: 0.2221\n",
      "Epoch 396/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1146 - val_loss: 0.2240\n",
      "Epoch 397/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1150 - val_loss: 0.2225\n",
      "Epoch 398/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1150 - val_loss: 0.2227\n",
      "Epoch 399/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1120 - val_loss: 0.2221\n",
      "Epoch 400/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1132 - val_loss: 0.2223\n",
      "Epoch 401/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1169 - val_loss: 0.2221\n",
      "Epoch 402/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1153 - val_loss: 0.2221\n",
      "Epoch 403/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1135 - val_loss: 0.2222\n",
      "Epoch 404/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1145 - val_loss: 0.2223\n",
      "Epoch 405/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1124 - val_loss: 0.2235\n",
      "Epoch 406/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1169 - val_loss: 0.2224\n",
      "Epoch 407/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1140 - val_loss: 0.2224\n",
      "Epoch 408/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1134 - val_loss: 0.2227\n",
      "Epoch 409/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1126 - val_loss: 0.2220\n",
      "Epoch 410/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1156 - val_loss: 0.2228\n",
      "Epoch 411/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1127 - val_loss: 0.2229\n",
      "Epoch 412/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1134 - val_loss: 0.2222\n",
      "Epoch 413/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1120 - val_loss: 0.2226\n",
      "Epoch 414/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1165 - val_loss: 0.2224\n",
      "Epoch 415/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1126 - val_loss: 0.2223\n",
      "Epoch 416/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1123 - val_loss: 0.2217\n",
      "Epoch 417/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1117 - val_loss: 0.2220\n",
      "Epoch 418/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1135 - val_loss: 0.2219\n",
      "Epoch 419/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1120 - val_loss: 0.2221\n",
      "Epoch 420/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1136 - val_loss: 0.2222\n",
      "Epoch 421/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1147 - val_loss: 0.2219\n",
      "Epoch 422/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1149 - val_loss: 0.2223\n",
      "Epoch 423/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1122 - val_loss: 0.2216\n",
      "Epoch 424/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1125 - val_loss: 0.2225\n",
      "Epoch 425/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1122 - val_loss: 0.2223\n",
      "Epoch 426/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1135 - val_loss: 0.2223\n",
      "Epoch 427/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1137 - val_loss: 0.2219\n",
      "Epoch 428/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1145 - val_loss: 0.2219\n",
      "Epoch 429/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1140 - val_loss: 0.2225\n",
      "Epoch 430/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1132 - val_loss: 0.2217\n",
      "Epoch 431/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1119 - val_loss: 0.2219\n",
      "Epoch 432/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1126 - val_loss: 0.2221\n",
      "Epoch 433/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1116 - val_loss: 0.2218\n",
      "Epoch 434/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1130 - val_loss: 0.2223\n",
      "Epoch 435/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1132 - val_loss: 0.2220\n",
      "Epoch 436/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1107 - val_loss: 0.2221\n",
      "Epoch 437/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1098 - val_loss: 0.2219\n",
      "Epoch 438/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1125 - val_loss: 0.2240\n",
      "Epoch 439/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1095 - val_loss: 0.2221\n",
      "Epoch 440/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1113 - val_loss: 0.2248\n",
      "Epoch 441/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1128 - val_loss: 0.2216\n",
      "Epoch 442/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1126 - val_loss: 0.2220\n",
      "Epoch 443/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1126 - val_loss: 0.2226\n",
      "Epoch 444/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1114 - val_loss: 0.2222\n",
      "Epoch 445/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1121 - val_loss: 0.2216\n",
      "Epoch 446/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1131 - val_loss: 0.2218\n",
      "\n",
      "Epoch 00446: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 447/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1101 - val_loss: 0.2209\n",
      "Epoch 448/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1092 - val_loss: 0.2205\n",
      "Epoch 449/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1077 - val_loss: 0.2206\n",
      "Epoch 450/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1097 - val_loss: 0.2207\n",
      "Epoch 451/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1119 - val_loss: 0.2205\n",
      "Epoch 452/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1100 - val_loss: 0.2205\n",
      "Epoch 453/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1085 - val_loss: 0.2206\n",
      "Epoch 454/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1092 - val_loss: 0.2203\n",
      "Epoch 455/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1084 - val_loss: 0.2204\n",
      "Epoch 456/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1096 - val_loss: 0.2204\n",
      "Epoch 457/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1092 - val_loss: 0.2204\n",
      "Epoch 458/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1102 - val_loss: 0.2205\n",
      "Epoch 459/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1101 - val_loss: 0.2205\n",
      "Epoch 460/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1105 - val_loss: 0.2203\n",
      "Epoch 461/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1094 - val_loss: 0.2205\n",
      "Epoch 462/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1093 - val_loss: 0.2204\n",
      "Epoch 463/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1071 - val_loss: 0.2203\n",
      "Epoch 464/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1082 - val_loss: 0.2203\n",
      "Epoch 465/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1101 - val_loss: 0.2205\n",
      "Epoch 466/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1074 - val_loss: 0.2203\n",
      "Epoch 467/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1108 - val_loss: 0.2203\n",
      "Epoch 468/1000\n",
      "665/665 [==============================] - 12s 17ms/step - loss: 0.1119 - val_loss: 0.2204\n",
      "Epoch 469/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1065 - val_loss: 0.2203\n",
      "Epoch 470/1000\n",
      "665/665 [==============================] - 12s 17ms/step - loss: 0.1080 - val_loss: 0.2203\n",
      "Epoch 471/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1101 - val_loss: 0.2201\n",
      "Epoch 472/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1103 - val_loss: 0.2203\n",
      "Epoch 473/1000\n",
      "665/665 [==============================] - 12s 18ms/step - loss: 0.1104 - val_loss: 0.2205\n",
      "Epoch 474/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1079 - val_loss: 0.2204\n",
      "Epoch 475/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1084 - val_loss: 0.2203\n",
      "Epoch 476/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1099 - val_loss: 0.2204\n",
      "Epoch 477/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1078 - val_loss: 0.2204\n",
      "Epoch 478/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1096 - val_loss: 0.2203\n",
      "Epoch 479/1000\n",
      "665/665 [==============================] - 12s 17ms/step - loss: 0.1093 - val_loss: 0.2202\n",
      "Epoch 480/1000\n",
      "665/665 [==============================] - 11s 17ms/step - loss: 0.1088 - val_loss: 0.2203\n",
      "Epoch 481/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1087 - val_loss: 0.2203\n",
      "Epoch 482/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1090 - val_loss: 0.2202\n",
      "Epoch 483/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1096 - val_loss: 0.2203\n",
      "Epoch 484/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1090 - val_loss: 0.2203\n",
      "Epoch 485/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1104 - val_loss: 0.2202\n",
      "Epoch 486/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1063 - val_loss: 0.2203\n",
      "Epoch 487/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1118 - val_loss: 0.2202\n",
      "Epoch 488/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1082 - val_loss: 0.2202\n",
      "Epoch 489/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1105 - val_loss: 0.2201\n",
      "Epoch 490/1000\n",
      "665/665 [==============================] - 10s 16ms/step - loss: 0.1108 - val_loss: 0.2201\n",
      "Epoch 491/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1091 - val_loss: 0.2203\n",
      "Epoch 492/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1101 - val_loss: 0.2204\n",
      "Epoch 493/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1081 - val_loss: 0.2203\n",
      "Epoch 494/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1124 - val_loss: 0.2202\n",
      "Epoch 495/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1085 - val_loss: 0.2201\n",
      "Epoch 496/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1091 - val_loss: 0.2202\n",
      "Epoch 497/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1079 - val_loss: 0.2203\n",
      "Epoch 498/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1086 - val_loss: 0.2202\n",
      "Epoch 499/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1091 - val_loss: 0.2201\n",
      "Epoch 500/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1089 - val_loss: 0.2204\n",
      "Epoch 501/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1077 - val_loss: 0.2202\n",
      "\n",
      "Epoch 00501: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 502/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1100 - val_loss: 0.2205\n",
      "Epoch 503/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1088 - val_loss: 0.2202\n",
      "Epoch 504/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1094 - val_loss: 0.2202\n",
      "Epoch 505/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1099 - val_loss: 0.2202\n",
      "Epoch 506/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1070 - val_loss: 0.2202\n",
      "Epoch 507/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1093 - val_loss: 0.2201\n",
      "Epoch 508/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1079 - val_loss: 0.2203\n",
      "Epoch 509/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1075 - val_loss: 0.2203\n",
      "Epoch 510/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1084 - val_loss: 0.2201\n",
      "Epoch 511/1000\n",
      "665/665 [==============================] - 11s 16ms/step - loss: 0.1089 - val_loss: 0.2200\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00511: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.5135583\n",
      "Training 3JHN out of ['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN']\n",
      "Categories (8, object): ['1JHC', '2JHH', '1JHN', '2JHN', '2JHC', '3JHH', '3JHC', '3JHN'] \n",
      "\n",
      "Epoch 1/1000\n",
      "74/74 [==============================] - 3s 21ms/step - loss: 1.0870 - val_loss: 0.8171\n",
      "Epoch 2/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.3642 - val_loss: 0.7970\n",
      "Epoch 3/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.3006 - val_loss: 0.7104\n",
      "Epoch 4/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.2721 - val_loss: 0.6052\n",
      "Epoch 5/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.2575 - val_loss: 0.4515\n",
      "Epoch 6/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.2291 - val_loss: 0.3454\n",
      "Epoch 7/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.2210 - val_loss: 0.2457\n",
      "Epoch 8/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.2061 - val_loss: 0.2475\n",
      "Epoch 9/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.2057 - val_loss: 0.2271\n",
      "Epoch 10/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1952 - val_loss: 0.2392\n",
      "Epoch 11/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1881 - val_loss: 0.2118\n",
      "Epoch 12/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1846 - val_loss: 0.2064\n",
      "Epoch 13/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1824 - val_loss: 0.1911\n",
      "Epoch 14/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1722 - val_loss: 0.2024\n",
      "Epoch 15/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1744 - val_loss: 0.1945\n",
      "Epoch 16/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1700 - val_loss: 0.2124\n",
      "Epoch 17/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.1660 - val_loss: 0.1945\n",
      "Epoch 18/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.1605 - val_loss: 0.2002\n",
      "Epoch 19/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1600 - val_loss: 0.1782\n",
      "Epoch 20/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1515 - val_loss: 0.1748\n",
      "Epoch 21/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1517 - val_loss: 0.1834\n",
      "Epoch 22/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1555 - val_loss: 0.1818\n",
      "Epoch 23/1000\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 0.1501 - val_loss: 0.2083\n",
      "Epoch 24/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.1552 - val_loss: 0.2193\n",
      "Epoch 25/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1564 - val_loss: 0.1824\n",
      "Epoch 26/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.1468 - val_loss: 0.1551\n",
      "Epoch 27/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1421 - val_loss: 0.1761\n",
      "Epoch 28/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1414 - val_loss: 0.1728\n",
      "Epoch 29/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1404 - val_loss: 0.1656\n",
      "Epoch 30/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1397 - val_loss: 0.1764\n",
      "Epoch 31/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1425 - val_loss: 0.1588\n",
      "Epoch 32/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1341 - val_loss: 0.1563\n",
      "Epoch 33/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1291 - val_loss: 0.1575\n",
      "Epoch 34/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1309 - val_loss: 0.1555\n",
      "Epoch 35/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.1285 - val_loss: 0.1477\n",
      "Epoch 36/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1307 - val_loss: 0.1570\n",
      "Epoch 37/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1257 - val_loss: 0.1469\n",
      "Epoch 38/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1300 - val_loss: 0.1624\n",
      "Epoch 39/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1257 - val_loss: 0.1717\n",
      "Epoch 40/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1313 - val_loss: 0.1661\n",
      "Epoch 41/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1241 - val_loss: 0.1525\n",
      "Epoch 42/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1225 - val_loss: 0.1652\n",
      "Epoch 43/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1262 - val_loss: 0.1463\n",
      "Epoch 44/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.1200 - val_loss: 0.1494\n",
      "Epoch 45/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1172 - val_loss: 0.1547\n",
      "Epoch 46/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1212 - val_loss: 0.1624\n",
      "Epoch 47/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1223 - val_loss: 0.1495\n",
      "Epoch 48/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.1218 - val_loss: 0.1557\n",
      "Epoch 49/1000\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 0.1185 - val_loss: 0.1455\n",
      "Epoch 50/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1172 - val_loss: 0.1497\n",
      "Epoch 51/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1179 - val_loss: 0.1652\n",
      "Epoch 52/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.1157 - val_loss: 0.1434\n",
      "Epoch 53/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.1151 - val_loss: 0.1437\n",
      "Epoch 54/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.1111 - val_loss: 0.1389\n",
      "Epoch 55/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1113 - val_loss: 0.1480\n",
      "Epoch 56/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1171 - val_loss: 0.1357\n",
      "Epoch 57/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1099 - val_loss: 0.1508\n",
      "Epoch 58/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1117 - val_loss: 0.1378\n",
      "Epoch 59/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.1088 - val_loss: 0.1420\n",
      "Epoch 60/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1075 - val_loss: 0.1364\n",
      "Epoch 61/1000\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 0.1055 - val_loss: 0.1442\n",
      "Epoch 62/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1088 - val_loss: 0.1420\n",
      "Epoch 63/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1118 - val_loss: 0.1352\n",
      "Epoch 64/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.1062 - val_loss: 0.1335\n",
      "Epoch 65/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1018 - val_loss: 0.1481\n",
      "Epoch 66/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1083 - val_loss: 0.1355\n",
      "Epoch 67/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1035 - val_loss: 0.1407\n",
      "Epoch 68/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1061 - val_loss: 0.1384\n",
      "Epoch 69/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.1041 - val_loss: 0.1347\n",
      "Epoch 70/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.1012 - val_loss: 0.1342\n",
      "Epoch 71/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1022 - val_loss: 0.1449\n",
      "Epoch 72/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1050 - val_loss: 0.1288\n",
      "Epoch 73/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1042 - val_loss: 0.1406\n",
      "Epoch 74/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.1042 - val_loss: 0.1451\n",
      "Epoch 75/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.1036 - val_loss: 0.1319\n",
      "Epoch 76/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0979 - val_loss: 0.1244\n",
      "Epoch 77/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0967 - val_loss: 0.1234\n",
      "Epoch 78/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0987 - val_loss: 0.1320\n",
      "Epoch 79/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0977 - val_loss: 0.1331\n",
      "Epoch 80/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1017 - val_loss: 0.1336\n",
      "Epoch 81/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0967 - val_loss: 0.1321\n",
      "Epoch 82/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0981 - val_loss: 0.1528\n",
      "Epoch 83/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.1002 - val_loss: 0.1405\n",
      "Epoch 84/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0983 - val_loss: 0.1361\n",
      "Epoch 85/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0991 - val_loss: 0.1437\n",
      "Epoch 86/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0973 - val_loss: 0.1280\n",
      "Epoch 87/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0962 - val_loss: 0.1312\n",
      "Epoch 88/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0947 - val_loss: 0.1300\n",
      "Epoch 89/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0938 - val_loss: 0.1324\n",
      "Epoch 90/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0938 - val_loss: 0.1259\n",
      "Epoch 91/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0954 - val_loss: 0.1260\n",
      "Epoch 92/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0947 - val_loss: 0.1248\n",
      "Epoch 93/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0911 - val_loss: 0.1250\n",
      "Epoch 94/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0912 - val_loss: 0.1241\n",
      "Epoch 95/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0900 - val_loss: 0.1280\n",
      "Epoch 96/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0977 - val_loss: 0.1236\n",
      "Epoch 97/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0949 - val_loss: 0.1315\n",
      "Epoch 98/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0897 - val_loss: 0.1239\n",
      "Epoch 99/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0911 - val_loss: 0.1368\n",
      "Epoch 100/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0940 - val_loss: 0.1193\n",
      "Epoch 101/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0884 - val_loss: 0.1227\n",
      "Epoch 102/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0880 - val_loss: 0.1254\n",
      "Epoch 103/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0872 - val_loss: 0.1195\n",
      "Epoch 104/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0905 - val_loss: 0.1289\n",
      "Epoch 105/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0945 - val_loss: 0.1224\n",
      "Epoch 106/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0898 - val_loss: 0.1257\n",
      "Epoch 107/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0935 - val_loss: 0.1263\n",
      "Epoch 108/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0868 - val_loss: 0.1233\n",
      "Epoch 109/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0896 - val_loss: 0.1185\n",
      "Epoch 110/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0900 - val_loss: 0.1230\n",
      "Epoch 111/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0845 - val_loss: 0.1255\n",
      "Epoch 112/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0856 - val_loss: 0.1284\n",
      "Epoch 113/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0865 - val_loss: 0.1203\n",
      "Epoch 114/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0844 - val_loss: 0.1218\n",
      "Epoch 115/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0850 - val_loss: 0.1176\n",
      "Epoch 116/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0828 - val_loss: 0.1337\n",
      "Epoch 117/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0876 - val_loss: 0.1262\n",
      "Epoch 118/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0837 - val_loss: 0.1250\n",
      "Epoch 119/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0865 - val_loss: 0.1242\n",
      "Epoch 120/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0868 - val_loss: 0.1190\n",
      "Epoch 121/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0834 - val_loss: 0.1152\n",
      "Epoch 122/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0821 - val_loss: 0.1235\n",
      "Epoch 123/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0856 - val_loss: 0.1212\n",
      "Epoch 124/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0857 - val_loss: 0.1175\n",
      "Epoch 125/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0828 - val_loss: 0.1160\n",
      "Epoch 126/1000\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 0.0849 - val_loss: 0.1173\n",
      "Epoch 127/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0820 - val_loss: 0.1170\n",
      "Epoch 128/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0790 - val_loss: 0.1226\n",
      "Epoch 129/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0886 - val_loss: 0.1205\n",
      "Epoch 130/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0855 - val_loss: 0.1186\n",
      "Epoch 131/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0856 - val_loss: 0.1265\n",
      "Epoch 132/1000\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 0.0826 - val_loss: 0.1177\n",
      "Epoch 133/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0794 - val_loss: 0.1157\n",
      "Epoch 134/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0806 - val_loss: 0.1191\n",
      "Epoch 135/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0811 - val_loss: 0.1250\n",
      "Epoch 136/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0803 - val_loss: 0.1232\n",
      "Epoch 137/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0832 - val_loss: 0.1173\n",
      "Epoch 138/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0789 - val_loss: 0.1181\n",
      "Epoch 139/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0764 - val_loss: 0.1183\n",
      "Epoch 140/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0795 - val_loss: 0.1146\n",
      "Epoch 141/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0760 - val_loss: 0.1150\n",
      "Epoch 142/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0788 - val_loss: 0.1173\n",
      "Epoch 143/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0768 - val_loss: 0.1208\n",
      "Epoch 144/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0793 - val_loss: 0.1202\n",
      "Epoch 145/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0770 - val_loss: 0.1147\n",
      "Epoch 146/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0784 - val_loss: 0.1156\n",
      "Epoch 147/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0777 - val_loss: 0.1177\n",
      "Epoch 148/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0805 - val_loss: 0.1272\n",
      "Epoch 149/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0802 - val_loss: 0.1153\n",
      "Epoch 150/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0764 - val_loss: 0.1142\n",
      "Epoch 151/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0784 - val_loss: 0.1164\n",
      "Epoch 152/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0801 - val_loss: 0.1174\n",
      "Epoch 153/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0828 - val_loss: 0.1159\n",
      "Epoch 154/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0749 - val_loss: 0.1135\n",
      "Epoch 155/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0768 - val_loss: 0.1135\n",
      "Epoch 156/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0755 - val_loss: 0.1117\n",
      "Epoch 157/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0759 - val_loss: 0.1138\n",
      "Epoch 158/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0722 - val_loss: 0.1136\n",
      "Epoch 159/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0822 - val_loss: 0.1167\n",
      "Epoch 160/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0749 - val_loss: 0.1127\n",
      "Epoch 161/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0726 - val_loss: 0.1118\n",
      "Epoch 162/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0746 - val_loss: 0.1198\n",
      "Epoch 163/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0734 - val_loss: 0.1142\n",
      "Epoch 164/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0724 - val_loss: 0.1132\n",
      "Epoch 165/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0751 - val_loss: 0.1140\n",
      "Epoch 166/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0741 - val_loss: 0.1103\n",
      "Epoch 167/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0726 - val_loss: 0.1201\n",
      "Epoch 168/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0738 - val_loss: 0.1089\n",
      "Epoch 169/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0747 - val_loss: 0.1116\n",
      "Epoch 170/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0710 - val_loss: 0.1121\n",
      "Epoch 171/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0727 - val_loss: 0.1128\n",
      "Epoch 172/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0719 - val_loss: 0.1168\n",
      "Epoch 173/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0740 - val_loss: 0.1113\n",
      "Epoch 174/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0698 - val_loss: 0.1161\n",
      "Epoch 175/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0722 - val_loss: 0.1103\n",
      "Epoch 176/1000\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 0.0726 - val_loss: 0.1127\n",
      "Epoch 177/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0701 - val_loss: 0.1194\n",
      "Epoch 178/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0837 - val_loss: 0.1119\n",
      "Epoch 179/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0711 - val_loss: 0.1088\n",
      "Epoch 180/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0734 - val_loss: 0.1108\n",
      "Epoch 181/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0695 - val_loss: 0.1095\n",
      "Epoch 182/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0702 - val_loss: 0.1123\n",
      "Epoch 183/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0731 - val_loss: 0.1132\n",
      "Epoch 184/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0691 - val_loss: 0.1126\n",
      "Epoch 185/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0747 - val_loss: 0.1108\n",
      "Epoch 186/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0726 - val_loss: 0.1143\n",
      "Epoch 187/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0720 - val_loss: 0.1091\n",
      "Epoch 188/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0695 - val_loss: 0.1121\n",
      "Epoch 189/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0732 - val_loss: 0.1100\n",
      "Epoch 190/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0699 - val_loss: 0.1215\n",
      "Epoch 191/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0711 - val_loss: 0.1131\n",
      "Epoch 192/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0726 - val_loss: 0.1112\n",
      "Epoch 193/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0709 - val_loss: 0.1101\n",
      "Epoch 194/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0706 - val_loss: 0.1072\n",
      "Epoch 195/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0693 - val_loss: 0.1102\n",
      "Epoch 196/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0697 - val_loss: 0.1101\n",
      "Epoch 197/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0692 - val_loss: 0.1079\n",
      "Epoch 198/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0692 - val_loss: 0.1093\n",
      "Epoch 199/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0680 - val_loss: 0.1057\n",
      "Epoch 200/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0656 - val_loss: 0.1068\n",
      "Epoch 201/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0689 - val_loss: 0.1125\n",
      "Epoch 202/1000\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 0.0697 - val_loss: 0.1154\n",
      "Epoch 203/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0691 - val_loss: 0.1144\n",
      "Epoch 204/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0669 - val_loss: 0.1124\n",
      "Epoch 205/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0658 - val_loss: 0.1089\n",
      "Epoch 206/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0654 - val_loss: 0.1090\n",
      "Epoch 207/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0700 - val_loss: 0.1119\n",
      "Epoch 208/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0697 - val_loss: 0.1085\n",
      "Epoch 209/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0686 - val_loss: 0.1105\n",
      "Epoch 210/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0665 - val_loss: 0.1053\n",
      "Epoch 211/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0682 - val_loss: 0.1064\n",
      "Epoch 212/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0656 - val_loss: 0.1056\n",
      "Epoch 213/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0693 - val_loss: 0.1114\n",
      "Epoch 214/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0669 - val_loss: 0.1121\n",
      "Epoch 215/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0672 - val_loss: 0.1088\n",
      "Epoch 216/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0733 - val_loss: 0.1077\n",
      "Epoch 217/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0655 - val_loss: 0.1062\n",
      "Epoch 218/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0620 - val_loss: 0.1142\n",
      "Epoch 219/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0674 - val_loss: 0.1091\n",
      "Epoch 220/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0649 - val_loss: 0.1049\n",
      "Epoch 221/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0637 - val_loss: 0.1072\n",
      "Epoch 222/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0659 - val_loss: 0.1046\n",
      "Epoch 223/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0627 - val_loss: 0.1025\n",
      "Epoch 224/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0623 - val_loss: 0.1064\n",
      "Epoch 225/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0698 - val_loss: 0.1101\n",
      "Epoch 226/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0644 - val_loss: 0.1087\n",
      "Epoch 227/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0688 - val_loss: 0.1062\n",
      "Epoch 228/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0660 - val_loss: 0.1061\n",
      "Epoch 229/1000\n",
      "74/74 [==============================] - 2s 22ms/step - loss: 0.0671 - val_loss: 0.1053\n",
      "Epoch 230/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0648 - val_loss: 0.1069\n",
      "Epoch 231/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0646 - val_loss: 0.1044\n",
      "Epoch 232/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0641 - val_loss: 0.1062\n",
      "Epoch 233/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0642 - val_loss: 0.1075\n",
      "Epoch 234/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0674 - val_loss: 0.1105\n",
      "Epoch 235/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0631 - val_loss: 0.1066\n",
      "Epoch 236/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0645 - val_loss: 0.1098\n",
      "Epoch 237/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0677 - val_loss: 0.1097\n",
      "Epoch 238/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0658 - val_loss: 0.1095\n",
      "Epoch 239/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0678 - val_loss: 0.1066\n",
      "Epoch 240/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0663 - val_loss: 0.1104\n",
      "Epoch 241/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0623 - val_loss: 0.1061\n",
      "Epoch 242/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0638 - val_loss: 0.1074\n",
      "Epoch 243/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0637 - val_loss: 0.1044\n",
      "Epoch 244/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0646 - val_loss: 0.1053\n",
      "Epoch 245/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0633 - val_loss: 0.1034\n",
      "Epoch 246/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0614 - val_loss: 0.1060\n",
      "Epoch 247/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0666 - val_loss: 0.1038\n",
      "Epoch 248/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0627 - val_loss: 0.1031\n",
      "Epoch 249/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0607 - val_loss: 0.1111\n",
      "Epoch 250/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0646 - val_loss: 0.1045\n",
      "Epoch 251/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0601 - val_loss: 0.1046\n",
      "Epoch 252/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0630 - val_loss: 0.1072\n",
      "Epoch 253/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0642 - val_loss: 0.1032\n",
      "\n",
      "Epoch 00253: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 254/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0536 - val_loss: 0.0936\n",
      "Epoch 255/1000\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 0.0472 - val_loss: 0.0926\n",
      "Epoch 256/1000\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 0.0453 - val_loss: 0.0924\n",
      "Epoch 257/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0424 - val_loss: 0.0918\n",
      "Epoch 258/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0432 - val_loss: 0.0916\n",
      "Epoch 259/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0459 - val_loss: 0.0914\n",
      "Epoch 260/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0427 - val_loss: 0.0920\n",
      "Epoch 261/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0443 - val_loss: 0.0915\n",
      "Epoch 262/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0428 - val_loss: 0.0918\n",
      "Epoch 263/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0421 - val_loss: 0.0909\n",
      "Epoch 264/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0432 - val_loss: 0.0915\n",
      "Epoch 265/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0437 - val_loss: 0.0910\n",
      "Epoch 266/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0396 - val_loss: 0.0907\n",
      "Epoch 267/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0423 - val_loss: 0.0911\n",
      "Epoch 268/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0432 - val_loss: 0.0907\n",
      "Epoch 269/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0420 - val_loss: 0.0907\n",
      "Epoch 270/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0405 - val_loss: 0.0911\n",
      "Epoch 271/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0445 - val_loss: 0.0912\n",
      "Epoch 272/1000\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 0.0440 - val_loss: 0.0910\n",
      "Epoch 273/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0408 - val_loss: 0.0909\n",
      "Epoch 274/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0393 - val_loss: 0.0906\n",
      "Epoch 275/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0394 - val_loss: 0.0911\n",
      "Epoch 276/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0410 - val_loss: 0.0906\n",
      "Epoch 277/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0422 - val_loss: 0.0911\n",
      "Epoch 278/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0402 - val_loss: 0.0907\n",
      "Epoch 279/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0410 - val_loss: 0.0911\n",
      "Epoch 280/1000\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 0.0422 - val_loss: 0.0914\n",
      "Epoch 281/1000\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 0.0414 - val_loss: 0.0905\n",
      "Epoch 282/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0391 - val_loss: 0.0919\n",
      "Epoch 283/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0384 - val_loss: 0.0910\n",
      "Epoch 284/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0404 - val_loss: 0.0906\n",
      "Epoch 285/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0400 - val_loss: 0.0907\n",
      "Epoch 286/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0409 - val_loss: 0.0908\n",
      "Epoch 287/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0410 - val_loss: 0.0907\n",
      "Epoch 288/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0395 - val_loss: 0.0910\n",
      "Epoch 289/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0367 - val_loss: 0.0910\n",
      "Epoch 290/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0375 - val_loss: 0.0912\n",
      "Epoch 291/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0404 - val_loss: 0.0901\n",
      "Epoch 292/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0381 - val_loss: 0.0909\n",
      "Epoch 293/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0397 - val_loss: 0.0902\n",
      "Epoch 294/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0406 - val_loss: 0.0906\n",
      "Epoch 295/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0373 - val_loss: 0.0903\n",
      "Epoch 296/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0404 - val_loss: 0.0906\n",
      "Epoch 297/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0397 - val_loss: 0.0907\n",
      "Epoch 298/1000\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 0.0404 - val_loss: 0.0903\n",
      "Epoch 299/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0392 - val_loss: 0.0898\n",
      "Epoch 300/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0375 - val_loss: 0.0903\n",
      "Epoch 301/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0390 - val_loss: 0.0906\n",
      "Epoch 302/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0429 - val_loss: 0.0907\n",
      "Epoch 303/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0379 - val_loss: 0.0897\n",
      "Epoch 304/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0410 - val_loss: 0.0899\n",
      "Epoch 305/1000\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 0.0396 - val_loss: 0.0903\n",
      "Epoch 306/1000\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 0.0448 - val_loss: 0.0901\n",
      "Epoch 307/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0401 - val_loss: 0.0907\n",
      "Epoch 308/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0393 - val_loss: 0.0912\n",
      "Epoch 309/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0396 - val_loss: 0.0906\n",
      "Epoch 310/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0390 - val_loss: 0.0901\n",
      "Epoch 311/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0416 - val_loss: 0.0900\n",
      "Epoch 312/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0364 - val_loss: 0.0906\n",
      "Epoch 313/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0382 - val_loss: 0.0905\n",
      "Epoch 314/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0448 - val_loss: 0.0902\n",
      "Epoch 315/1000\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 0.0421 - val_loss: 0.0910\n",
      "Epoch 316/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0417 - val_loss: 0.0901\n",
      "Epoch 317/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0387 - val_loss: 0.0903\n",
      "Epoch 318/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0380 - val_loss: 0.0904\n",
      "Epoch 319/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0401 - val_loss: 0.0902\n",
      "Epoch 320/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0393 - val_loss: 0.0912\n",
      "Epoch 321/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0382 - val_loss: 0.0902\n",
      "Epoch 322/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0408 - val_loss: 0.0918\n",
      "Epoch 323/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0413 - val_loss: 0.0908\n",
      "Epoch 324/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0408 - val_loss: 0.0897\n",
      "Epoch 325/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0399 - val_loss: 0.0901\n",
      "Epoch 326/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0389 - val_loss: 0.0907\n",
      "Epoch 327/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0386 - val_loss: 0.0901\n",
      "Epoch 328/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0406 - val_loss: 0.0903\n",
      "Epoch 329/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0402 - val_loss: 0.0916\n",
      "\n",
      "Epoch 00329: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 330/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0380 - val_loss: 0.0894\n",
      "Epoch 331/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0399 - val_loss: 0.0892\n",
      "Epoch 332/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0380 - val_loss: 0.0889\n",
      "Epoch 333/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0381 - val_loss: 0.0889\n",
      "Epoch 334/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0370 - val_loss: 0.0889\n",
      "Epoch 335/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0375 - val_loss: 0.0889\n",
      "Epoch 336/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0344 - val_loss: 0.0888\n",
      "Epoch 337/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0359 - val_loss: 0.0888\n",
      "Epoch 338/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0350 - val_loss: 0.0888\n",
      "Epoch 339/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0379 - val_loss: 0.0887\n",
      "Epoch 340/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0393 - val_loss: 0.0888\n",
      "Epoch 341/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0392 - val_loss: 0.0887\n",
      "Epoch 342/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0373 - val_loss: 0.0887\n",
      "Epoch 343/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0357 - val_loss: 0.0887\n",
      "Epoch 344/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0382 - val_loss: 0.0888\n",
      "Epoch 345/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0340 - val_loss: 0.0887\n",
      "Epoch 346/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0353 - val_loss: 0.0887\n",
      "Epoch 347/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0353 - val_loss: 0.0887\n",
      "Epoch 348/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0361 - val_loss: 0.0888\n",
      "Epoch 349/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0360 - val_loss: 0.0887\n",
      "Epoch 350/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0356 - val_loss: 0.0886\n",
      "Epoch 351/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0364 - val_loss: 0.0886\n",
      "Epoch 352/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0371 - val_loss: 0.0887\n",
      "Epoch 353/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0385 - val_loss: 0.0887\n",
      "Epoch 354/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0365 - val_loss: 0.0889\n",
      "Epoch 355/1000\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 0.0372 - val_loss: 0.0886\n",
      "Epoch 356/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0367 - val_loss: 0.0886\n",
      "Epoch 357/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0346 - val_loss: 0.0886\n",
      "Epoch 358/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0365 - val_loss: 0.0886\n",
      "Epoch 359/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0374 - val_loss: 0.0886\n",
      "Epoch 360/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0371 - val_loss: 0.0886\n",
      "Epoch 361/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0344 - val_loss: 0.0886\n",
      "Epoch 362/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0356 - val_loss: 0.0886\n",
      "Epoch 363/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0363 - val_loss: 0.0886\n",
      "Epoch 364/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0390 - val_loss: 0.0886\n",
      "Epoch 365/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0364 - val_loss: 0.0886\n",
      "Epoch 366/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0362 - val_loss: 0.0886\n",
      "Epoch 367/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0385 - val_loss: 0.0886\n",
      "Epoch 368/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0376 - val_loss: 0.0886\n",
      "Epoch 369/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0339 - val_loss: 0.0886\n",
      "Epoch 370/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0389 - val_loss: 0.0885\n",
      "Epoch 371/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0366 - val_loss: 0.0886\n",
      "Epoch 372/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0378 - val_loss: 0.0886\n",
      "Epoch 373/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0367 - val_loss: 0.0886\n",
      "Epoch 374/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0350 - val_loss: 0.0887\n",
      "Epoch 375/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0352 - val_loss: 0.0888\n",
      "Epoch 376/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0359 - val_loss: 0.0886\n",
      "Epoch 377/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0364 - val_loss: 0.0885\n",
      "Epoch 378/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0357 - val_loss: 0.0886\n",
      "Epoch 379/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0340 - val_loss: 0.0886\n",
      "Epoch 380/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0409 - val_loss: 0.0885\n",
      "Epoch 381/1000\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 0.0395 - val_loss: 0.0885\n",
      "Epoch 382/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0356 - val_loss: 0.0885\n",
      "Epoch 383/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0367 - val_loss: 0.0887\n",
      "Epoch 384/1000\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 0.0359 - val_loss: 0.0886\n",
      "Epoch 385/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0368 - val_loss: 0.0885\n",
      "Epoch 386/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0359 - val_loss: 0.0885\n",
      "Epoch 387/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0353 - val_loss: 0.0885\n",
      "\n",
      "Epoch 00387: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 388/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0366 - val_loss: 0.0885\n",
      "Epoch 389/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0366 - val_loss: 0.0885\n",
      "Epoch 390/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0362 - val_loss: 0.0885\n",
      "Epoch 391/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0398 - val_loss: 0.0885\n",
      "Epoch 392/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0360 - val_loss: 0.0885\n",
      "Epoch 393/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0385 - val_loss: 0.0885\n",
      "Epoch 394/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0390 - val_loss: 0.0885\n",
      "Epoch 395/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0392 - val_loss: 0.0885\n",
      "Epoch 396/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0348 - val_loss: 0.0885\n",
      "Epoch 397/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0399 - val_loss: 0.0885\n",
      "Epoch 398/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0371 - val_loss: 0.0885\n",
      "Epoch 399/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0347 - val_loss: 0.0885\n",
      "Epoch 400/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0386 - val_loss: 0.0885\n",
      "Epoch 401/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0363 - val_loss: 0.0885\n",
      "Epoch 402/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0379 - val_loss: 0.0884\n",
      "Epoch 403/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0377 - val_loss: 0.0884\n",
      "Epoch 404/1000\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 0.0353 - val_loss: 0.0884\n",
      "Epoch 405/1000\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 0.0381 - val_loss: 0.0885\n",
      "Epoch 406/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0360 - val_loss: 0.0885\n",
      "Epoch 407/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0350 - val_loss: 0.0885\n",
      "Epoch 408/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0359 - val_loss: 0.0885\n",
      "Epoch 409/1000\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 0.0387 - val_loss: 0.0885\n",
      "Epoch 410/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0381 - val_loss: 0.0885\n",
      "Epoch 411/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0365 - val_loss: 0.0885\n",
      "Epoch 412/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0355 - val_loss: 0.0885\n",
      "Epoch 413/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0358 - val_loss: 0.0885\n",
      "Epoch 414/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0378 - val_loss: 0.0885\n",
      "Epoch 415/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0355 - val_loss: 0.0885\n",
      "Epoch 416/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0366 - val_loss: 0.0885\n",
      "Epoch 417/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0401 - val_loss: 0.0885\n",
      "Epoch 418/1000\n",
      "74/74 [==============================] - 1s 18ms/step - loss: 0.0373 - val_loss: 0.0884\n",
      "Epoch 419/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0381 - val_loss: 0.0884\n",
      "\n",
      "Epoch 00419: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "Epoch 420/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0379 - val_loss: 0.0884\n",
      "Epoch 421/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0373 - val_loss: 0.0884\n",
      "Epoch 422/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0404 - val_loss: 0.0884\n",
      "Epoch 423/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0338 - val_loss: 0.0885\n",
      "Epoch 424/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0362 - val_loss: 0.0885\n",
      "Epoch 425/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0381 - val_loss: 0.0885\n",
      "Epoch 426/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0346 - val_loss: 0.0885\n",
      "Epoch 427/1000\n",
      "74/74 [==============================] - 1s 17ms/step - loss: 0.0403 - val_loss: 0.0885\n",
      "Epoch 428/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0347 - val_loss: 0.0885\n",
      "Epoch 429/1000\n",
      "74/74 [==============================] - 1s 16ms/step - loss: 0.0364 - val_loss: 0.0885\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00429: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.4249797\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "mol_types=train_csv[\"type\"].unique()\n",
    "cv_score=[]\n",
    "cv_score_total=0\n",
    "epoch_n = 1000\n",
    "verbose = 1\n",
    "batch_size = 2048\n",
    "    \n",
    "# Set to True if we want to train from scratch.  False will reuse saved models as a starting point.\n",
    "retrain =True\n",
    "\n",
    "start_time=datetime.now()\n",
    "test_prediction=np.zeros(len(test_csv))\n",
    "input_features = ['atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7',\n",
    "       'atom_8','atom_9', 'atom_10','d_1_0', 'd_2_0', 'd_2_1', 'd_3_0',\n",
    "       'd_3_1', 'd_3_2', 'd_4_0', 'd_4_1', 'd_4_2', 'd_4_3', 'd_5_0',\n",
    "       'd_5_1', 'd_5_2', 'd_5_3', 'd_6_0', 'd_6_1', 'd_6_2', 'd_6_3',\n",
    "       'd_7_0', 'd_7_1', 'd_7_2', 'd_7_3', 'd_8_0', 'd_8_1', 'd_8_2',\n",
    "       'd_8_3', 'd_9_0', 'd_9_1', 'd_9_2', 'd_9_3', 'd_10_0', 'd_10_1', 'd_10_2',\n",
    "       'd_10_3']\n",
    "\n",
    "\n",
    "\n",
    "# Loop through each molecule type\n",
    "for mol_type in mol_types:\n",
    "\n",
    "    model_name_wrt = ('/kaggle/working/molecule_model_%s.hdf5' % mol_type)\n",
    "    print('Training %s' % mol_type, 'out of', mol_types, '\\n')\n",
    "\n",
    "    full = build_couple_dataframe(train_csv, structures_csv, mol_type, n_atoms=11)\n",
    "    full2 = build_couple_dataframe(test_csv, structures_csv, mol_type, n_atoms=11)\n",
    "    df_train_ = take_n_atoms(full, 11)\n",
    "    df_test_ = take_n_atoms(full2, 11)\n",
    "    df_train_  = df_train_.fillna(0)\n",
    "    df_test_  = df_test_.fillna(0)\n",
    "    \n",
    "    # Standard Scaler from sklearn does seem to work better here than other Scalers\n",
    "    input_data=StandardScaler().fit_transform(pd.concat([df_train_.loc[:,input_features],df_test_.loc[:,input_features]]))   \n",
    "    #input_data=StandardScaler().fit_transform(df_train_.loc[:,input_features])\n",
    "    target_data=df_train_.loc[:,\"scalar_coupling_constant\"].values\n",
    "\n",
    "    # Simple split to provide us a validation set to do our CV checks with\n",
    "    train_index, cv_index = train_test_split(np.arange(len(df_train_)),random_state=111, test_size=0.1)\n",
    "    # Split all our input and targets by train and cv indexes\n",
    "    train_target=target_data[train_index]\n",
    "    cv_target=target_data[cv_index]\n",
    "    train_input=input_data[train_index]\n",
    "    cv_input=input_data[cv_index]\n",
    "    test_input=input_data[len(df_train_):,:]\n",
    "\n",
    "    # Build the Neural Net\n",
    "    nn_model=create_nn_model(train_input.shape[1])\n",
    "    \n",
    "    # If retrain==False, then we load a previous saved model as a starting point.\n",
    "    if not retrain:\n",
    "        nn_model = load_model(model_name_rd)\n",
    "        \n",
    "    nn_model.compile(loss='mae', optimizer=Adam())#, metrics=[auc])\n",
    "    \n",
    "    # Callback for Early Stopping... May want to raise the min_delta for small numbers of epochs\n",
    "    es = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=40,verbose=1, mode='auto', restore_best_weights=True)\n",
    "    # Callback for Reducing the Learning Rate... when the monitor levels out for 'patience' epochs, then the LR is reduced\n",
    "    rlr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,patience=30, min_lr=1e-6, mode='auto', verbose=1)\n",
    "    # Save the best value of the model for future use\n",
    "    sv_mod = callbacks.ModelCheckpoint(model_name_wrt, monitor='val_loss', save_best_only=True, period=1)\n",
    "    history = nn_model.fit(train_input,[train_target], \n",
    "            validation_data=(cv_input,[cv_target]), \n",
    "            callbacks=[es, rlr, sv_mod], epochs=epoch_n, batch_size=batch_size, verbose=verbose)\n",
    "    \n",
    "    cv_predict=nn_model.predict(cv_input)\n",
    "    plot_history(history, mol_type)\n",
    "    accuracy=np.mean(np.abs(cv_target-cv_predict[:,0]))\n",
    "    print(np.log(accuracy))\n",
    "    cv_score.append(np.log(accuracy))\n",
    "    cv_score_total+=np.log(accuracy)\n",
    "    \n",
    "    # Predict on the test data set using our trained model\n",
    "    test_predict=nn_model.predict(test_input)\n",
    "    \n",
    "    # for each molecule type we'll grab the predicted values\n",
    "    test_prediction[test_csv[\"type\"]==mol_type]=test_predict[:,0]\n",
    "    tf.compat.v1.keras.backend.clear_session()\n",
    "\n",
    "cv_score_total/=len(mol_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-08T14:40:01.063389Z",
     "iopub.status.busy": "2021-03-08T14:40:01.059666Z",
     "iopub.status.idle": "2021-03-08T14:40:01.066561Z",
     "shell.execute_reply": "2021-03-08T14:40:01.065749Z"
    },
    "papermill": {
     "duration": 104.57608,
     "end_time": "2021-03-08T14:40:01.066726",
     "exception": false,
     "start_time": "2021-03-08T14:38:16.490646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time:  4:50:47.794398\n",
      "1JHC : cv score is  -0.47094947\n",
      "2JHH : cv score is  -2.1613662\n",
      "1JHN : cv score is  -1.1003686\n",
      "2JHN : cv score is  -2.1901865\n",
      "2JHC : cv score is  -1.607858\n",
      "3JHH : cv score is  -2.1478362\n",
      "3JHC : cv score is  -1.5135583\n",
      "3JHN : cv score is  -2.4249797\n",
      "total cv score is -1.702137865126133\n"
     ]
    }
   ],
   "source": [
    "print ('Total training time: ', datetime.now() - start_time)\n",
    "\n",
    "i=0\n",
    "for mol_type in mol_types: \n",
    "    print(mol_type,\": cv score is \",cv_score[i])\n",
    "    i+=1\n",
    "print(\"total cv score is\",cv_score_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-08T14:43:30.636424Z",
     "iopub.status.busy": "2021-03-08T14:43:30.635681Z",
     "iopub.status.idle": "2021-03-08T14:43:33.342814Z",
     "shell.execute_reply": "2021-03-08T14:43:33.344353Z"
    },
    "papermill": {
     "duration": 107.355354,
     "end_time": "2021-03-08T14:43:33.344639",
     "exception": false,
     "start_time": "2021-03-08T14:41:45.989285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4658147</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4658148</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4658149</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4658150</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4658151</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  scalar_coupling_constant\n",
       "0  4658147  1                       \n",
       "1  4658148  1                       \n",
       "2  4658149  1                       \n",
       "3  4658150  1                       \n",
       "4  4658151  1                       "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_csv = pd.read_csv(\"../input/test-data/test.csv\")\n",
    "submission_csv['scalar_coupling_constant'] = 1\n",
    "submission_csv = submission_csv.drop(['molecule_name', 'atom_index_0', 'atom_index_1', 'type'], axis=1)\n",
    "submission_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-08T14:47:03.231861Z",
     "iopub.status.busy": "2021-03-08T14:47:03.230248Z",
     "iopub.status.idle": "2021-03-08T14:47:14.241487Z",
     "shell.execute_reply": "2021-03-08T14:47:14.240067Z"
    },
    "papermill": {
     "duration": 116.031504,
     "end_time": "2021-03-08T14:47:14.241691",
     "exception": false,
     "start_time": "2021-03-08T14:45:18.210187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def submits(predictions):\n",
    " \n",
    "    submission_csv[\"scalar_coupling_constant\"] = predictions\n",
    "    submission_csv.to_csv(\"/kaggle/working/submission.csv\", index=False)\n",
    "submits(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 104.646088,
     "end_time": "2021-03-08T14:50:44.370161",
     "exception": false,
     "start_time": "2021-03-08T14:48:59.724073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18235.834477,
   "end_time": "2021-03-08T14:52:31.951805",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-08T09:48:36.117328",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
