{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\nimport time\nimport pickle\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation,Dropout, BatchNormalization\nfrom sklearn.model_selection import KFold\nimport xgboost\nfrom xgboost import plot_importance\nimport lightgbm as lgb\nfrom sklearn.model_selection import  train_test_split\nfrom sklearn.model_selection import GridSearchCV   #Performing grid search\nfrom sklearn.model_selection import validation_curve\nimport gc\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(os.listdir(\"../input\"))\n\n#Loading Data \nfile_train = \"../input/train_v2.csv\"\nfile_test = \"../input/test_v2.csv\"\nchunk_size = 10000\n\ndef load_data(file,chunk_size,nrows_load=None,test_data=False):\n    df_res = pd.DataFrame()\n    df_reader = pd.read_csv(file,\n                            dtype={ 'date': str, 'fullVisitorId': str},\n                            chunksize=10000)\n    \n    for cidx, df in enumerate(df_reader):\n        df.reset_index(drop=True, inplace=True)   \n        process_df(df,test_data)\n        df_res = pd.concat([df_res,df ], axis=0).reset_index(drop=True)\n        del df #free memory\n        gc.collect()\n        #print every 20 iterations\n        if cidx % 20 == 0:\n            print('{}: rows loaded: {}'.format(cidx, df_res.shape[0]))\n        if nrows_load:\n            if res.shape[0] >= nrows_load:\n                break\n    return df_res","metadata":{"_uuid":"8eca02b0829a440d7f3298cc7b04c2c9b435cf6d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#every column as key and the important features to extract from each column\n\ndef parse_json(x,s):\n    res = json.loads(x)\n    try:\n        return res[s]\n    except:\n        return float('NaN') \n\ndef process_df(df,test_data):\n    #process date \n    df['days'] = df['date'].str[-2:]\n    df['days'] = df['days'].astype(int)\n    df['month'] = df['date'].str[-4:-2]\n    df['month'] = df['month'].astype(int)\n    df['year'] = df['date'].str[:4]\n    df['year'] = df['year'].astype(int)\n\n    #process json fields\n    process_dict = {\n        'totals':['transactionRevenue','newVisits','pageviews','hits'] ,\n        'trafficSource':['campaign','source','medium'] ,\n        'device':['browser'],\n        'geoNetwork': ['country','city','continent','region','subContinent']\n    }\n \n    #add new columns from json in df\n    for c,l in process_dict.items():\n        for it in l:\n            df[it] = df[c].apply(lambda x : parse_json(x,it))\n  \n    #labelencoding for continuous data\n    cols = ['country','campaign','source','medium','continent','city','region','socialEngagementType','browser'\n             ,'channelGrouping','subContinent','date']\n    labelencoder_X=LabelEncoder()\n    for c in cols:\n        df.loc[:,c] = labelencoder_X.fit_transform(df.loc[:,c])\n        \n    \n    #Dealing with missing values\n    #transactionsRevenue and NewVisits:  nans ->  0\n    df['transactionRevenue'].fillna(0,inplace=True)\n    df['newVisits'].fillna(0,inplace=True)\n    df['pageviews'].fillna(0,inplace=True)\n    \n    \n    #Casting Str columns to int\n    df['transactionRevenue'] = df['transactionRevenue'].astype('float32')\n    df['newVisits']= df['newVisits'].astype('uint16')\n    df['pageviews'] = df['pageviews'].astype('uint16')\n    df['hits'] = df['hits'].astype('uint32')\n    #df['index'] = df['index'].astype('uint32')\n    \n    #remove json field columns and some unwanted columns\n    \n    rm_col = ['subContinent',\n             'channelGrouping','date','continent','customDimensions','fullVisitorId']\n    if test_data:\n        rm_col = rm_col[:-1]\n    df.drop(list(process_dict.keys()) + rm_col, axis=1,inplace=True)\n    \n#load and process\ndf = load_data(file_train,chunk_size)\ndf_test =load_data(file_test,chunk_size,test_data=True)","metadata":{"_uuid":"e43630a92700da4879e025838e98a154545a5937","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#percent not zero \nper = sum(df['transactionRevenue'] > 0) / len(df['transactionRevenue'])\nprint('Percentage of transactions greater than 0 :   {} %'.format(per*100))","metadata":{"_uuid":"a49a3ad8bcbc09d9b527cce4fd938292f3e2e325","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Setting up Data\nX = df[df.columns[df.columns != 'transactionRevenue']]\nY = np.log1p(df['transactionRevenue'])\nX_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.4, random_state=42)\n\nX_test = df_test[df_test.columns[df_test.columns != 'transactionRevenue']]\nY_test = np.log1p(df_test['transactionRevenue'])\n\n#Handling the imbalanced dataset and looking at how it effects training\n\n#Sampling \n#Using all the 1% non zero transactions revenue with an equal num of rows from the zero results\nindices_nonzero = np.where (y_train > 0)\nindices_zero = np.where (y_train == 0)\nnum_nonzero = len(indices_nonzero[0])\nnum_zero = len(indices_zero[0])\nprint('Number of non-zero transactions revenues = ', num_nonzero)\nprint('Number of zero transactions revenues = ', num_zero)\n\n#Creating a sample dataset containing 50% of rows with non-zero transactions \nall_indx = list(indices_zero[0][0:num_nonzero]) + list(indices_nonzero[0])\nX_sample = X_train.iloc[all_indx,:]\ny_sample = y_train.iloc[all_indx]\n#split train validation \nX_train_sample, X_val_sample, y_train_sample, y_val_sample = train_test_split(X_sample, y_sample, test_size=0.45, random_state=42)\nprint('Sample X_Train shape ', X_train_sample.shape)","metadata":{"_uuid":"7071cdeb4146097ec49cb8cfb11ebd886f64a0f0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining Models\n<a id='def_models'></a>","metadata":{"_uuid":"515f4db426ecb7c8162ef25477daee154957f67c"}},{"cell_type":"code","source":"def create_model_lgbm(X_train,y_train,X_val=None,y_val=None):\n    dtrain = lgb.Dataset(X_train,label=y_train)\n    dval = lgb.Dataset(X_val,label=y_val)\n\n    param_up = {\"objective\" : \"regression\", \"metric\" : \"rmse\", \n               \"max_depth\": 2, \"min_child_samples\": 20, \n               \"reg_alpha\": 1.5, \"reg_lambda\": 1.5,\n               \"num_leaves\" : 15, \"learning_rate\" : 0.1, \n               \"subsample\" : 1, \"colsample_bytree\" : 1, \n               \"verbosity\": -1,'data_random_seed':4}\n    if not X_val is None:\n        valid_sets = (dtrain,dval)\n        valid_names = ['train','valid']\n    else:\n        valid_sets = (dtrain)\n        valid_names = ['train']\n    model = lgb.train(param_up,dtrain,num_boost_round=5000,valid_sets=valid_sets,valid_names=['train','valid'],verbose_eval=300,\n                     early_stopping_rounds=50)\n    return model","metadata":{"_uuid":"cbfe788f345a7e9eaed43b075dfc46c93bce3819","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining Training and Evaluation Methods\n<a id='t_evals'></a>","metadata":{"_uuid":"4a4e77cacc7ae4539b48f1cd16464f6871d12e84"}},{"cell_type":"code","source":"#Fitting and Training\ndef fit_train(x,y,X_val,y_val,layer_size=64):    \n    model = create_model_lgbm(x,y,X_val,y_val)\n\ndef calc_rmse(pred,y):\n    diff =  pred - y\n    RMSE = ((diff ** 2).mean()) ** .5\n    print('RMSE : ',RMSE)","metadata":{"_uuid":"a4f5dbbf2c5a4bd56fdedd7bfcf60ca9d42ba04c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LightGBM\n<a id='lgbm'></a>","metadata":{"_uuid":"04629459fff30546b280d30dd3a3134e0b09848e"}},{"cell_type":"code","source":"#Training\n#print('Training on training/val Dataset')\n#model_training = fit_train(X_train,y_train,X_val,y_val)\nprint('-'*40)\n#print('Training on Sample Dataset')\n#model_sample = fit_train(X_train_sample,y_train_sample,X_val_sample,y_val_sample,layer_size=500)\n#print('-'*40)\n#print('Training on Full training Dataset')\nmodel_full = create_model_lgbm(X, Y)\n\nprint('-'*40)","metadata":{"_uuid":"70b91ea76dd58d136f4c3426f7bf4a945682ff59","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission\n<a id='sub'></a>","metadata":{"_uuid":"654d9acd81fb7c10fe6649f6c90a39fe035851b5","trusted":true}},{"cell_type":"code","source":"#Baseline Predictions\ndf_test['predictions'] = model_full.predict(df_test.loc[:,df_test.columns[1:]],num_iteration=model_full.best_iteration)\ndf_test.loc[df_test['predictions']< 0 ,'predictions']= 0\n#set up dataframe for submission\nsub_df = pd.DataFrame({'fullVisitorId':df_test['fullVisitorId'] , 'PredictedLogRevenue': np.expm1(df_test['predictions'])})\nsub_df = sub_df.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\nsub_df.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\nsub_df[\"PredictedLogRevenue\"] = np.log1p(sub_df[\"PredictedLogRevenue\"])\nsub_df.to_csv(\"baseline_lgb_submission.csv\", index=False)","metadata":{"_uuid":"a3757b95c1ab769caa3bd761627027574ce3b4d7","trusted":true},"execution_count":null,"outputs":[]}]}